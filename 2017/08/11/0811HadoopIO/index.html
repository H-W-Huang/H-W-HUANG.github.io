<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hadoop IO操作 | HWHuang的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Hadoop的IO操作中涉及到几个问题：  保持数据完整性 数据压缩 序列化 存储数据的数据结构  数据完整性​    针对数据在存储过程损坏和丢失的问题，Haddop采取了数据校验和后台进程检测数据块的方法。​    数据校验采用的是循环冗余校验CRC-32。 验证时机 数据节点负责在存储数据及其校验和前验证它们收到的数据（发生于数据块副本传输的时候） （存储时）； 客户端读取数据节点上的数据时">
<meta name="keywords" content="分布式系统,HDFS">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop IO操作">
<meta property="og:url" content="http://yoursite.com/2017/08/11/0811HadoopIO/index.html">
<meta property="og:site_name" content="HWHuang的博客">
<meta property="og:description" content="Hadoop的IO操作中涉及到几个问题：  保持数据完整性 数据压缩 序列化 存储数据的数据结构  数据完整性​    针对数据在存储过程损坏和丢失的问题，Haddop采取了数据校验和后台进程检测数据块的方法。​    数据校验采用的是循环冗余校验CRC-32。 验证时机 数据节点负责在存储数据及其校验和前验证它们收到的数据（发生于数据块副本传输的时候） （存储时）； 客户端读取数据节点上的数据时">
<meta property="og:image" content="http://yoursite.com/2017/08/11/0811HadoopIO/The-internal-structure-of-a-sequence-file-with-no-compression-and-record-compression.png">
<meta property="og:image" content="http://yoursite.com/2017/08/11/0811HadoopIO/mapFile.png">
<meta property="og:image" content="http://yoursite.com/2017/08/11/0811HadoopIO/compressionTypes.tiff">
<meta property="og:image" content="http://yoursite.com/2017/08/11/0811HadoopIO/Writable-class-hierarchy.png">
<meta property="og:image" content="http://yoursite.com/2017/08/11/0811HadoopIO/writable1.tiff">
<meta property="og:updated_time" content="2017-08-12T02:17:52.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hadoop IO操作">
<meta name="twitter:description" content="Hadoop的IO操作中涉及到几个问题：  保持数据完整性 数据压缩 序列化 存储数据的数据结构  数据完整性​    针对数据在存储过程损坏和丢失的问题，Haddop采取了数据校验和后台进程检测数据块的方法。​    数据校验采用的是循环冗余校验CRC-32。 验证时机 数据节点负责在存储数据及其校验和前验证它们收到的数据（发生于数据块副本传输的时候） （存储时）； 客户端读取数据节点上的数据时">
<meta name="twitter:image" content="http://yoursite.com/2017/08/11/0811HadoopIO/The-internal-structure-of-a-sequence-file-with-no-compression-and-record-compression.png">
  
    <link rel="alternate" href="/atom.xml" title="HWHuang的博客" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/styles.css">
  

</head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/index.html">Home</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">HWHuang的博客</h1>
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          <article id="post-0811HadoopIO" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 class="article-title" itemprop="name">
      Hadoop IO操作
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/11/0811HadoopIO/" class="article-date"><time datetime="2017-08-11T11:31:25.000Z" itemprop="datePublished">2017-08-11</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Hadoop的IO操作中涉及到几个问题：</p>
<ol>
<li>保持数据完整性</li>
<li>数据压缩</li>
<li>序列化</li>
<li>存储数据的数据结构</li>
</ol>
<h2 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h2><p>​    针对数据在存储过程损坏和丢失的问题，Haddop采取了<strong>数据校验</strong>和<strong>后台进程检测数据块</strong>的方法。<br>​    数据校验采用的是<strong>循环冗余校验CRC-32</strong>。</p>
<h3 id="验证时机"><a href="#验证时机" class="headerlink" title="验证时机"></a>验证时机</h3><ol>
<li>数据节点负责在存储数据及其校验和前验证它们收到的数据（发生于数据块副本传输的时候） （存储时）；</li>
<li>客户端读取数据节点上的数据时候，会验证校验和（读取时）；</li>
<li>每个数据节点有一个后台进程定期验证存在该节点山的所有数据块（定时）；</li>
</ol>
<p>关于验证，涉及到两个类，一个是<code>FileSystem</code>，一个是<code>CheckSumFileSystem</code>。</p>
<h2 id="基于文件的数据结构"><a href="#基于文件的数据结构" class="headerlink" title="基于文件的数据结构"></a>基于文件的数据结构</h2><p>HDFS和MR主要针对大数据文件来设计，在<em>小文件处理上效率低</em>。解决方法是选择一个容器，将这些小文件包装起来， 将整个文件作为一条记录，可以获取更高效率的储存和处理， 避免多次打开关闭流耗费计算资源.hdfs提供了两种类型的容器 <strong>SequenceFile</strong>和<strong>MapFile</strong>。<br><em>一言蔽之，这些文件结构用来解决HDFS存储小文件的痛点。</em></p>
<h3 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h3><p>​    SequenceFile是Hadoop对二进制键／值对持久化的数据结构。<br>​    Sequence的内部结构如下：<br><img src="/2017/08/11/0811HadoopIO/The-internal-structure-of-a-sequence-file-with-no-compression-and-record-compression.png" alt=""></p>
<p>​    Sequence file由一系列的二进制key/value组成，如果key为小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。文件中每条记录是可序列化，可持久化的键值对，提相应的<strong>读写器</strong>和<strong>排序器</strong>，写操作根据压缩的类型分为3种：</p>
<ul>
<li><p>Write 无压缩写数据，</p>
</li>
<li><p>RecordCompressWriter记录级压缩文件，只压缩值</p>
</li>
<li><p>BlockCompressWrite块级压缩文件，键值采用独立压缩方式;</p>
<p>关于压缩的对象，value可以压缩，key不可以压缩。</p>
<h4 id="对SequenceFile的操作方法："><a href="#对SequenceFile的操作方法：" class="headerlink" title="对SequenceFile的操作方法："></a>对SequenceFile的操作方法：</h4><p>​    Hadoop-0.21.0版本开始中提供了SequenceFile，包括Writer，Reader和SequenceFileSorter类进行写， 读和排序操作。</p>
</li>
</ul>
<p>​    该方案对于小文件的存取都比较自由，不限制用户和文件的多少， <strong>支持Append追加写入</strong>，支持三级文档压缩(不压缩、文件级、块级别)。</p>
<h3 id="MapFile"><a href="#MapFile" class="headerlink" title="MapFile"></a>MapFile</h3><p>​    MapFile是<strong>经过排序</strong>过后的<strong>带索引</strong>的SequenceFile可以根据键进行查找。</p>
<p><img src="/2017/08/11/0811HadoopIO/mapFile.png" alt=""></p>
<p>​    一个MapFile ~可以通过SequenceFile的地址，进行分类查找的格式~ 。使用这个格 式的优点在于，首先会将SequenceFile中的地址都加载入内存，并且进行了key 值排序，从而提供更快的数据查找。<br>与SequenceFile只生成一个文件不同，MapFile<strong>生成一个文件夹</strong>。 索引模型按<strong>128个键</strong>建立的，可以通过<code>io.map.index.interval</code>来修改。</p>
<p>​    </p>
<h3 id="MapFile和SequenceFile的比较"><a href="#MapFile和SequenceFile的比较" class="headerlink" title="MapFile和SequenceFile的比较"></a>MapFile和SequenceFile的比较</h3><ol>
<li>SequenceFile文件是用来存储key-value数据的，但它并<strong>不保证这些存储的key-value是有序的</strong>；MapFile文件则可以看做是存储有序key-value的 SequenceFile文件。</li>
<li>MapFile文件保证key-value的有序（基于key）是通过 ~每一次写入 key-value时的检查机制~ ，这种检查机制其实很简单，就是 ~保证当前正要写入的key-value与上一个刚写入的key-value符合设定的顺序~ ；但是，这种有序是由用户来保证的，一旦写入的key-value不符合key的非递减顺序，则会直接报错而不是自动的去对输入的key-value排序；</li>
</ol>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><p>​    引入压缩是为了<strong>减少储存文件所需空间</strong>，还可以<strong>降低其在网络上传输的时间</strong>。</p>
<p>​    此处简单列出Hadoop上的常用的压缩：<br><img src="/2017/08/11/0811HadoopIO/compressionTypes.tiff" alt=""></p>
<p><em>关于切分</em><br>​    举Bzip2为例，Bzip2支持切分splitting.hdfs上文件1GB，如按照默认块64MB，那么这个 文件被分为16个块（1024/64）。如果把这个块放入MR任务 ，将有<strong>16个map任务输入</strong>。 如果算法不支持切分，后果是MR把这个文件作为<strong>一个Map输入</strong>。这样任务减少了，降低了数据的本地性。<br>​    </p>
<h3 id="Hadoop中实现压缩的方法"><a href="#Hadoop中实现压缩的方法" class="headerlink" title="Hadoop中实现压缩的方法"></a>Hadoop中实现压缩的方法</h3><p>​    大致说来有两种途径：CodeC和使用本地库。</p>
<h4 id="Codec"><a href="#Codec" class="headerlink" title="Codec"></a>Codec</h4><p>​    Hadoop中压缩解压类实现<code>CompressionCodec</code>接口<code>createOutputStream</code>来创建一个<code>CompressionOutputStream</code>，将其压缩格式写入底层的流。</p>
<h3 id="本地库"><a href="#本地库" class="headerlink" title="本地库"></a>本地库</h3><p>​    Hadoop使用java开发，但是有些需求和操作并不适合java，所以引入了本地 库 native。可以高效执行某些操作。<br>如果频繁使用原生库做压解压任务，可以使用 codecpool，通过CodecPool的getCompressor方法获得Compressor对象， 需要传入Codec 。这样可以节省创建Codec对象开销 ，允许反复使用。<br>​<br>​    不同的压缩方法性能上各不相同，<strong>根据不同的使用场景来选择最合适的压缩算法</strong>。</p>
<p>各个压缩算法的特点如下：    </p>
<ol>
<li>Gzip 优点<strong>是压缩率高，速度快</strong>。Hadoop支持与直接处理文本一 样。缺点不支持split，当文件压缩在128m内，都可以用gzip（也就是说存在于一个数据块内）；</li>
<li>Izo 优点压缩速度快，具有合理的压缩率，支持split，是<strong>最流行的</strong>压缩格式。支持native库；缺点是比gzip压缩率低，<strong>hadoop本身不支持</strong>，需要安装；在应用中对lzo格式文件需要处理如 指定 inputformat为lzo格式</li>
<li>Snappy压缩 高速压缩率合理支持本地库。不支持split， <strong>hadoop不支持</strong>，要安装linux没有对应命令；当MR输出数据较大， 作为到reduce数据压缩格式</li>
<li>Bzip2 支持split，很高的压缩率，比gzip高，<strong>hadoop支持但不支持native</strong>，linux自带命令使用方便。缺点压缩解压速度慢</li>
</ol>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><h3 id="Hadoop引入序列化的原因"><a href="#Hadoop引入序列化的原因" class="headerlink" title="Hadoop引入序列化的原因"></a>Hadoop引入序列化的原因</h3><ol>
<li>Hadoop在<strong>集群之间通信或者RPC调用时需要序列化</strong>，而且要求序列化要快，且体积要小，占用带宽小；</li>
<li><strong>Java的序列化机制不满足Hadoop的需求</strong>，占用大量计算开销，且序列化结果体积过大;；它的引用机制也导致大文件不能被切分，浪费空间；此外，很难对其他语言进行扩展使用；</li>
<li>java的反序列化过程每次都会<strong>构造新的对象，不能复用对象</strong>。</li>
</ol>
<h3 id="Hadoop的序列化类"><a href="#Hadoop的序列化类" class="headerlink" title="Hadoop的序列化类"></a>Hadoop的序列化类</h3><p>Hadoop实现了大量的序列化类，如下如所示：<br><img src="/2017/08/11/0811HadoopIO/Writable-class-hierarchy.png" alt=""><br>重点关注右上方的部分，基本上是对Java基本类型的封装。<br><img src="/2017/08/11/0811HadoopIO/writable1.tiff" alt=""></p>
<h3 id="Hadoop的序列化接口"><a href="#Hadoop的序列化接口" class="headerlink" title="Hadoop的序列化接口"></a>Hadoop的序列化接口</h3><h4 id="Writable"><a href="#Writable" class="headerlink" title="Writable"></a>Writable</h4><p>​    Hadoop的序列化类是通过实现序列化接口<code>Writable</code>实现的，<code>Writable</code>接口是基于DataInput与DatOutput的简单高效可序列化接口。继承该接口必须实现的两个方法是<code>readField()</code>和<code>write()</code></p>
<h4 id="WritableComparable"><a href="#WritableComparable" class="headerlink" title="WritableComparable"></a>WritableComparable</h4><p>​    同时实现序列化和比较。类似java的Comparable接口，用于类型的比较。MR其中一个阶段叫排序， 默认使用Key来排序。<br>需要同时实现三个方法<code>readField()</code>，<code>write()</code>和<code>compareTo()</code>。</p>
<p>WritableComparable的简单实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> entity;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.DataInput;</div><div class="line"><span class="keyword">import</span> java.io.DataOutput;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 本类是一个Text的二元组&lt;/br&gt;</span></div><div class="line"><span class="comment"> * 实现了WritableComparable接口&lt;/br&gt;</span></div><div class="line"><span class="comment"> * 比较的方式是，首先比较第一个元素，若相同，则比较第二个元素。&lt;/br&gt;</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> hw</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TextPair</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">TextPair</span>&gt; </span>&#123;</div><div class="line">	</div><div class="line">	<span class="keyword">private</span> Text firstText;</div><div class="line">	<span class="keyword">private</span> Text secondText;</div><div class="line">	</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">//因为Text本身也就实现了Writable结构，此时就直接调用二元组内的两个Text的write方法即可。</span></div><div class="line">		<span class="keyword">this</span>.firstText.write(out);</div><div class="line">		<span class="keyword">this</span>.secondText.write(out);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">// TODO Auto-generated method stub</span></div><div class="line">		<span class="keyword">this</span>.firstText.readFields(in);</div><div class="line">		<span class="keyword">this</span>.secondText.readFields(in);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(TextPair o)</span> </span>&#123;</div><div class="line">		<span class="comment">//首先比较firstText，不同则比较secondText</span></div><div class="line">		<span class="keyword">int</span> result = firstText.compareTo(o.getSecondText());</div><div class="line">		<span class="keyword">return</span> result!=<span class="number">0</span>? result: <span class="keyword">this</span>.secondText.compareTo(o.getSecondText());</div><div class="line">	&#125;</div><div class="line">	</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">getFirstText</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">return</span> firstText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFirstText</span><span class="params">(Text firstText)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.firstText = firstText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">getSecondText</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">return</span> secondText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSecondText</span><span class="params">(Text secondText)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.secondText = secondText;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="实现自定义的序列化对象"><a href="#实现自定义的序列化对象" class="headerlink" title="实现自定义的序列化对象"></a>实现自定义的序列化对象</h3><p>​    实现自定义的Hadoop序列化对象，也需要和内置的序列化对象一样，实现<code>Writable</code>接口，其中必须实现的两个方法是<code>readField()</code>和<code>write()</code>。</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/11/0811HadoopIO/" data-id="cj6c9rzrr00018ijo13l0fs2h" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
    
<ul id="article-nav" class="nav nav-pills nav-justified">
  
  <li role="presentation">
    <a href="/2017/08/10/0810HDFoverview/" id="article-nav-older" class="article-nav-link-wrap">
      <i class="fa fa-chevron-left pull-left"></i>
      <span class="article-nav-link-title">HDFS简述</span>
    </a>
  </li>
  
  
  <li role="presentation">
    <a href="/2017/08/11/0811JavaHadoop/" id="article-nav-newer" class="article-nav-link-wrap">
      <span class="article-nav-link-title">使用Java对HDFS进行文件操作</span>
      <i class="fa fa-chevron-right pull-right"></i>
    </a>
  </li>
  
</ul>


  
</article>




        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <p></p>

</div>


  
  <div class="sidebar-module">
    <h4>Categories</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/大数据/">大数据</a><span class="sidebar-module-list-count">7</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tags</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/HDFS/">HDFS</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/MapReduce/">MapReduce</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/分布式系统/">分布式系统</a><span class="sidebar-module-list-count">7</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/大数据/">大数据</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tag Cloud</h4>
    <p class="tagcloud">
      <a href="/tags/HDFS/" style="font-size: 15px;">HDFS</a> <a href="/tags/MapReduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/分布式系统/" style="font-size: 20px;">分布式系统</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a>
    </p>
  </div>


  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/08/">August 2017</a><span class="sidebar-module-list-count">7</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2017/08/14/Hadoop-MapReduce/">Hadoop_MapReduce</a>
        </li>
      
        <li>
          <a href="/2017/08/11/0811JavaHadoop/">使用Java对HDFS进行文件操作</a>
        </li>
      
        <li>
          <a href="/2017/08/11/0811HadoopIO/">Hadoop IO操作</a>
        </li>
      
        <li>
          <a href="/2017/08/10/0810HDFoverview/">HDFS简述</a>
        </li>
      
        <li>
          <a href="/2017/08/10/0810HDFSreadwrite/">HDFS的读写策略</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2017 H.W.Huang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>



<script src="/js/script.js"></script>

</body>
</html>
