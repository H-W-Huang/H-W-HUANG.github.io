<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>HWHuang的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="HWHuang的博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="HWHuang的博客">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HWHuang的博客">
  
    <link rel="alternate" href="/atom.xml" title="HWHuang的博客" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/styles.css">
  

</head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class="active"
                 href="/index.html">Home</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">HWHuang的博客</h1>
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          
  
    <article id="post-0811JavaHadoop" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/11/0811JavaHadoop/">使用Java对HDFS进行文件操作</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/11/0811JavaHadoop/" class="article-date"><time datetime="2017-08-11T11:38:25.000Z" itemprop="datePublished">2017-08-11</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="主要的API"><a href="#主要的API" class="headerlink" title="主要的API"></a>主要的API</h2><pre><code>在Java中，涉及到HDFS文件操作的API主要有以下几个：
</code></pre><ol>
<li>FileSystem</li>
<li>FSDataOutputStream</li>
<li>FSDataInputStream</li>
<li>Path</li>
<li>Configuration </li>
<li><p>IOUtils</p>
<h3 id="FileSystem"><a href="#FileSystem" class="headerlink" title="FileSystem"></a>FileSystem</h3><p>FileSytem的全限定名为<code>org.apache.hadoop.fs.FileSystem</code>。<br><code>org.apache.hadoop.fs.FileSystem</code>是在分布式环境中<strong>访问和管理HDFS中的文件/目录的通用类</strong>。<br>文件内容以多个大尺寸块（64M）的形式存储在datanode中，namenode中记录了这些块的信息和文件的元信息。<br>FileSystem可以读或者<strong>按块的顺序流式</strong>的访问。<br>FileSystem首先从NameNode中得到块的信息，然后<strong>一个接一个</strong>的读。 ~它打开第一个块，它将读完关闭后才访问下一块~ 。<br>HDFS的复制块带来了更高的可靠性和可扩展性。如果client是数据节点中的一个，它将先访问本地，如果失败的话，它才会到集群的其他节点去访问。</p>
<p><code>FileSystem</code>使用<code>FSDataOutputStream</code> 及<code>FSDataInputStream</code>来读写流的内容。Hadoop提供了多种FileSystem的实现：</p>
</li>
</ol>
<ul>
<li>DistributedFileSystem（DFS）：在分布式的环境中，访问HDFS文件</li>
<li>LocalFileSystem:在本地系统中访问HDFS文件</li>
<li>FTPFileSystem：访问HDFS文件的FTP客户端</li>
<li>WebHdfsFileSystem：通过web接口访问HDFS文件。</li>
</ul>
<h4 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h4><p>FileSystem 的常用操作和相关方法：</p>
<ol>
<li>创建文件<ul>
<li><code>booleancreateNewFile(Path f )</code><ul>
<li><strong>不会覆盖</strong>已有文件</li>
<li>创建成功返回true,失败返回false<br>• <code>FSDataOutputStreamcreate(Path f)</code><br>• <strong>覆盖</strong>已有文件<br>• 创建文件并返回输出流<br>• <code>FSDataOutputStreamcreate(Path f,booleanoverwrite)</code><br>• 创建文件并返回输出流<br>• <code>FSDataOutputStreamcreate(Path f,boolean overwrite,int buffer)</code><br>• <code>FSDataOutputStreamcreate(Path f,boolean overwrite,int buffer,short replication,long blockSize)</code></li>
</ul>
</li>
</ul>
</li>
<li>打开文件<ul>
<li><code>FSDataInputStreamopen(Pathf)</code></li>
<li><code>FSDataInputStreamopen(Pathf,intbufferSize)</code><ul>
<li>返回输入流</li>
<li>如果文件不存在会抛出异常</li>
<li>不指定bufferSize时,会从Configuration中读取 io.file.buffer.size,默认为4096字节</li>
</ul>
</li>
</ul>
</li>
<li>文件追加<br>• <code>FSDataOutputStream append(Path f)</code><br>• <code>FSDataOutputStream append(Path f, int bufferSize)</code><br>   • 块不足64M时,会补足到64M<br>   • 块达到64M之前,该块不可见,ls看不到该块新增的大小,也无 法读取<br>   • 不能同时多个writer追加同一个文件</li>
<li>从本地拷贝文件到HDFS<ul>
<li><code>void copyFromLocalFile (Path src, Path dst)</code></li>
<li><code>void moveFromLocalFile (Path src, Path dst)</code></li>
</ul>
</li>
<li>从HDFS拷贝文件到本地<ul>
<li><code>void copyToLocalFile(boolean delsrc,Path src,Path dst)</code></li>
<li><code>void moveToLocalFile (Path src, Path dst)</code></li>
</ul>
</li>
<li>创建目录<ul>
<li>boolean mkdirs (Path f)<br>• boolean mkdirs (Path f, FsPermission permission)<br>• static boolean mkdirs (FileSystem fs, Path dir, FsPermission permission)<br>• 支持多级目录同时创建 (类似mkdir -p)<br>• 默认权限是755<br>• 成功返回true</li>
</ul>
</li>
<li>删除及重命名<ul>
<li><code>boolean delete (Path f, boolean recursive)</code></li>
<li><code>boolean deleteOnExit (f)</code> 当关闭FileSystem时,才会删除</li>
</ul>
</li>
<li>获取文件或目录信息<ul>
<li>FileStatus[ ] listStatus (Path f, PathFilter filter)<br>• FileStatus[ ] listStatus (Path[ ] dir, PathFilter filter)<br>FileStatus信息包括:</li>
<li>绝对路径</li>
<li>文件大小(单位:字节)</li>
<li>文件访问时间</li>
<li>块大小、复制份数</li>
<li>文件所属用户、组、访问权限</li>
</ul>
</li>
<li>设置文件或目录属性<ul>
<li>void setOwner (Path p, String username, String groupname)<ul>
<li>设置文件或目录所属用户及组</li>
<li>参数p指定文件或目录</li>
<li>参数username,设置此文件或目录的所属用户 – 只返回列出指定目录下的文件或目录信息<br>• void setPermission (Path p, FsPermission permission)<br>• 设置文件或目录权限<br>• 参数p指定文件或目录<br>• 参数permission,指定权限,权限同linux权限雷同<br>• void setReplication (Path f, short replication)<br>• 设置文件复制份数<br>• 参数f指定文件<br>• 参数replication指定复制份数</li>
</ul>
</li>
</ul>
</li>
<li>获取文件或目录信息<ul>
<li>void setTimes (Path f, long mtime, long atime)<br>设置文件的修改及访问时间</li>
</ul>
</li>
</ol>
<h3 id="FSDataOutputStream"><a href="#FSDataOutputStream" class="headerlink" title="FSDataOutputStream"></a>FSDataOutputStream</h3><pre><code>FSDataOutputStream的全限定名是`org.apache.hadoop.fs.FSDataOutputStream`。
使用Java向HDFS**写入数据**时需要用到的流。
</code></pre><h3 id="FSDataInputStream"><a href="#FSDataInputStream" class="headerlink" title="FSDataInputStream"></a>FSDataInputStream</h3><pre><code>FSDataInputStream的全限定名是`org.apache.hadoop.fs. FSDataInputStream `。
使用Java向HDFS**读取数据**时需要用到的流。
</code></pre><h3 id="Path"><a href="#Path" class="headerlink" title="Path"></a>Path</h3><pre><code>Path的全限定名为`org.apache.hadoop.fs.Path`
用来封装HDFS的路径。
</code></pre><h4 id="常用的方法"><a href="#常用的方法" class="headerlink" title="常用的方法"></a>常用的方法</h4><p>Path的常有方法有：</p>
<ol>
<li><code>int depth()</code>：返回路径的深度</li>
<li><code>String getName()</code>： 返回路径上最后的资源名称</li>
<li><code>Path getParent()</code>：返回父目录，如果已是根目录则返回null</li>
<li><code>Path suffix(String suffix)</code>：参数suffix给Path增加后缀，返回加完后缀的Path实例；</li>
<li><code>getFileSystem (Configuration conf)</code>：返回该Path所属的文件系统实例</li>
</ol>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><pre><code>Configuration全限定名为
</code></pre><p><code>org.apache.hadoop.conf. Configuration</code>。<br>​<br>Configuration对配置文件的读取顺序：先加载缺省配置文件，再加载用户定义的配置文件；且对于每一个文件只加载一次：第一个在classpath出现的（这个和Linux系统上在匹配命令时候的逻辑是一样的）</p>
<p><em>classpath的目录顺序:</em></p>
<ol>
<li>$HADOOP_CONF_DIR</li>
<li>$JAVA_HOME/lib/tools.jar 如果$HADOOP_HOME目录下有build目录，则添加build下各子目录</li>
<li>$HADOOP_HOME/hadoop-core-*.jar</li>
<li>$HADOOP_HOME/lib/*.jar</li>
<li>用户在hadoop-env.sh中定义的$HADOOP_CLASS_PATH</li>
<li>当前作为hadoop jar …参数提交的JAR包</li>
</ol>
<h4 id="常用方法-1"><a href="#常用方法-1" class="headerlink" title="常用方法"></a>常用方法</h4><pre><code>Configuration的常用方法有:
</code></pre><ol>
<li>static void addDefaultResource(String name)</li>
<li>void addResource(InputStream in)</li>
<li>void addResource (Path file)  本地文件</li>
<li>void addResource(String name)  classpath中的文件</li>
<li>void addResource (URL url)void set(String name, String value)</li>
<li>void setBoolean(String name, boolean value)</li>
<li>void setInt(String name, String value)</li>
<li>void setLong(String name, long value)</li>
<li>void setFloat(String name, float value)</li>
<li>void setIfUnset(String name, String value)</li>
<li>void setBooleanIfUnset(String name, boolean value)</li>
<li>String get(String name)</li>
<li>boolean getBoolean(String name, boolean defaultValue)</li>
</ol>
<h3 id="IOUtils"><a href="#IOUtils" class="headerlink" title="IOUtils"></a>IOUtils</h3><pre><code>IOUtils的全限定名是`org.apache.hadoop.fs. IOUtils `。
IOUtils是一个I/O帮助类,提供的都是静态方法,不需要实例化。
</code></pre><h4 id="常用方法-2"><a href="#常用方法-2" class="headerlink" title="常用方法"></a>常用方法</h4><p>IOUtils的常用方法有：</p>
<ul>
<li>public static void copyBytes (InputStream in , outputStream out,<br>Configuration conf)<br>• public static void copyBytes (InputStream in , outputStream out, Configuration conf, boolean close)</li>
</ul>
<h2 id="开发基本步骤"><a href="#开发基本步骤" class="headerlink" title="开发基本步骤"></a>开发基本步骤</h2><p>使用Java在HDFS上的基本开发步骤如下：</p>
<ol>
<li>实例化<code>Configuration</code></li>
<li>实例化<code>FileSystem</code><ol>
<li>获取相关Stream进行文件／目录操作</li>
</ol>
</li>
</ol>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/11/0811JavaHadoop/" data-id="cj68oal8u0005xhjomyrc9epz" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0811HadoopIO" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/11/0811HadoopIO/">Hadoop IO操作</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/11/0811HadoopIO/" class="article-date"><time datetime="2017-08-11T11:31:25.000Z" itemprop="datePublished">2017-08-11</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Hadoop的IO操作中涉及到几个问题：</p>
<ol>
<li>保持数据完整性</li>
<li>数据压缩</li>
<li>序列化</li>
<li>存储数据的数据结构</li>
</ol>
<h2 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h2><p>​    针对数据在存储过程损坏和丢失的问题，Haddop采取了<strong>数据校验</strong>和<strong>后台进程检测数据块</strong>的方法。<br>​    数据校验采用的是<strong>循环冗余校验CRC-32</strong>。</p>
<h3 id="验证时机"><a href="#验证时机" class="headerlink" title="验证时机"></a>验证时机</h3><ol>
<li>数据节点负责在存储数据及其校验和前验证它们收到的数据（发生于数据块副本传输的时候） （存储时）；</li>
<li>客户端读取数据节点上的数据时候，会验证校验和（读取时）；</li>
<li>每个数据节点有一个后台进程定期验证存在该节点山的所有数据块（定时）；</li>
</ol>
<p>关于验证，涉及到两个类，一个是<code>FileSystem</code>，一个是<code>CheckSumFileSystem</code>。</p>
<h2 id="基于文件的数据结构"><a href="#基于文件的数据结构" class="headerlink" title="基于文件的数据结构"></a>基于文件的数据结构</h2><p>HDFS和MR主要针对大数据文件来设计，在<em>小文件处理上效率低</em>。解决方法是选择一个容器，将这些小文件包装起来， 将整个文件作为一条记录，可以获取更高效率的储存和处理， 避免多次打开关闭流耗费计算资源.hdfs提供了两种类型的容器 <strong>SequenceFile</strong>和<strong>MapFile</strong>。<br><em>一言蔽之，这些文件结构用来解决HDFS存储小文件的痛点。</em></p>
<h3 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h3><p>​    SequenceFile是Hadoop对二进制键／值对持久化的数据结构。<br>​    Sequence的内部结构如下：<br><img src="/2017/08/11/0811HadoopIO/The-internal-structure-of-a-sequence-file-with-no-compression-and-record-compression.png" alt=""></p>
<p>​    Sequence file由一系列的二进制key/value组成，如果key为小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。文件中每条记录是可序列化，可持久化的键值对，提相应的<strong>读写器</strong>和<strong>排序器</strong>，写操作根据压缩的类型分为3种：</p>
<ul>
<li><p>Write 无压缩写数据，</p>
</li>
<li><p>RecordCompressWriter记录级压缩文件，只压缩值</p>
</li>
<li><p>BlockCompressWrite块级压缩文件，键值采用独立压缩方式;</p>
<p>关于压缩的对象，value可以压缩，key不可以压缩。</p>
<h4 id="对SequenceFile的操作方法："><a href="#对SequenceFile的操作方法：" class="headerlink" title="对SequenceFile的操作方法："></a>对SequenceFile的操作方法：</h4><p>​    Hadoop-0.21.0版本开始中提供了SequenceFile，包括Writer，Reader和SequenceFileSorter类进行写， 读和排序操作。</p>
</li>
</ul>
<p>​    该方案对于小文件的存取都比较自由，不限制用户和文件的多少， <strong>支持Append追加写入</strong>，支持三级文档压缩(不压缩、文件级、块级别)。</p>
<h3 id="MapFile"><a href="#MapFile" class="headerlink" title="MapFile"></a>MapFile</h3><p>​    MapFile是<strong>经过排序</strong>过后的<strong>带索引</strong>的SequenceFile可以根据键进行查找。</p>
<p><img src="/2017/08/11/0811HadoopIO/mapFile.png" alt=""></p>
<p>​    一个MapFile ~可以通过SequenceFile的地址，进行分类查找的格式~ 。使用这个格 式的优点在于，首先会将SequenceFile中的地址都加载入内存，并且进行了key 值排序，从而提供更快的数据查找。<br>与SequenceFile只生成一个文件不同，MapFile<strong>生成一个文件夹</strong>。 索引模型按<strong>128个键</strong>建立的，可以通过<code>io.map.index.interval</code>来修改。</p>
<p>​    </p>
<h3 id="MapFile和SequenceFile的比较"><a href="#MapFile和SequenceFile的比较" class="headerlink" title="MapFile和SequenceFile的比较"></a>MapFile和SequenceFile的比较</h3><ol>
<li>SequenceFile文件是用来存储key-value数据的，但它并<strong>不保证这些存储的key-value是有序的</strong>；MapFile文件则可以看做是存储有序key-value的 SequenceFile文件。</li>
<li>MapFile文件保证key-value的有序（基于key）是通过 ~每一次写入 key-value时的检查机制~ ，这种检查机制其实很简单，就是 ~保证当前正要写入的key-value与上一个刚写入的key-value符合设定的顺序~ ；但是，这种有序是由用户来保证的，一旦写入的key-value不符合key的非递减顺序，则会直接报错而不是自动的去对输入的key-value排序；</li>
</ol>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><p>​    引入压缩是为了<strong>减少储存文件所需空间</strong>，还可以<strong>降低其在网络上传输的时间</strong>。</p>
<p>​    此处简单列出Hadoop上的常用的压缩：<br><img src="/2017/08/11/0811HadoopIO/compressionTypes.tiff" alt=""></p>
<p><em>关于切分</em><br>​    举Bzip2为例，Bzip2支持切分splitting.hdfs上文件1GB，如按照默认块64MB，那么这个 文件被分为16个块（1024/64）。如果把这个块放入MR任务 ，将有<strong>16个map任务输入</strong>。 如果算法不支持切分，后果是MR把这个文件作为<strong>一个Map输入</strong>。这样任务减少了，降低了数据的本地性。<br>​    </p>
<h3 id="Hadoop中实现压缩的方法"><a href="#Hadoop中实现压缩的方法" class="headerlink" title="Hadoop中实现压缩的方法"></a>Hadoop中实现压缩的方法</h3><p>​    大致说来有两种途径：CodeC和使用本地库。</p>
<h4 id="Codec"><a href="#Codec" class="headerlink" title="Codec"></a>Codec</h4><p>​    Hadoop中压缩解压类实现<code>CompressionCodec</code>接口<code>createOutputStream</code>来创建一个<code>CompressionOutputStream</code>，将其压缩格式写入底层的流。</p>
<h3 id="本地库"><a href="#本地库" class="headerlink" title="本地库"></a>本地库</h3><p>​    Hadoop使用java开发，但是有些需求和操作并不适合java，所以引入了本地 库 native。可以高效执行某些操作。<br>如果频繁使用原生库做压解压任务，可以使用 codecpool，通过CodecPool的getCompressor方法获得Compressor对象， 需要传入Codec 。这样可以节省创建Codec对象开销 ，允许反复使用。<br>​<br>​    不同的压缩方法性能上各不相同，<strong>根据不同的使用场景来选择最合适的压缩算法</strong>。</p>
<p>各个压缩算法的特点如下：    </p>
<ol>
<li>Gzip 优点<strong>是压缩率高，速度快</strong>。Hadoop支持与直接处理文本一 样。缺点不支持split，当文件压缩在128m内，都可以用gzip（也就是说存在于一个数据块内）；</li>
<li>Izo 优点压缩速度快，具有合理的压缩率，支持split，是<strong>最流行的</strong>压缩格式。支持native库；缺点是比gzip压缩率低，<strong>hadoop本身不支持</strong>，需要安装；在应用中对lzo格式文件需要处理如 指定 inputformat为lzo格式</li>
<li>Snappy压缩 高速压缩率合理支持本地库。不支持split， <strong>hadoop不支持</strong>，要安装linux没有对应命令；当MR输出数据较大， 作为到reduce数据压缩格式</li>
<li>Bzip2 支持split，很高的压缩率，比gzip高，<strong>hadoop支持但不支持native</strong>，linux自带命令使用方便。缺点压缩解压速度慢</li>
</ol>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><h3 id="Hadoop引入序列化的原因"><a href="#Hadoop引入序列化的原因" class="headerlink" title="Hadoop引入序列化的原因"></a>Hadoop引入序列化的原因</h3><ol>
<li>Hadoop在<strong>集群之间通信或者RPC调用时需要序列化</strong>，而且要求序列化要快，且体积要小，占用带宽小；</li>
<li><strong>Java的序列化机制不满足Hadoop的需求</strong>，占用大量计算开销，且序列化结果体积过大;；它的引用机制也导致大文件不能被切分，浪费空间；此外，很难对其他语言进行扩展使用；</li>
<li>java的反序列化过程每次都会<strong>构造新的对象，不能复用对象</strong>。</li>
</ol>
<h3 id="Hadoop的序列化类"><a href="#Hadoop的序列化类" class="headerlink" title="Hadoop的序列化类"></a>Hadoop的序列化类</h3><p>Hadoop实现了大量的序列化类，如下如所示：<br><img src="/2017/08/11/0811HadoopIO/Writable-class-hierarchy.png" alt=""><br>重点关注右上方的部分，基本上是对Java基本类型的封装。<br><img src="/2017/08/11/0811HadoopIO/writable1.tiff" alt=""></p>
<h3 id="Hadoop的序列化接口"><a href="#Hadoop的序列化接口" class="headerlink" title="Hadoop的序列化接口"></a>Hadoop的序列化接口</h3><h4 id="Writable"><a href="#Writable" class="headerlink" title="Writable"></a>Writable</h4><p>​    Hadoop的序列化类是通过实现序列化接口<code>Writable</code>实现的，<code>Writable</code>接口是基于DataInput与DatOutput的简单高效可序列化接口。继承该接口必须实现的两个方法是<code>readField()</code>和<code>write()</code></p>
<h4 id="WritableComparable"><a href="#WritableComparable" class="headerlink" title="WritableComparable"></a>WritableComparable</h4><p>​    同时实现序列化和比较。类似java的Comparable接口，用于类型的比较。MR其中一个阶段叫排序， 默认使用Key来排序。<br>需要同时实现三个方法<code>readField()</code>，<code>write()</code>和<code>compareTo()</code>。</p>
<p>WritableComparable的简单实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> entity;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.DataInput;</div><div class="line"><span class="keyword">import</span> java.io.DataOutput;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 本类是一个Text的二元组&lt;/br&gt;</span></div><div class="line"><span class="comment"> * 实现了WritableComparable接口&lt;/br&gt;</span></div><div class="line"><span class="comment"> * 比较的方式是，首先比较第一个元素，若相同，则比较第二个元素。&lt;/br&gt;</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> hw</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TextPair</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">TextPair</span>&gt; </span>&#123;</div><div class="line">	</div><div class="line">	<span class="keyword">private</span> Text firstText;</div><div class="line">	<span class="keyword">private</span> Text secondText;</div><div class="line">	</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">//因为Text本身也就实现了Writable结构，此时就直接调用二元组内的两个Text的write方法即可。</span></div><div class="line">		<span class="keyword">this</span>.firstText.write(out);</div><div class="line">		<span class="keyword">this</span>.secondText.write(out);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">// TODO Auto-generated method stub</span></div><div class="line">		<span class="keyword">this</span>.firstText.readFields(in);</div><div class="line">		<span class="keyword">this</span>.secondText.readFields(in);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(TextPair o)</span> </span>&#123;</div><div class="line">		<span class="comment">//首先比较firstText，不同则比较secondText</span></div><div class="line">		<span class="keyword">int</span> result = firstText.compareTo(o.getSecondText());</div><div class="line">		<span class="keyword">return</span> result!=<span class="number">0</span>? result: <span class="keyword">this</span>.secondText.compareTo(o.getSecondText());</div><div class="line">	&#125;</div><div class="line">	</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">getFirstText</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">return</span> firstText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFirstText</span><span class="params">(Text firstText)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.firstText = firstText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">getSecondText</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">return</span> secondText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSecondText</span><span class="params">(Text secondText)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.secondText = secondText;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="实现自定义的序列化对象"><a href="#实现自定义的序列化对象" class="headerlink" title="实现自定义的序列化对象"></a>实现自定义的序列化对象</h3><p>​    实现自定义的Hadoop序列化对象，也需要和内置的序列化对象一样，实现<code>Writable</code>接口，其中必须实现的两个方法是<code>readField()</code>和<code>write()</code>。</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/11/0811HadoopIO/" data-id="cj68oal8k0004xhjo0rrao42t" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0810HDFSreadwrite" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/10/0810HDFSreadwrite/">HDFS的读写策略</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/10/0810HDFSreadwrite/" class="article-date"><time datetime="2017-08-10T13:29:25.000Z" itemprop="datePublished">2017-08-10</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>​    向HDFS读写数据的过程中涉及到以下几个部分：</p>
<ol>
<li>客户端节点</li>
<li>客户端节点上的JVM</li>
<li>数据节点</li>
<li>名称节点</li>
</ol>
<h2 id="从HDFS读取数据"><a href="#从HDFS读取数据" class="headerlink" title="从HDFS读取数据"></a>从HDFS读取数据</h2><p><img src="/2017/08/10/0810HDFSreadwrite/a_client_reading_data_from_HDFS.png" alt=""><br>​<br>流程如下：</p>
<ol>
<li>客户端回调用FileSystem的实例<code>DistributedFileSystem</code>的open方法，获得这个文件的<strong>输入流</strong>；</li>
<li>以RPC的方式调用名称节点，以确定文件开头部分块的位置（包括副本的位置，按照与客户端的距离来排序），</li>
<li>客户端按照就近原则，调用输入流的read方法从最近的DataNode读取数据， 若客户端本身就是一个数据节点，则直接从节点上读取（期间将操作日志发送给NameNode）；</li>
<li>客户端读取到数据块末端，将关闭与这个DataNode 的连接，然后<strong>重新查找</strong>下一个数据块。</li>
<li>重复2-4直到当客户端读取完成，<code>DFSInputStream</code>将被关闭；</li>
</ol>
<h2 id="向HDFS写数据"><a href="#向HDFS写数据" class="headerlink" title="向HDFS写数据"></a>向HDFS写数据</h2><p><img src="/2017/08/10/0810HDFSreadwrite/a_client_writing_data_to_HDFS.png" alt=""></p>
<ol>
<li>客户端通过在<code>DistributedFileSystem</code>调用<code>create</code>方法来创建文件；</li>
<li><code>DistributedFileSystem</code>的一个RPC调用NameNode，在文件系统的密码<strong>命名空间</strong>中创建一个新文件；NameNode 将通过一些检查，比如 ~文件是否存在~，客户端 ~是否拥有创建权限~ 等;通过检查之后，在NameNode 添加文件信息。注意，因为此时文件没有数据，所以<strong>NameNode 上也没有文件数据块的信息</strong>。</li>
<li>创建结束之后，HDFS会返回一个输出流 DFSDataOutputStream 给客户端；</li>
<li>客户端调用输出流DFSDataOutputStream的write方法向HDFS 中对应的文件写入数据。</li>
<li>数据首先会被<strong>分包</strong>，这些分包会写人一个<strong>输出流的内部队列</strong> Data 队列中， ~接收完数据分包，输出流DFSDataOutputStream会向NameNode申请保存文件和副本数据块的若干个DataNode ， 这若干个DataNode 会形成一个数据传输管道。~ DFSDataOutputStream 将数据传输给距离上最短的DataNode ，这个DataNode 接收到数据包之后会传给下一个DataNode 。 ~数据在各DataNode之间通过管道流动，而不是全部由输出流分发~， 以减少传输开销。</li>
<li>因为各DataNode位于不同机器上，数据需要通过网络发送，所以，为了保证所有DataNode 的数据都是准确的， ~接收到数据的 DataNode 要向<strong>发送者</strong>发送确认包(ACK Packet )~ 。对于某个数据块，只有当DFSDataOutputStream 收到了所有DataNode 的正确ACK，才能确认传输结束。DFSDataOutputStream 内部专 门维护了一个<strong>等待ACK 队列</strong>，这一队列保存已经进入管道传输数据、但是并未被完全确认的数据包。</li>
<li>重复4-6，DFSDataInputStream 继续等待直到所有数据写入完毕并<strong>被确认</strong>后，调用<code>close</code>关闭流，调用<code>complete</code>方法通知NameNode 文件写入完成。NameNode 接收到complete消息之后， ~等待相应数量的副本写入完毕后~， 告知客户端。</li>
</ol>
<p><em>数据流列表形成一个管线。</em><br><em>一个包只有在被管线中的<strong>所有节点</strong>确认后才会被移出确认队列</em></p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/10/0810HDFSreadwrite/" data-id="cj68oal880000xhjo28mijjey" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0810HDFoverview" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/10/0810HDFoverview/">HDFS简述</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/10/0810HDFoverview/" class="article-date"><time datetime="2017-08-10T13:29:25.000Z" itemprop="datePublished">2017-08-10</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="什么是HDFS"><a href="#什么是HDFS" class="headerlink" title="什么是HDFS"></a>什么是HDFS</h2><pre><code>HDFS是Hadoop的一个分布式文件系统 (Hadoop Distributed File System)。对外部客户机而言,HDFS 就像一个传统的分级文件系统。可以创建、删除、移动或重命名文件,等等。
HDFS在Hadoop框架中所处的位置如下图所示：
</code></pre><p><img src="/2017/08/10/0810HDFoverview/Hframework.tiff" alt=""></p>
<p><em>关于文件系统</em></p>
<blockquote>
<pre><code>文件系统由三部分组成:与**文件管理有关软件**、 **被管理文件**以及**实施文件管理所需数据结构**。常见的有 Explorer、Total Commander在每台存储设 备里有很多被 管理文件如通用结构, 由超级块、节 点、数据块、 目录块、间接 块组成。  
从系统角度来看,文件系统是对文件存储器空 间进行组织和分配,负责文件存储并对存入的 文件进行保护和检索的系统。  
</code></pre></blockquote>
<h2 id="HDFS所能提供的"><a href="#HDFS所能提供的" class="headerlink" title="HDFS所能提供的"></a>HDFS所能提供的</h2><h3 id="大规模数据分布存储能力"><a href="#大规模数据分布存储能力" class="headerlink" title="大规模数据分布存储能力"></a>大规模数据分布存储能力</h3><p>​    Hadoop能够将超大的数据分块存储到不同的分区上，从而实现存储超大数据的功能。</p>
<h3 id="高并发访问能力"><a href="#高并发访问能力" class="headerlink" title="高并发访问能力"></a>高并发访问能力</h3><p><img src="/2017/08/10/0810HDFoverview/concurrent.tiff" alt=""><br>​    数据存在于多个节点上，读取的时候可以并发读取）</p>
<h3 id="流式文件访问-提供简单的一致性模型"><a href="#流式文件访问-提供简单的一致性模型" class="headerlink" title="流式文件访问,提供简单的一致性模型"></a>流式文件访问,提供简单的一致性模型</h3><p>​    DFS是用<strong>流处理方式</strong>处理文件, 每个文件在系统里都能找到它的本地化映像,所以对于用户来说,别管文件是什么格式的,也不用在意被分到哪里,只管从DFS里取出就可以了。<br>​    一次写入,多次读取。 <u>数据源通常由源生成或从数据源直接复制而来,接着长时间在此数据集上进行各类分析</u>,大数据不需要搬来搬去。</p>
<h3 id="强大的容错能力（冗余存储）"><a href="#强大的容错能力（冗余存储）" class="headerlink" title="强大的容错能力（冗余存储）"></a>强大的容错能力（冗余存储）</h3><p><img src="/2017/08/10/0810HDFoverview/check.tiff" alt=""><br>​    每个分片文件需要分片服务器校验。</p>
<p>​    </p>
<p>​    此处就又有一些问题：</p>
<p>​    文件分布后调用会效率很低吗? 文件分布处理过程丢失了怎 么办?文件种类很多到底分到哪好?<br>​    解决以上问题，HDFS采用分片冗余，本地校验。</p>
<p>​    数据冗余式存储, 直接将多份的分片文件交给分片后的存储服务器去校验 。</p>
<p><img src="/2017/08/10/0810HDFoverview/redundency.tiff" alt=""><br>​    冗余后的分片文件还有个额外功能, <u>只要冗余的分片文件中有一份是完整的,经过多次协同调整后,其他分片文件也将完整</u> 。（自动修正能力）</p>
<h2 id="HDFS的架构"><a href="#HDFS的架构" class="headerlink" title="HDFS的架构"></a>HDFS的架构</h2><p><img src="/2017/08/10/0810HDFoverview/Block.tiff" alt=""></p>
<p>在HDFS这个分布式的文件系统中，存在着<strong>两种节点</strong>，DataNode和NameNode。<br>一个HDFS集群是由<strong>一个</strong>NameNode和<strong>一定数目</strong>的DataNode组成。</p>
<h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>​    NameNode是一个主服务器,用来<u>管理整个 文件系统的命名空间和元数据</u>,以及<u>处理来自外界的文件访问请求</u> 。</p>
<p>​    NameNode 保存了文件系统的三种元数据：<br>• 命名空间, 即<strong>整个分布式文件系统的目录结构</strong>；<br>• 数据块与文件名的映射表；<br>• 每个数据块副本的位置信息,每一个数据块默认有3 个副本；<br>（一言蔽之，DataNode用来<strong>管理客户端访问</strong>，<strong>管理数据</strong>）</p>
<p>​    稍微具体地说，即：<br>​    Namenode执行文件系统的名字空间操作，比如<strong>打开</strong>、<strong>关闭</strong>、<strong>重命名</strong>文件或目录。它也负责<strong>确定数据块到具体Datanode节点的映射</strong>。</p>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>​    稍微具体地说，即：<br>​    DataNode 负责处理文件系统用户具体的数 据读写请求,同时也可以处理NameNode 对数据块的创建、删除副本的指令。</p>
<h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h3><p>​    HFDS的文件以<strong>块</strong>作为基本单位，默认大小设为64M字节。<br>​    HDFS将一个文件分为一个或数个块来存储。</p>
<p>使用数据块有以下的优点：</p>
<ol>
<li>当一个文件大于集群中任意一个磁盘的时候,文件系统可以充分利用集群中所 有的磁盘；</li>
<li>管理块使底层的存储子系统相对简单 ；</li>
<li>块更加适合备份,从而为容错和高可用性的实现带来方便；</li>
<li><p>采用块方式,实现了名字与位置的分离,实现了的存储位置的独立性；</p>
<p>上述优点的<strong>容错</strong>体现在块的<strong>冗余备份</strong>上，简单说来如下：</p>
</li>
</ol>
<ul>
<li>每个块在集群上会存储多 份(replica)，默认是三份；</li>
<li>某个块的所有备份都是同一个ID，这样一来对于块的管理变很便利了，不需要专门去记录某一块是那一份数据的；</li>
<li><p>系统可以根据机架的配置自动分配备份位置；也就是说，某一个数据块存于某个机架上的节点上，那么另外的几份备份会被存在另外的机架的两个节点上；</p>
<p>此外，在关于数据校验的设计上，每个块会在本地文件系统产生两个文件,一个是<strong>实际的数据文件</strong>,另一个是<strong>块的附加信息文件,其中包括数据的校验和</strong>。</p>
</li>
</ul>
<h3 id="元数据-－－-NameNode的账目"><a href="#元数据-－－-NameNode的账目" class="headerlink" title="元数据  －－  NameNode的账目"></a>元数据  －－  NameNode的账目</h3><p>HDFS的元数据包括：</p>
<ul>
<li>文件系统目录树信息<br>• 文件名,目录名<br>• 文件和目录的从属关系<br>• 文件和目录的大小,创建及最后访问时间<br>• 权限</li>
<li>文件和块的对应关系<br>• 文件由哪些块组成</li>
<li>块的存放位置<br>• 机器名,块ID</li>
</ul>
<h4 id="HDFS对元数据和实际数据的存储方法"><a href="#HDFS对元数据和实际数据的存储方法" class="headerlink" title="HDFS对元数据和实际数据的存储方法"></a>HDFS对元数据和实际数据的存储方法</h4><p>​    元数据存储在一台指定的服务器上，而且是存在<strong>内存</strong>中的(<strong>NameNode</strong>)<br>​    实际数据储存在集群的其他机器的本地文件系统中 (<strong>DataNode</strong>)</p>
<h2 id="访问HDFS的方法"><a href="#访问HDFS的方法" class="headerlink" title="访问HDFS的方法"></a>访问HDFS的方法</h2><p>采用命令行客户端，或者使用相关的API客户端。</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/10/0810HDFoverview/" data-id="cj68oal8d0001xhjopnrrsbth" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-cap" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/07/cap/">cap理论</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/07/cap/" class="article-date"><time datetime="2017-08-07T13:03:25.000Z" itemprop="datePublished">2017-08-07</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="CAP理论概述"><a href="#CAP理论概述" class="headerlink" title="CAP理论概述"></a>CAP理论概述</h3><p>C: Consistency 一致性<br>A: Availability 可用性<br>P: Partition tolerance 分区容错性</p>
<p>该理论认为，一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这<strong>三项中的两项</strong>。</p>
<p>（数据）一致性：<br>all nodes see the same data at the same time，即更新操作成功并返回客户端完成后，<strong>所有节点</strong>在<strong>同一时间</strong>的<strong>数据完全一致</strong></p>
<p> （服务）可用性：<br>Reads and writes always succeed，也就是说，服务必须<strong>一直可用</strong>，而且是正常响应时间。</p>
<p>（系统）分区容错性：<br>the system continues to operate despite arbitrary message loss or failure of part of the system，即分布式系统在~遇到某节点或网络分区故障~的时候，仍然能够对外提供满足<strong>一致性</strong>和<strong>可用性</strong>的服务。</p>
<p>参考：<a href="http://www.hollischuang.com/archives/666" target="_blank" rel="external">分布式系统的CAP理论-HollisChuang’s Blog</a></p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/07/cap/" data-id="cj68oal8v0006xhjomxfvvc35" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-test" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/07/test/">第一天作业笔记</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/07/test/" class="article-date"><time datetime="2017-08-07T10:42:19.000Z" itemprop="datePublished">2017-08-07</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="数据的单位：-K-M-G-T-P-E-Z-Y-D-N"><a href="#数据的单位：-K-M-G-T-P-E-Z-Y-D-N" class="headerlink" title="数据的单位： K M G T P E Z Y D N"></a>数据的单位： K M G T P E Z Y D N</h3><p>大数据设计到的数据单位有下：<br>B ，KB，MB，GB，TB，PB，EB，ZB，YB<br>其中，从左到右，单位之间的关系如下：<br>1B     = 8 bites<br>1KB    =  1024B<br>1MB=  1024KB<br>1GB    =  1024MB<br>1TB    =  1024GB<br>1PB    =  1024TB<br>1EB    =  1024PB<br>1ZB    =  1024EB<br>1YB    =  1024ZB<br>1DB    =  1024YB<br>1NB    =  1024DB</p>
<hr>
<h3 id="什么是大数据？"><a href="#什么是大数据？" class="headerlink" title="什么是大数据？"></a>什么是大数据？</h3><blockquote>
<p>Big data is a term for <strong>data sets</strong> that are ~so large or complex that traditional data processing application software is inadequate to deal with them~.<br><a href="http://www.webopedia.com/TERM/S/structured_data.html" target="_blank" rel="external">What is Structured Data? Webopedia Definition</a><br>根据维基百科的对大数据的定义，可以看到大数据是一个关于数据集的术语。<br>本身足够大或者足够复杂以至于传统的数据处理应用软件不足够处理它们。</p>
</blockquote>
<ol>
<li>数据类型： 结构化的， 非结构化的</li>
</ol>
<p>结构化数据结构:</p>
<blockquote>
<p>Structured data refers to any data that <strong>resides in a fixed field</strong> within a record or file. This includes data contained in <strong>relational databases</strong> and <strong>spreadsheets</strong>.<br>​    也就是说，结构化的数据是行数据，存储在数据库里,可以用二维表结构来逻辑表达实现的数据。</p>
</blockquote>
<p>非结构话的数据结构:</p>
<blockquote>
<p>Unstructured data is all those things that <strong>can’t be so readily classified and fit into a neat box</strong>: photos and graphic images, videos, streaming instrument data, webpages, PDF files, PowerPoint presentations, emails, blog entries, wikis and word processing documents.<br>​    也即是说，非结构化的数据大多指的是办公文档、文本、图片、XML、HTML、各类报表、图像和音频/视频信息等等数据。</p>
</blockquote>
<hr>
<h3 id="大数据的4个V"><a href="#大数据的4个V" class="headerlink" title="大数据的4个V"></a>大数据的4个V</h3><p>大数据的四个V包括：</p>
<ol>
<li>Velocity：实现<strong>快速的数据流传</strong></li>
<li>Variety： 具有<strong>多样的数据类型</strong></li>
<li>Volume： 存有<strong>海量的数据规模</strong>（TB，PB，EB级别）</li>
<li>Value：存在着<strong>巨大的价值</strong></li>
</ol>
<hr>
<h3 id="大数据的工作流程"><a href="#大数据的工作流程" class="headerlink" title="大数据的工作流程"></a>大数据的工作流程</h3><ul>
<li>采集数据<br>数据的类型多种多样。可以是各大网络上每天产生的数据，可以是各种传感器所产生的数据，可以是科研或者实验日志。<br>采集的方法一般有以下几种：<ul>
<li>系统日志采集方法，即在系统内部使用专用的数据采集工具来采集日志</li>
<li>网络数据采集方法：通过网络爬虫或者网站提供的API来获取数据。这种方法多用来收集非结构化数据；</li>
</ul>
</li>
<li>清洗数据ETL</li>
<li>ETL的概念<br> ETL表示<code>Extract Transform Load</code>，也就是抽取，转换，装载的过<br> 程，是构建<strong>数据仓库</strong>的一个重要环节；</li>
<li>数据清理<br> 数据清洗（data cleansing/data cleaning/data scrubing）是一个<strong>减少错误</strong>和<strong>不一致性</strong>、<strong>解决对象识别</strong>的过程。<br> 也就是说，在数据清理结果，将通过ETL处理过程，减少原数据的错误和不一致型，从业有利于下一阶段的分析；</li>
<li>分析数据<br>就是对清洗过后的数据进行分析和挖掘。<br>利用分布式数据库，或者分布式计算集群来对存储于其内的海量数据进行普通的分析和分类汇总等，以满足大多数常见的分析需求。<br>一般来说，本阶段会有<strong>数据量大</strong>，其对<strong>系统资源，特别是I/O</strong>会有极大的占用等问题。</li>
<li>呈现<br>也就是将分析后的数据进行呈现。<br>一般采用各种可视化工具，将数据转化为图表等形式进行呈现。<br>当前可用的可视化工具有：Tableau，ChartBlocks，Datawrapper，Plotly，RAW等等；</li>
</ul>
<hr>
<h3 id="计算模式"><a href="#计算模式" class="headerlink" title="计算模式"></a>计算模式</h3><ul>
<li>批处理<br>批处理就是对某对象进行批量的处理。<br>在大数据处理中，最适合的批处理是<strong>MapReduce</strong>。<br>简单的讲，MapReduce对具有<strong>简单数据关系</strong>，<strong>易于划分</strong>的大规模数据采用<strong>分而治之</strong>的方式进行<strong>并行处理</strong>。本身是一个~单输入、两阶段( Map 和Reduce) 的数据处理过程~。</li>
<li>流计算<br>流计算对一定<strong>时间窗口内</strong>应用系统产生的新数据完成<strong>实时的计算</strong>，避免造成数据堆积和丢失。<br>流计算能够较好地解决MapReduce模型存在的延迟大的问题。<br>使用场景有：<br>统计网站中每一个页面，域名的点击次数<br>内部系统的运行监控（统计被监控服务器的运行状态）<br>记录最大值和最小值<br><a href="http://yangxiaowei.cn/wordpress/?p=551" target="_blank" rel="external">关于大数据下流计算的一些问题</a></li>
<li>迭代计算<br>迭代计算能够解决批量计算的难以迭代的缺陷，该计算模式通常用于处理大规模的科学计算。<br>相关的框架有：Apache Giraph，HaLoop，Twister等等。</li>
<li>交互式处理<br>特点：<ul>
<li>系统与操作人员以人机对话的方式<strong>一问一答</strong>；</li>
<li>操作人员提出请求,数据以对话的方式输入,系统便提供相应的数据或提示信息,引导操作人员<strong>逐步</strong>完成所需的操作,直至获得最后处理结果；</li>
<li>存储在系统中的数据文件<strong>能够被及时处理修改</strong>,同时处理<strong>结果可以立刻被使用</strong>；<br>典型系统：Dremel、spark</li>
</ul>
</li>
<li>图计算<br>“图计算”是以“<strong>图论</strong>”为基础的对现实世界的一种“图”结构的抽象表达，以及在这种数据结构上的计算模式。<br>引入图计算是因为图数据结构很好的表达了<strong>数据之间的关联性</strong>( dependencies between data )，关联性计算是大数据计算的核心——~通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用的信息~。比如，通过为购物者之间的关系建模，就能很快找到口味相似的用户，并为之推荐商品；或者在社交网络中，通过传播关系发现意见领袖。<br>典型的系统包括Google 公司的Pregel 、Facebook Giraph 、Spark 下的GraphX；<br><a href="http://www.csdn.net/article/1970-01-01/2825748" target="_blank" rel="external">如何利用“图计算”实现大规模实时预测分析-CSDN.NET</a></li>
<li>内存计算<br>内存计算是以<strong>大数据为中心</strong>、依托计算机硬件的发展、依靠新型的软件体系结构,即,通过对体系结构及编程模型等进行重大革新,~将数据装入内存中处理,而尽量避免 I/O 操作的一种新型的以数据为中心~的并行计算模式.<a href="http://www.jos.org.cn/ch/reader/create_pdf.aspx?file_no=5103&amp;journal_id=jos" target="_blank" rel="external">内存计算技术研究综述</a></li>
</ul>
<hr>
<h3 id="数据库类型："><a href="#数据库类型：" class="headerlink" title="数据库类型："></a>数据库类型：</h3><p><a href="http://www.jianshu.com/p/107c6b045245" target="_blank" rel="external">超全的数据库分类介绍 - 简书</a></p>
<ul>
<li>列存储（Column-oriented）数据库<br>列存储数据库将数据存储在<strong>列族</strong>中，一个列族存储<strong>经常被一起查询的相关数据</strong>，比如人类，我们经常会查询某个人的姓名和年龄，而不是薪资。这种情况下姓名和年龄会被放到一个列族中，薪资会被放到另一个列族中。<br>这种数据库通常用来应对分布式存储海量数据。<br>典型产品：Cassandra、HBase。</li>
<li>基于文档（mongoldb）<br>文档型数据库的灵感是来自于Lotus Notes办公软件，而且它同第一种键值数据库类似。该类型的数据模型是<strong>版本化的文档</strong>，<strong>半结构化的文档</strong>以<strong>特定的格式</strong>存储，比如JSON。~文档型数据库可以看作是键值数据库的升级版，允许之间嵌套键值~。而且文档型数据库比键值数据库的查询效率更高。<br>面向文档数据库会将数据以文档形式存储。~每个文档都是<strong>自包含</strong>的数据单元，是一系列数据项的集合。~每个数据项都有一个名词与对应值，值既可以是简单的数据类型，如字符串、数字和日期等；也可以是复杂的类型，如有序列表和关联对象。数据存储的<strong>最小单位</strong>是<strong>文档</strong>，同一个表中存储的文档属性可以是不同的，数据可以使用XML、JSON或JSONB等多种形式存储。<br>典型产品：MongoDB、CouchDB</li>
<li>基于内存<br>内存数据库是指一种将<strong>全部内容存放在内存</strong>中，而非传统数据库那样存放在外部存储器中的数据库。内存数据库指的是所有的数据访问控制都在内存中进行，这是与磁盘数据库相对而言的。<br>典型产品：Redis</li>
</ul>
<hr>
<h3 id="分布式系统（拜占庭将军问题）"><a href="#分布式系统（拜占庭将军问题）" class="headerlink" title="分布式系统（拜占庭将军问题）"></a>分布式系统（拜占庭将军问题）</h3><p>引用一下Distributed Systems Concepts and Design（Third Edition）中的一句话：<br>A distributed system is one in which components <strong>located at networked computers</strong> communicate and coordinate their actions <strong>only by passing messages</strong>。<br>从此得出两个要点：</p>
<ul>
<li>组件分布与网络计算机；</li>
<li>仅通过消息传递来通信和协调；</li>
</ul>
<p>在分布式系统中有一个著名的问题：拜占庭将军问题。<br>此处引用巴比特网的一边文章简单介绍：<br>有以下几个要点需要注意：</p>
<ul>
<li>问题假设了消息的信道是没有问题的（当考虑了信道是有问题的，则涉及到另一个问题了<strong>两军问题</strong>）；</li>
<li><strong>一致性</strong>和<strong>正确性</strong>是该问题的要求；</li>
<li>结论是：若叛徒数为m，当将军总数n至少为3m+1时，问题可解；</li>
</ul>
<p><a href="http://www.8btc.com/baizhantingjiangjun" target="_blank" rel="external">拜占庭将军问题深入探讨 | 巴比特</a></p>
<hr>
<h3 id="关于CDH"><a href="#关于CDH" class="headerlink" title="关于CDH"></a>关于CDH</h3><p>CDH全称为Cloudera’s Distribution Including Apache Hadoop。<br>CDH首先是100%开源，基于Apache协议。基于Apache Hadoop和相关projiect开发。可以做批量处理，交互式sql查询和及时查询，基于角色的权限控制。在企业中使用最广的hadoop分发版本。</p>
<p>CHD和Hadoop的关系类似于 Red Hat 之于Linux。</p>
<hr>
<h3 id="Hadoop技术栈"><a href="#Hadoop技术栈" class="headerlink" title="Hadoop技术栈"></a>Hadoop技术栈</h3><ol>
<li>hdfs<br>Hadoop的分布式文件系统，提供数据存储的功能。</li>
<li>mapreduce<br>Hadoop 的核心。是一个可以对大量数据进行分布式处理的软件框架，基于Map/Reduce技术。</li>
<li>hive<br>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。<br><a href="https://baike.baidu.com/item/hive/67986" target="_blank" rel="external">hive（数据仓库工具）_百度百科</a></li>
<li>hbase<br>HBase是一个开源的非关系型分布式数据库（NoSQL），它参考了谷歌的BigTable建模，实现的编程语言为Java。它是Apache软件基金会Hadoop项目的一部分，运行于HDFS文件系统之上，为Hadoop提供类似于BigTable规模的服务。<br>用于改善<strong>数据的访问</strong>。<br><a href="https://www.bing.com/knows/search?q=hbase&amp;mkt=zh-cn" target="_blank" rel="external">hbase - Bing 网典</a></li>
<li>Sqoop<br>主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。<br>主要用于<strong>其他数据库</strong>和<strong>Hadoop中的数据库</strong>举行数据传输。</li>
<li>Zookeeper<br>zookeeper作为一个开源的分布式应用协调系统，已经用到了许多分布式项目中，用来完成<strong>统一命名服务</strong>、<strong>状态同步服务</strong>、<strong>集群管理</strong>、<strong>分布式应用配置项的管理</strong>等工作。<br>换句话说，Zookeeper主要用来对分布式项目进行管理。</li>
<li>Mahout<br>Mahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。Mahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。<br><a href="https://baike.baidu.com/item/mahout" target="_blank" rel="external">mahout_百度百科</a></li>
</ol>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/07/test/" data-id="cj68oal8y0009xhjoui6vup17" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>


    </footer>
  </div>
  
</article>



  




        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <p></p>

</div>


  
  <div class="sidebar-module">
    <h4>Categories</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/大数据/">大数据</a><span class="sidebar-module-list-count">6</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tags</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/HDFS/">HDFS</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/分布式系统/">分布式系统</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/大数据/">大数据</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tag Cloud</h4>
    <p class="tagcloud">
      <a href="/tags/HDFS/" style="font-size: 15px;">HDFS</a> <a href="/tags/分布式系统/" style="font-size: 20px;">分布式系统</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a>
    </p>
  </div>


  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/08/">August 2017</a><span class="sidebar-module-list-count">6</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2017/08/11/0811JavaHadoop/">使用Java对HDFS进行文件操作</a>
        </li>
      
        <li>
          <a href="/2017/08/11/0811HadoopIO/">Hadoop IO操作</a>
        </li>
      
        <li>
          <a href="/2017/08/10/0810HDFSreadwrite/">HDFS的读写策略</a>
        </li>
      
        <li>
          <a href="/2017/08/10/0810HDFoverview/">HDFS简述</a>
        </li>
      
        <li>
          <a href="/2017/08/07/cap/">cap理论</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2017 H.W.Huang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>



<script src="/js/script.js"></script>

</body>
</html>
