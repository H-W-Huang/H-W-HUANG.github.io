<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>HWHuang的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="HWHuang的博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="HWHuang的博客">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HWHuang的博客">
  
    <link rel="alternate" href="/atom.xml" title="HWHuang的博客" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/styles.css">
  

</head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class="active"
                 href="/index.html">Home</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">HWHuang的博客</h1>
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          
  
    <article id="post-hadoopMRAdv" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/15/hadoopMRAdv/">hadoop高级</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/15/hadoopMRAdv/" class="article-date"><time datetime="2017-08-15T11:39:30.000Z" itemprop="datePublished">2017-08-15</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="MapReduce的具体过程"><a href="#MapReduce的具体过程" class="headerlink" title="MapReduce的具体过程"></a>MapReduce的具体过程</h2><p>​    MapReduce的工作流程涉及到5个小流程：input，splitting，<strong>mapping</strong>，shuffling，<strong>reducing</strong>。如下图所示：<br><img src="/2017/08/15/hadoopMRAdv/MRProcess.tiff" alt=""></p>
<h3 id="Splitting"><a href="#Splitting" class="headerlink" title="Splitting"></a>Splitting</h3><p>​    在这个过程主要涉及到两个类<code>InputFormat</code>和<code>InputSplit</code>。</p>
<h4 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a>InputFormat</h4><p>​    通过使用InputFormat，MapReduce框架可以做到：</p>
<ol>
<li>验证作业的输入的正确性；</li>
<li>把<strong>输入文件</strong>切分成多个<strong>逻辑InputSplits</strong>，并把每一个InputSplit分别分发给一个<strong>单独的MapperTask</strong>；</li>
<li><p>提供RecordReader的实现，这个RecordReader从指定的InputSplit中正确读出一条一条的Ｋ－Ｖ对，~这些 Ｋ－Ｖ对将由我们写的Mapper方法处理~ 。</p>
<p>当数据传送给map时，map会将输入分片传送到InputFormat，InputFormat则调用方法<code>getRecordReader()</code>生成<code>RecordReader</code>，RecordReader再通过<code>creatKey()</code>、<code>creatValue()</code>方法创建可供map处理的一个一个的<key,value>对。简而言之，<code>InputFormat()</code>方法是用来<strong>生成可供map处理的<key,value>对</key,value></strong>的。</key,value></p>
<p>默认使用的ITextInputFormat，在TextInputFormat中，每个文件（或其一部分）都会单独地作为map的输入，而这个是继承自FileInputFormat的。<br>之后，每行数据都会生成一条记录，每条记录则表示成<key,value>形式：</key,value></p>
</li>
</ol>
<ul>
<li>key值是每个数据的记录在数据分片中<strong>字节偏移量</strong>，数据类型LongWritable；</li>
<li>value值是每行的内容，数据类型是Text。</li>
</ul>
<h4 id="关于inputSplit"><a href="#关于inputSplit" class="headerlink" title="关于inputSplit"></a>关于inputSplit</h4><p>​    任何分割操作的实现都继承自Apache抽象基类——InputSplit，它定义了分割的长度及位置。<br>​    InputSplit是hadoop定义的用来传送给每个单独的map的数据，InputSplit存储的<strong>并非数据本身</strong>，而是一个<strong>分片长度</strong>和一个<strong>记录数据位置的数组</strong>。生成InputSplit的方法可以通过InputFormat()来设置。</p>
<p>参考：</p>
<ol>
<li><a href="http://blog.csdn.net/zolalad/article/details/12653093" target="_blank" rel="external">MapReduce中InputFormat和InputSplit解读 - zolalad的专栏        - CSDN博客</a></li>
</ol>
<h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><p><img src="/2017/08/15/hadoopMRAdv/mapstages.tiff" alt=""></p>
<h4 id="Combiner"><a href="#Combiner" class="headerlink" title="Combiner"></a>Combiner</h4><p>​    每一个map都可能会产生大量的本地输出，Combiner的作用就是对map端 的输出先做一次合并，以减少在map和reduce节点之间的数据传输量， 以提高网络IO性能。</p>
<p>​    Combiner的作用</p>
<ol>
<li>Combiner实现<strong>本地key</strong>的聚合，对map输出的key排序value进行迭代；</li>
<li>Combiner还有本地reduce功能（其本质上就是一个reduce）</li>
</ol>
<p>Combiner作用图解：<br><img src="/2017/08/15/hadoopMRAdv/combiner.tiff" alt=""><br>可以看到，combiner是在分区之前开始工作的。</p>
<p>​    Combiner的使用方法<br>在main方法里，使用一下方法指定某个job的combiner</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">job.setCombinerClass(MyReducer.class);</div></pre></td></tr></table></figure>
<h4 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h4><p>​    MapReduce的使用者通常会指定Reduce任务和Reduce任务输出文件的数量（R）。<br>​    用户在中间key上使用分区函数来对数据进行分区，之后在输入到后续任务执行进程。一个默认的分区函数式使用hash方法（比如常见的：hash(key) mod R）进行分区。hash方法能够产生非常平衡的分区。</p>
<p>分区Partitioner主要作用在于以下两点</p>
<ol>
<li>根据业务需要，产生多个输出文件</li>
<li>多个reduce任务并发运行，提高整体job的运行效率</li>
</ol>
<h3 id="Shuffling"><a href="#Shuffling" class="headerlink" title="Shuffling"></a>Shuffling</h3><h4 id="shuffle的定义"><a href="#shuffle的定义" class="headerlink" title="shuffle的定义"></a>shuffle的定义</h4><p>在《Hadoop权威指南》中，对Shuffle的解释如下：</p>
<blockquote>
<p>MapReduce makes the guarantee that the input to every reducer is sorted by key. The process by which the system performs the sort—and transfers the map outputs to the reducers as inputs—is known as the shuffle. </p>
</blockquote>
<p>​    MapReduce保证每一个reducer的输入<strong>都是已经按照key拍过序的</strong>。这个<u>由系统执行的将mapper的输出排序后传输到reducer作为其输入的过程</u>称为shuffle（洗牌）。<br>​    从定义可以看出，sort这个动作发生在shuffle这个阶段。<br>​    这个过程有<strong>排序</strong>，也有<strong>复制</strong>。</p>
<p>shuffle和sort在mr过程中的体现如下图：<br><img src="/2017/08/15/hadoopMRAdv/shufflingAndSort.png" alt=""><br>输入，map，分区，排序，分组，存到磁盘<br>fetch，sort，combine，reduce    </p>
<h4 id="Shuffle中的Map端："><a href="#Shuffle中的Map端：" class="headerlink" title="Shuffle中的Map端："></a>Shuffle中的Map端：</h4><p><img src="/2017/08/15/hadoopMRAdv/mapInShuffle.png" alt=""><br>对上图的解释如下：<br>​    在map端，先是InputSplit，在InputSplit中含有DataNode中的数据，每 个InputSplit都会分配一个Mapper任务，Mapper任务结束后产 <k2，v2>的输出， <strong>这些输出先存放在缓存中</strong>，每个map有 个环形内存缓冲区， 于存储任务的 输出。默认为100MB(io.sort.mb属性)， 旦达到阀值0.8(<code>io.sort.spill.percent</code>)， 一个后台线程就把内容写到(spill)Linux<strong>本地磁盘</strong>中的指定目录(mapred.local.dir)下的新建的一个溢出写文件。<br>​    写磁盘前，<strong>要进 partition、sort和combine等操作</strong>。通过分区，将同类型的数据分开处 ，之后对同分区的数据进行排序，如果有Combiner，还要对排序后的数据进 combine。等最后记录写完，将全部溢出 件合并为 个分区且 排序的 件<br>最后将磁盘中的数据送到Reduce中，图中Map输出有三个分区，有一个分区数据被送到图示的Reduce任务中，剩下的两个分区被送到其他Reducer任务中。  图示的Reducer任务的其他的三个输 则来其他节点的Map输出。</k2，v2></p>
<p>一些要点：</p>
<ol>
<li>map的输出存在一个环形内存缓冲区，内容超过80%的时候，内存的内容会被<strong>新开的线程</strong>写入到HDFS；</li>
<li>线程根据数据最终被送到的recuder，将<strong>数据划分为相应的分区</strong>；</li>
<li>在每个分区中，后台线程按照key进行内排序，此时若是有一个combiner，combiner将基于排序后的输出来运行。也就是说在溢写文件被写入到磁盘<strong>之前</strong>运行了combiner；</li>
<li>一个map有可能会产生若干个溢写文件（spill file），最终溢写文件会被<strong>合并</strong>成一个<strong>已分区</strong>且<strong>已排序</strong>的文件。</li>
<li>map的输出文件位于其tasktracker的<strong>本地磁盘</strong>。</li>
</ol>
<p>​    </p>
<h4 id="Shuffle中的Reduce端"><a href="#Shuffle中的Reduce端" class="headerlink" title="Shuffle中的Reduce端"></a>Shuffle中的Reduce端</h4><p><img src="/2017/08/15/hadoopMRAdv/ReduceInShuffle.tiff" alt=""><br>​    Copy阶段:Reducer通过Http式得到输出文件的分区。reduce端可能从n个map的结果中获取数据， 这些map的执行速度不尽相同， 当其中一个map运行结束时，reduce就会从JobTracker中获取该信息。map运行结束后TaskTracker会得到消息，进而将消息汇报给JobTracker，reduce定时从 JobTracker获取该信息，reduce端默认有5个数据复制线程从map端复制数据；<br>​    Merge阶段:如果形成多个磁盘文件会进行合并。从map端复制来的数据 先写到reduce端的<strong>缓存</strong>中，同样缓存占用<strong>到达一定阈值后会将数据写到磁盘中</strong>，<strong>同样会进 partition、combine、排序等过程</strong>。如果形成多个磁盘 文件还会进行合并，<strong>最后一次合并的结果</strong>作为reduce的输入而不是写到磁盘中；</p>
<p>一些要点：</p>
<ol>
<li>只要有一个map任务完成了，reduce任务就开始复制该map的输出</li>
<li>reducer通过<strong>HTTP</strong>得到输出文件的<strong>分区</strong>。</li>
<li>在将数据从内存存到磁盘中这个过程，就可以使用各种<strong>压缩</strong>算法了；</li>
</ol>
<h4 id="Shuffle过程中需要注意的以下点"><a href="#Shuffle过程中需要注意的以下点" class="headerlink" title="Shuffle过程中需要注意的以下点"></a>Shuffle过程中需要注意的以下点</h4><p>Shuffle同时涉及到Map和Reduce两端。<br>在Map端，map的所处的节点上，会有一个partition，combine，sort的操作；<br>在Reduce端，同样会有partition，combine，sort的操作，不过这次发生在分布式文件系统上；</p>
<h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><p><img src="/2017/08/15/hadoopMRAdv/reduce.png" alt=""><br>此处主要是对受到的的<k2,v2>执行程序员编写的reduce方法。</k2,v2></p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/15/hadoopMRAdv/" data-id="cj6dixftm000ginjos1pc6xsc" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-wordcount" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/15/wordcount/">wordcount的编写</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/15/wordcount/" class="article-date"><time datetime="2017-08-15T11:29:34.000Z" itemprop="datePublished">2017-08-15</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>在Eclipse创建MapReduce项目，新建一个MyWordCount类。</p>
<p>编写代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">package</span> test;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.net.URI;</div><div class="line"><span class="keyword">import</span> java.util.Iterator;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyWordCount</span> </span>&#123;</div><div class="line">	</div><div class="line">		<span class="keyword">private</span> <span class="keyword">static</span>  String INPUT_PATH = <span class="string">"hdfs://192.168.178.128:9000/input/words.txt"</span>;</div><div class="line">		<span class="keyword">private</span> <span class="keyword">static</span>  String OUTPUT_PATH = <span class="string">"hdfs://192.168.178.128:9000/output01"</span>;</div><div class="line">	</div><div class="line">		<span class="comment">// mapper</span></div><div class="line">		<span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line">			</div><div class="line">			<span class="meta">@Override</span></div><div class="line">			<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, // offset 偏移量</span></span></div><div class="line"><span class="function"><span class="params">					Text value,</span></span></div><div class="line"><span class="function"><span class="params">					Context context)</span></span></div><div class="line"><span class="function">							<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">				<span class="comment">//map处理开始</span></div><div class="line">				<span class="comment">//切词</span></div><div class="line">				String[] lines = value.toString().split(<span class="string">"\\s+"</span>);</div><div class="line">				<span class="comment">// 举个例子：line x: hello world</span></div><div class="line">				<span class="comment">// 得到的结果就是: (hello,1) (world,1) ，分别写入到context中</span></div><div class="line">				<span class="keyword">for</span> (String word : lines) &#123;</div><div class="line">					context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</div><div class="line">				&#125;</div><div class="line">				System.out.println(<span class="string">"mapping......"</span>);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		</div><div class="line">		<span class="comment">//reducer</span></div><div class="line">		<span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line">			</div><div class="line">			<span class="meta">@Override</span></div><div class="line">			<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, // map -&gt; sort part -&gt; reduce</span></span></div><div class="line"><span class="function"><span class="params">					Iterable&lt;IntWritable&gt; value,</span></span></div><div class="line"><span class="function"><span class="params">					Context context)</span></span></div><div class="line"><span class="function">							<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">				</div><div class="line">				<span class="comment">// reducer得到的key是唯一的</span></div><div class="line">				<span class="comment">// 此处举个例子， 该reducer处理的key是 hello</span></div><div class="line">				<span class="comment">// 目前有的键值对有 hello 1 </span></div><div class="line">				<span class="comment">// 				hello 1</span></div><div class="line">				<span class="comment">// 				hello 1</span></div><div class="line">				<span class="comment">//得到的结果将会是 hello 3</span></div><div class="line">				<span class="keyword">int</span> sum = <span class="number">0</span>;</div><div class="line">				Iterator&lt;IntWritable&gt; it = value.iterator();</div><div class="line">				<span class="keyword">while</span>(it.hasNext())&#123;</div><div class="line">					sum += it.next().get();</div><div class="line">				&#125;</div><div class="line"> 				</div><div class="line">				<span class="comment">//将结果写入到context</span></div><div class="line">				context.write(key, <span class="keyword">new</span> IntWritable(sum));	</div><div class="line">				System.out.println(<span class="string">"reducing......"</span>);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		</div><div class="line">		</div><div class="line">		</div><div class="line"></div><div class="line">		<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">			</div><div class="line">			<span class="comment">// 作业前的准备</span></div><div class="line">			Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">			FileSystem fs = FileSystem.get(URI.create(OUTPUT_PATH),conf);</div><div class="line">			<span class="comment">// 检查目录是否存在，是则但删除之</span></div><div class="line">			<span class="keyword">if</span>(fs.exists(<span class="keyword">new</span> Path(OUTPUT_PATH)))&#123;</div><div class="line">				fs.delete(<span class="keyword">new</span> Path(OUTPUT_PATH));</div><div class="line">			&#125;</div><div class="line">				</div><div class="line">			</div><div class="line">			<span class="comment">//第一步，创建Job</span></div><div class="line">			Job job = <span class="keyword">new</span> Job(conf,<span class="string">"myjob"</span>);</div><div class="line">			</div><div class="line">			<span class="comment">//设置Job,mapper,reducer,combiner(optional)</span></div><div class="line">			job.setJarByClass(MyWordCount.class);</div><div class="line">			job.setMapperClass(MyMapper.class);</div><div class="line">			job.setReducerClass(MyReducer.class);</div><div class="line">			job.setCombinerClass(MyReducer.class);</div><div class="line">			</div><div class="line">			</div><div class="line">			<span class="comment">//设置Output类型</span></div><div class="line">			job.setOutputKeyClass(Text.class);</div><div class="line">			job.setOutputValueClass(IntWritable.class);</div><div class="line">			</div><div class="line">			FileInputFormat.addInputPath(job,<span class="keyword">new</span> Path(INPUT_PATH));</div><div class="line">			FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(OUTPUT_PATH));</div><div class="line">			</div><div class="line">			<span class="comment">//最后一步</span></div><div class="line">			job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line">			</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>运行输出如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div></pre></td><td class="code"><pre><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">34</span> WARN util.NativeCodeLoader: Unable to load <span class="keyword">native</span>-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">35</span> INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">35</span> INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">36</span> WARN mapreduce.JobSubmitter: Hadoop command-line option parsing not performed. Implement the Tool <span class="class"><span class="keyword">interface</span> <span class="title">and</span> <span class="title">execute</span> <span class="title">your</span> <span class="title">application</span> <span class="title">with</span> <span class="title">ToolRunner</span> <span class="title">to</span> <span class="title">remedy</span> <span class="title">this</span>.</span></div><div class="line">17/08/15 19:37:36 WARN mapreduce.JobSubmitter: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">36</span> INFO input.FileInputFormat: Total input paths to process : <span class="number">1</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">36</span> INFO mapreduce.JobSubmitter: number of splits:<span class="number">1</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">36</span> INFO mapreduce.JobSubmitter: Submitting tokens <span class="keyword">for</span> job: job_local484001035_0001</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapreduce.Job: The url to track the job: http:<span class="comment">//localhost:8080/</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapreduce.Job: Running job: job_local484001035_0001</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.LocalJobRunner: OutputCommitter set in config <span class="keyword">null</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.LocalJobRunner: Waiting <span class="keyword">for</span> map tasks</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.LocalJobRunner: Starting task: attempt_local484001035_0001_m_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.Task:  Using ResourceCalculatorProcessTree : <span class="keyword">null</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: Processing split: hdfs:<span class="comment">//192.168.178.128:9000/input/words.txt:0+86</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: (EQUATOR) <span class="number">0</span> kvi <span class="number">26214396</span>(<span class="number">104857584</span>)</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: mapreduce.task.io.sort.mb: <span class="number">100</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: soft limit at <span class="number">83886080</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: bufstart = <span class="number">0</span>; bufvoid = <span class="number">104857600</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: kvstart = <span class="number">26214396</span>; length = <span class="number">6553600</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: Map output collector <span class="class"><span class="keyword">class</span> </span>= org.apache.hadoop.mapred.MapTask$MapOutputBuffer</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapreduce.Job: Job job_local484001035_0001 running in uber mode : <span class="keyword">false</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapreduce.Job:  map <span class="number">0</span>% reduce <span class="number">0</span>%</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: </div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: Starting flush of map output</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: Spilling map output</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: bufstart = <span class="number">0</span>; bufend = <span class="number">134</span>; bufvoid = <span class="number">104857600</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: kvstart = <span class="number">26214396</span>(<span class="number">104857584</span>); kvend = <span class="number">26214352</span>(<span class="number">104857408</span>); length = <span class="number">45</span>/<span class="number">6553600</span></div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: Finished spill <span class="number">0</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Task: Task:attempt_local484001035_0001_m_000000_0 is done. And is in the process of committing</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: map</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Task: Task <span class="string">'attempt_local484001035_0001_m_000000_0'</span> done.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: Finishing task: attempt_local484001035_0001_m_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: map task executor complete.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: Waiting <span class="keyword">for</span> reduce tasks</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: Starting task: attempt_local484001035_0001_r_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Task:  Using ResourceCalculatorProcessTree : <span class="keyword">null</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@<span class="number">2</span>ae4ec62</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=<span class="number">1336252800</span>, maxSingleShuffleLimit=<span class="number">334063200</span>, mergeThreshold=<span class="number">881926912</span>, ioSortFactor=<span class="number">10</span>, memToMemMergeOutputsThreshold=<span class="number">10</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.EventFetcher: attempt_local484001035_0001_r_000000_0 Thread started: EventFetcher <span class="keyword">for</span> fetching Map Completion Events</div><div class="line">17/08/15 19:37:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local484001035_0001_m_000000_0 decomp: 95 len: 99 to MEMORY</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.InMemoryMapOutput: Read <span class="number">95</span> bytes from map-output <span class="keyword">for</span> attempt_local484001035_0001_m_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: <span class="number">95</span>, inMemoryMapOutputs.size() -&gt; <span class="number">1</span>, commitMemory -&gt; <span class="number">0</span>, usedMemory -&gt;<span class="number">95</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: <span class="number">1</span> / <span class="number">1</span> copied.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: finalMerge called with <span class="number">1</span> in-memory map-outputs and <span class="number">0</span> on-disk map-outputs</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Merger: Merging <span class="number">1</span> sorted segments</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Merger: Down to the last merge-pass, with <span class="number">1</span> segments left of total size: <span class="number">88</span> bytes</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: Merged <span class="number">1</span> segments, <span class="number">95</span> bytes to disk to satisfy reduce memory limit</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: Merging <span class="number">1</span> files, <span class="number">99</span> bytes from disk</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: Merging <span class="number">0</span> segments, <span class="number">0</span> bytes from memory into reduce</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Merger: Merging <span class="number">1</span> sorted segments</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Merger: Down to the last merge-pass, with <span class="number">1</span> segments left of total size: <span class="number">88</span> bytes</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: <span class="number">1</span> / <span class="number">1</span> copied.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapreduce.Job:  map <span class="number">100</span>% reduce <span class="number">0</span>%</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.Task: Task:attempt_local484001035_0001_r_000000_0 is done. And is in the process of committing</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.LocalJobRunner: <span class="number">1</span> / <span class="number">1</span> copied.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.Task: Task attempt_local484001035_0001_r_000000_0 is allowed to commit now</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO output.FileOutputCommitter: Saved output of task <span class="string">'attempt_local484001035_0001_r_000000_0'</span> to hdfs:<span class="comment">//192.168.178.128:9000/output01/_temporary/0/task_local484001035_0001_r_000000</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.LocalJobRunner: reduce &gt; reduce</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.Task: Task <span class="string">'attempt_local484001035_0001_r_000000_0'</span> done.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.LocalJobRunner: Finishing task: attempt_local484001035_0001_r_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.LocalJobRunner: reduce task executor complete.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">40</span> INFO mapreduce.Job:  map <span class="number">100</span>% reduce <span class="number">100</span>%</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">40</span> INFO mapreduce.Job: Job job_local484001035_0001 completed successfully</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">40</span> INFO mapreduce.Job: Counters: <span class="number">35</span></div><div class="line">	File System Counters</div><div class="line">		FILE: Number of bytes read=<span class="number">562</span></div><div class="line">		FILE: Number of bytes written=<span class="number">491225</span></div><div class="line">		FILE: Number of read operations=<span class="number">0</span></div><div class="line">		FILE: Number of large read operations=<span class="number">0</span></div><div class="line">		FILE: Number of write operations=<span class="number">0</span></div><div class="line">		HDFS: Number of bytes read=<span class="number">172</span></div><div class="line">		HDFS: Number of bytes written=<span class="number">65</span></div><div class="line">		HDFS: Number of read operations=<span class="number">15</span></div><div class="line">		HDFS: Number of large read operations=<span class="number">0</span></div><div class="line">		HDFS: Number of write operations=<span class="number">6</span></div><div class="line">	Map-Reduce Framework</div><div class="line">		Map input records=<span class="number">7</span></div><div class="line">		Map output records=<span class="number">12</span></div><div class="line">		Map output bytes=<span class="number">134</span></div><div class="line">		Map output materialized bytes=<span class="number">99</span></div><div class="line">		Input split bytes=<span class="number">108</span></div><div class="line">		Combine input records=<span class="number">12</span></div><div class="line">		Combine output records=<span class="number">7</span></div><div class="line">		Reduce input groups=<span class="number">7</span></div><div class="line">		Reduce shuffle bytes=<span class="number">99</span></div><div class="line">		Reduce input records=<span class="number">7</span></div><div class="line">		Reduce output records=<span class="number">7</span></div><div class="line">		Spilled Records=<span class="number">14</span></div><div class="line">		Shuffled Maps =<span class="number">1</span></div><div class="line">		Failed Shuffles=<span class="number">0</span></div><div class="line">		Merged Map outputs=<span class="number">1</span></div><div class="line">		<span class="function">GC time <span class="title">elapsed</span> <span class="params">(ms)</span></span>=<span class="number">0</span></div><div class="line">		<span class="function">Total committed heap <span class="title">usage</span> <span class="params">(bytes)</span></span>=<span class="number">475004928</span></div><div class="line">	Shuffle Errors</div><div class="line">		BAD_ID=<span class="number">0</span></div><div class="line">		CONNECTION=<span class="number">0</span></div><div class="line">		IO_ERROR=<span class="number">0</span></div><div class="line">		WRONG_LENGTH=<span class="number">0</span></div><div class="line">		WRONG_MAP=<span class="number">0</span></div><div class="line">		WRONG_REDUCE=<span class="number">0</span></div><div class="line">	File Input Format Counters </div><div class="line">		Bytes Read=<span class="number">86</span></div><div class="line">	File Output Format Counters </div><div class="line">		Bytes Written=<span class="number">65</span></div></pre></td></tr></table></figure>
<p>输入文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hello liuchengwu</div><div class="line">hello liulao</div><div class="line">hello dora</div><div class="line">hello liu</div><div class="line">hello hadoop</div><div class="line">helloWorld</div><div class="line">helloWorld</div></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">dora	1</div><div class="line">hadoop	1</div><div class="line">hello	5</div><div class="line">helloWorld	2</div><div class="line">liu	1</div><div class="line">liuchengwu	1</div><div class="line">liulao	1</div></pre></td></tr></table></figure>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/15/wordcount/" data-id="cj6dizlfo0000itjoqltt16ta" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/代码/">代码</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-Hadoop-MapReduce" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/14/Hadoop-MapReduce/">Hadoop_MapReduce</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/14/Hadoop-MapReduce/" class="article-date"><time datetime="2017-08-14T14:39:19.000Z" itemprop="datePublished">2017-08-14</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="什么是MapReduce"><a href="#什么是MapReduce" class="headerlink" title="什么是MapReduce"></a>什么是MapReduce</h2><p>​    MapReduce是一个编程模型，用以进行大数据量的计算。<br>​    Hadoop MapReduce是一个软件框架，基于该框架能够 容易地编写应用程序，这些应用程序能够运行在由上千个商用机器组成的大集群上，并以一种<strong>可靠的</strong>，<strong>具有容错能力</strong>的方式并行地处理上TB级别的海量数据集。</p>
<h3 id="MapReduce的特点"><a href="#MapReduce的特点" class="headerlink" title="MapReduce的特点"></a>MapReduce的特点</h3><p>MapReduce的特点如下：</p>
<ol>
<li>软件框架</li>
<li>并行处理 </li>
<li>可靠且容错 </li>
<li>大规模集群 </li>
<li>海量数据集</li>
</ol>
<h3 id="MapReduce的处理方式"><a href="#MapReduce的处理方式" class="headerlink" title="MapReduce的处理方式"></a>MapReduce的处理方式</h3><p>​    MapReduce处理工作的主要思想是：<strong>分而治之</strong>。<br>​    整个工作流程中涉及到4个实体：客户端，JobTracker，TaskTracker，HDFS。<br>​    在这四个实体中，和目前的编程有关的就是<strong>客户端</strong>，编写的就是MapReduce程序。<br>​    该工作流程如下图所示：<br><img src="/2017/08/14/Hadoop-MapReduce/runAMapReduceJob.jpg" alt=""></p>
<p>​    客户端：提交MapReduce工作；<br>​    JobTracker：协调作业的运行，只有一个；<br>​    TaskTracker：处理作业划分后的任务，可以有多个；<br>​    HDFS：在其他实体之间<strong>共享</strong>作业文件；<br>‼️工作流程如下：</p>
<ol>
<li>客户端要<strong>编写mapreduce程序</strong>，配置好mapreduce的作业也就是<strong>job</strong>， 接下来向JobTracker请求一个jobID，这个时候JobTracker为该job任务分配一个新的ID值；</li>
<li>接着JokTracker进行各种检查和计算，例如，<strong>输出目录是否存在</strong>（存在则无法运行），<strong>输入目录是否存在</strong>（不存在则不可运行），<strong>据输入计算输入分片(Input Split)</strong>，完成之后JobTracker为Job分配资源，客户端向JobTracker提交作业(submitJob)；</li>
<li>JobTracker初始化作业，初始化主要做的是<strong>将Job放入一个内部的队列</strong>，让配置好的<strong>作业调度器</strong>能调度到这个作业。作业调度器会初始化这个job，初始化就是创建一个正在运行的job对象(封装任务和记录信息)， 以便JobTracker跟踪job的状态和进程</li>
<li>初始化完毕，作业调度器获取输入分片信息，<strong>每个分片创建一个map任务</strong>；</li>
<li>开始分配任务，taskTracker通过心跳包💗（5s为间隔）向JobTracker发送自己的状态，两者通过心跳包通信；</li>
<li>开始执行任务。当jobtracker获得了最后一个完成指定任务的 tasktracker操作成功的通知时候，jobtracker会把整个job状态置为成功。</li>
</ol>
<h3 id="MapReduce的运行机制"><a href="#MapReduce的运行机制" class="headerlink" title="MapReduce的运行机制"></a>MapReduce的运行机制</h3><p><img src="/2017/08/14/Hadoop-MapReduce/mapreduceArch.tiff" alt=""><br>​    在Hadoop中，一个MapReduce作业会把输入的数据集切分为<strong>若干独立的数据块</strong>，由Map任务以<strong>完全并行</strong>的方式处理；<br>​    框架会<strong>对Map的输出先进行排序，</strong>然后把结果输入给Reduce任务；<br>作业的输入和输出都会<strong>被存储在文件系统中</strong>，整个框架负责任务的调度和监控，以及重新执行已经关闭的任务；<br>​    MapReduce框架和分布式文件系统是运行在<strong>一组相同的节点</strong>， 计算节点和存储节点都是在一起的；</p>
<h3 id="MapReduce的输入输出"><a href="#MapReduce的输入输出" class="headerlink" title="MapReduce的输入输出"></a>MapReduce的输入输出</h3><p>​    在整个MapReduce作业中，涉及到<strong>三组键值对</strong>。</p>
<p><img src="/2017/08/14/Hadoop-MapReduce/keysAndValues.tiff" alt=""><br>​    <k1，v1>：初始输入，作为map的输入；<br>​    <k2，v2>：map的输出，在经过shuffle（排序等操作）后，作为reduce的输入；<br>​    <k3，v3>：reduce的输出。</k3，v3></k2，v2></k1，v1></p>
<h3 id="MapReduce的工作流程"><a href="#MapReduce的工作流程" class="headerlink" title="MapReduce的工作流程"></a>MapReduce的工作流程</h3><p>​    MapReduce的工作流程涉及到5个小流程：input，splitting，<strong>mapping</strong>，shuffling，<strong>reducing</strong>，final result。如下图所示：</p>
<p><img src="/2017/08/14/Hadoop-MapReduce/MRProcess.tiff" alt=""><br>​    教程将其重新划分为<strong>5个阶段</strong>。输入分片(input split)、 map阶段、 combiner阶段、 shuffle阶段和 reduce阶段。</p>
<h4 id="输入分片-input-split"><a href="#输入分片-input-split" class="headerlink" title="输入分片(input split):"></a>输入分片(input split):</h4><p>​    在进行map计算之前，mapreduce会根 据输入文件计算输入分片(input split)，每个输入分片(input split)针对一个map任务；<br>​    输入分片(input split)存储的并非数据本身，而是一个分片长度和 一个记录数据的位置的数组，输入分片(input split)往往和hdfs 的block(块)关系很密切</p>
<blockquote>
<p>假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别 是3mb、65mb和127mb，那么mapreduce会把<strong>3mb文件分为一个输入分片（3mb &lt; 64mb）</strong> (input split)，<strong>65mb则是两个( 128mb &gt; 65mb &gt; 64mb)</strong>输入分片(input split)；而<strong>127mb也是两个输入分片（128mb &gt; 127mb &gt; 64mb）</strong>(input split)；<br>即我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个 map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce 优化计算的一个关键点。  </p>
</blockquote>
<h4 id="map阶段"><a href="#map阶段" class="headerlink" title="map阶段"></a>map阶段</h4><p>​    程序员编写的map函数；</p>
<h4 id="combine阶段"><a href="#combine阶段" class="headerlink" title="combine阶段"></a>combine阶段</h4><p>​    Combiner阶段是程序员可以选择的，<strong>combiner其实也是一种reduce操作</strong>，因此我们看见WordCount类里是用reduce进行加载的。<br>​    Combiner是一个<strong>本地化的reduce操作</strong>，它是map运算的后续操作，主要是在map计算出中间文件前做一个<strong>简单的合并重复key值的操作</strong>。</p>
<blockquote>
<p>例如：我们对文件里的单词频率做统计，map计算时候如果碰到一个hadoop的单词就会记录为1，但是这篇文章里hadoop可能会出现n多次，那么map输出 文件冗余就会很多，因此在reduce计算前对相同的key做一个合并操作， 那么文件会变小，这样就提高了宽带的传输效率，毕竟hadoop计算力宽带资源往往是计算的瓶颈也是最为宝贵的资源，但是combiner操作是有 风险的，使用它的原则是<strong>combiner的输入不会影响到reduce计算的最终输入</strong>，<br>​    例如:如果计算只是求总数，最大值，最小值可以使用combiner，但是做平 均值计算使用combiner的话，最终的reduce计算结果就会出错。  </p>
</blockquote>
<h4 id="shuffle阶段"><a href="#shuffle阶段" class="headerlink" title="shuffle阶段"></a>shuffle阶段</h4><p>​    将map的输出作为reduce的输入的过程。</p>
<h4 id="reduce阶段"><a href="#reduce阶段" class="headerlink" title="reduce阶段"></a>reduce阶段</h4><p>​    和map函数一样也是程序员编写的,最终结果是存储在 hdfs上的。</p>
<h2 id="琐碎"><a href="#琐碎" class="headerlink" title="琐碎"></a>琐碎</h2><blockquote>
<p>每一个片都会开一个mapper，最终将所有分片进行聚合<br>所有的这些过程都是通过jobtracker进行调度的。<br>每个阶段都是一个进程（class），每个进程启动无数个mapper（线程）<br>有多少个key，就有多少个reducer。<br>分块－不同块内的数据是不相同的  </p>
</blockquote>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/14/Hadoop-MapReduce/" data-id="cj6dixftc0009injo2rvpcwdw" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0811JavaHadoop" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/11/0811JavaHadoop/">使用Java对HDFS进行文件操作</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/11/0811JavaHadoop/" class="article-date"><time datetime="2017-08-11T11:38:25.000Z" itemprop="datePublished">2017-08-11</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="主要的API"><a href="#主要的API" class="headerlink" title="主要的API"></a>主要的API</h2><pre><code>在Java中，涉及到HDFS文件操作的API主要有以下几个：
</code></pre><ol>
<li>FileSystem</li>
<li>FSDataOutputStream</li>
<li>FSDataInputStream</li>
<li>Path</li>
<li>Configuration </li>
<li><p>IOUtils</p>
<h3 id="FileSystem"><a href="#FileSystem" class="headerlink" title="FileSystem"></a>FileSystem</h3><p>FileSytem的全限定名为<code>org.apache.hadoop.fs.FileSystem</code>。<br><code>org.apache.hadoop.fs.FileSystem</code>是在分布式环境中<strong>访问和管理HDFS中的文件/目录的通用类</strong>。<br>文件内容以多个大尺寸块（64M）的形式存储在datanode中，namenode中记录了这些块的信息和文件的元信息。<br>FileSystem可以读或者<strong>按块的顺序流式</strong>的访问。<br>FileSystem首先从NameNode中得到块的信息，然后<strong>一个接一个</strong>的读。 ~它打开第一个块，它将读完关闭后才访问下一块~ 。<br>HDFS的复制块带来了更高的可靠性和可扩展性。如果client是数据节点中的一个，它将先访问本地，如果失败的话，它才会到集群的其他节点去访问。</p>
<p><code>FileSystem</code>使用<code>FSDataOutputStream</code> 及<code>FSDataInputStream</code>来读写流的内容。Hadoop提供了多种FileSystem的实现：</p>
</li>
</ol>
<ul>
<li>DistributedFileSystem（DFS）：在分布式的环境中，访问HDFS文件</li>
<li>LocalFileSystem:在本地系统中访问HDFS文件</li>
<li>FTPFileSystem：访问HDFS文件的FTP客户端</li>
<li>WebHdfsFileSystem：通过web接口访问HDFS文件。</li>
</ul>
<h4 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h4><p>FileSystem 的常用操作和相关方法：</p>
<ol>
<li>创建文件<ul>
<li><code>booleancreateNewFile(Path f )</code><ul>
<li><strong>不会覆盖</strong>已有文件</li>
<li>创建成功返回true,失败返回false<br>• <code>FSDataOutputStreamcreate(Path f)</code><br>• <strong>覆盖</strong>已有文件<br>• 创建文件并返回输出流<br>• <code>FSDataOutputStreamcreate(Path f,booleanoverwrite)</code><br>• 创建文件并返回输出流<br>• <code>FSDataOutputStreamcreate(Path f,boolean overwrite,int buffer)</code><br>• <code>FSDataOutputStreamcreate(Path f,boolean overwrite,int buffer,short replication,long blockSize)</code></li>
</ul>
</li>
</ul>
</li>
<li>打开文件<ul>
<li><code>FSDataInputStreamopen(Pathf)</code></li>
<li><code>FSDataInputStreamopen(Pathf,intbufferSize)</code><ul>
<li>返回输入流</li>
<li>如果文件不存在会抛出异常</li>
<li>不指定bufferSize时,会从Configuration中读取 io.file.buffer.size,默认为4096字节</li>
</ul>
</li>
</ul>
</li>
<li>文件追加<br>• <code>FSDataOutputStream append(Path f)</code><br>• <code>FSDataOutputStream append(Path f, int bufferSize)</code><br>   • 块不足64M时,会补足到64M<br>   • 块达到64M之前,该块不可见,ls看不到该块新增的大小,也无 法读取<br>   • 不能同时多个writer追加同一个文件</li>
<li>从本地拷贝文件到HDFS<ul>
<li><code>void copyFromLocalFile (Path src, Path dst)</code></li>
<li><code>void moveFromLocalFile (Path src, Path dst)</code></li>
</ul>
</li>
<li>从HDFS拷贝文件到本地<ul>
<li><code>void copyToLocalFile(boolean delsrc,Path src,Path dst)</code></li>
<li><code>void moveToLocalFile (Path src, Path dst)</code></li>
</ul>
</li>
<li>创建目录<ul>
<li>boolean mkdirs (Path f)<br>• boolean mkdirs (Path f, FsPermission permission)<br>• static boolean mkdirs (FileSystem fs, Path dir, FsPermission permission)<br>• 支持多级目录同时创建 (类似mkdir -p)<br>• 默认权限是755<br>• 成功返回true</li>
</ul>
</li>
<li>删除及重命名<ul>
<li><code>boolean delete (Path f, boolean recursive)</code></li>
<li><code>boolean deleteOnExit (f)</code> 当关闭FileSystem时,才会删除</li>
</ul>
</li>
<li>获取文件或目录信息<ul>
<li>FileStatus[ ] listStatus (Path f, PathFilter filter)<br>• FileStatus[ ] listStatus (Path[ ] dir, PathFilter filter)<br>FileStatus信息包括:</li>
<li>绝对路径</li>
<li>文件大小(单位:字节)</li>
<li>文件访问时间</li>
<li>块大小、复制份数</li>
<li>文件所属用户、组、访问权限</li>
</ul>
</li>
<li>设置文件或目录属性<ul>
<li>void setOwner (Path p, String username, String groupname)<ul>
<li>设置文件或目录所属用户及组</li>
<li>参数p指定文件或目录</li>
<li>参数username,设置此文件或目录的所属用户 – 只返回列出指定目录下的文件或目录信息<br>• void setPermission (Path p, FsPermission permission)<br>• 设置文件或目录权限<br>• 参数p指定文件或目录<br>• 参数permission,指定权限,权限同linux权限雷同<br>• void setReplication (Path f, short replication)<br>• 设置文件复制份数<br>• 参数f指定文件<br>• 参数replication指定复制份数</li>
</ul>
</li>
</ul>
</li>
<li>获取文件或目录信息<ul>
<li>void setTimes (Path f, long mtime, long atime)<br>设置文件的修改及访问时间</li>
</ul>
</li>
</ol>
<h3 id="FSDataOutputStream"><a href="#FSDataOutputStream" class="headerlink" title="FSDataOutputStream"></a>FSDataOutputStream</h3><pre><code>FSDataOutputStream的全限定名是`org.apache.hadoop.fs.FSDataOutputStream`。
使用Java向HDFS**写入数据**时需要用到的流。
</code></pre><h3 id="FSDataInputStream"><a href="#FSDataInputStream" class="headerlink" title="FSDataInputStream"></a>FSDataInputStream</h3><pre><code>FSDataInputStream的全限定名是`org.apache.hadoop.fs. FSDataInputStream `。
使用Java向HDFS**读取数据**时需要用到的流。
</code></pre><h3 id="Path"><a href="#Path" class="headerlink" title="Path"></a>Path</h3><pre><code>Path的全限定名为`org.apache.hadoop.fs.Path`
用来封装HDFS的路径。
</code></pre><h4 id="常用的方法"><a href="#常用的方法" class="headerlink" title="常用的方法"></a>常用的方法</h4><p>Path的常有方法有：</p>
<ol>
<li><code>int depth()</code>：返回路径的深度</li>
<li><code>String getName()</code>： 返回路径上最后的资源名称</li>
<li><code>Path getParent()</code>：返回父目录，如果已是根目录则返回null</li>
<li><code>Path suffix(String suffix)</code>：参数suffix给Path增加后缀，返回加完后缀的Path实例；</li>
<li><code>getFileSystem (Configuration conf)</code>：返回该Path所属的文件系统实例</li>
</ol>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><pre><code>Configuration全限定名为
</code></pre><p><code>org.apache.hadoop.conf. Configuration</code>。<br>​<br>Configuration对配置文件的读取顺序：先加载缺省配置文件，再加载用户定义的配置文件；且对于每一个文件只加载一次：第一个在classpath出现的（这个和Linux系统上在匹配命令时候的逻辑是一样的）</p>
<p><em>classpath的目录顺序:</em></p>
<ol>
<li>$HADOOP_CONF_DIR</li>
<li>$JAVA_HOME/lib/tools.jar 如果$HADOOP_HOME目录下有build目录，则添加build下各子目录</li>
<li>$HADOOP_HOME/hadoop-core-*.jar</li>
<li>$HADOOP_HOME/lib/*.jar</li>
<li>用户在hadoop-env.sh中定义的$HADOOP_CLASS_PATH</li>
<li>当前作为hadoop jar …参数提交的JAR包</li>
</ol>
<h4 id="常用方法-1"><a href="#常用方法-1" class="headerlink" title="常用方法"></a>常用方法</h4><pre><code>Configuration的常用方法有:
</code></pre><ol>
<li>static void addDefaultResource(String name)</li>
<li>void addResource(InputStream in)</li>
<li>void addResource (Path file)  本地文件</li>
<li>void addResource(String name)  classpath中的文件</li>
<li>void addResource (URL url)void set(String name, String value)</li>
<li>void setBoolean(String name, boolean value)</li>
<li>void setInt(String name, String value)</li>
<li>void setLong(String name, long value)</li>
<li>void setFloat(String name, float value)</li>
<li>void setIfUnset(String name, String value)</li>
<li>void setBooleanIfUnset(String name, boolean value)</li>
<li>String get(String name)</li>
<li>boolean getBoolean(String name, boolean defaultValue)</li>
</ol>
<h3 id="IOUtils"><a href="#IOUtils" class="headerlink" title="IOUtils"></a>IOUtils</h3><pre><code>IOUtils的全限定名是`org.apache.hadoop.fs. IOUtils `。
IOUtils是一个I/O帮助类,提供的都是静态方法,不需要实例化。
</code></pre><h4 id="常用方法-2"><a href="#常用方法-2" class="headerlink" title="常用方法"></a>常用方法</h4><p>IOUtils的常用方法有：</p>
<ul>
<li>public static void copyBytes (InputStream in , outputStream out,<br>Configuration conf)<br>• public static void copyBytes (InputStream in , outputStream out, Configuration conf, boolean close)</li>
</ul>
<h2 id="开发基本步骤"><a href="#开发基本步骤" class="headerlink" title="开发基本步骤"></a>开发基本步骤</h2><p>使用Java在HDFS上的基本开发步骤如下：</p>
<ol>
<li>实例化<code>Configuration</code></li>
<li>实例化<code>FileSystem</code><ol>
<li>获取相关Stream进行文件／目录操作</li>
</ol>
</li>
</ol>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/11/0811JavaHadoop/" data-id="cj6dixft10004injoujruvqzv" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0811HadoopIO" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/11/0811HadoopIO/">Hadoop IO操作</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/11/0811HadoopIO/" class="article-date"><time datetime="2017-08-11T11:31:25.000Z" itemprop="datePublished">2017-08-11</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Hadoop的IO操作中涉及到几个问题：</p>
<ol>
<li>保持数据完整性</li>
<li>数据压缩</li>
<li>序列化</li>
<li>存储数据的数据结构</li>
</ol>
<h2 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h2><p>​    针对数据在存储过程损坏和丢失的问题，Haddop采取了<strong>数据校验</strong>和<strong>后台进程检测数据块</strong>的方法。<br>​    数据校验采用的是<strong>循环冗余校验CRC-32</strong>。</p>
<h3 id="验证时机"><a href="#验证时机" class="headerlink" title="验证时机"></a>验证时机</h3><ol>
<li>数据节点负责在存储数据及其校验和前验证它们收到的数据（发生于数据块副本传输的时候） （存储时）；</li>
<li>客户端读取数据节点上的数据时候，会验证校验和（读取时）；</li>
<li>每个数据节点有一个后台进程定期验证存在该节点山的所有数据块（定时）；</li>
</ol>
<p>关于验证，涉及到两个类，一个是<code>FileSystem</code>，一个是<code>CheckSumFileSystem</code>。</p>
<h2 id="基于文件的数据结构"><a href="#基于文件的数据结构" class="headerlink" title="基于文件的数据结构"></a>基于文件的数据结构</h2><p>HDFS和MR主要针对大数据文件来设计，在<em>小文件处理上效率低</em>。解决方法是选择一个容器，将这些小文件包装起来， 将整个文件作为一条记录，可以获取更高效率的储存和处理， 避免多次打开关闭流耗费计算资源.hdfs提供了两种类型的容器 <strong>SequenceFile</strong>和<strong>MapFile</strong>。<br><em>一言蔽之，这些文件结构用来解决HDFS存储小文件的痛点。</em></p>
<h3 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h3><p>​    SequenceFile是Hadoop对二进制键／值对持久化的数据结构。<br>​    Sequence的内部结构如下：<br><img src="/2017/08/11/0811HadoopIO/The-internal-structure-of-a-sequence-file-with-no-compression-and-record-compression.png" alt=""></p>
<p>​    Sequence file由一系列的二进制key/value组成，如果key为小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。文件中每条记录是可序列化，可持久化的键值对，提相应的<strong>读写器</strong>和<strong>排序器</strong>，写操作根据压缩的类型分为3种：</p>
<ul>
<li><p>Write 无压缩写数据，</p>
</li>
<li><p>RecordCompressWriter记录级压缩文件，只压缩值</p>
</li>
<li><p>BlockCompressWrite块级压缩文件，键值采用独立压缩方式;</p>
<p>关于压缩的对象，value可以压缩，key不可以压缩。</p>
<h4 id="对SequenceFile的操作方法："><a href="#对SequenceFile的操作方法：" class="headerlink" title="对SequenceFile的操作方法："></a>对SequenceFile的操作方法：</h4><p>​    Hadoop-0.21.0版本开始中提供了SequenceFile，包括Writer，Reader和SequenceFileSorter类进行写， 读和排序操作。</p>
</li>
</ul>
<p>​    该方案对于小文件的存取都比较自由，不限制用户和文件的多少， <strong>支持Append追加写入</strong>，支持三级文档压缩(不压缩、文件级、块级别)。</p>
<h3 id="MapFile"><a href="#MapFile" class="headerlink" title="MapFile"></a>MapFile</h3><p>​    MapFile是<strong>经过排序</strong>过后的<strong>带索引</strong>的SequenceFile可以根据键进行查找。</p>
<p><img src="/2017/08/11/0811HadoopIO/mapFile.png" alt=""></p>
<p>​    一个MapFile ~可以通过SequenceFile的地址，进行分类查找的格式~ 。使用这个格 式的优点在于，首先会将SequenceFile中的地址都加载入内存，并且进行了key 值排序，从而提供更快的数据查找。<br>与SequenceFile只生成一个文件不同，MapFile<strong>生成一个文件夹</strong>。 索引模型按<strong>128个键</strong>建立的，可以通过<code>io.map.index.interval</code>来修改。</p>
<p>​    </p>
<h3 id="MapFile和SequenceFile的比较"><a href="#MapFile和SequenceFile的比较" class="headerlink" title="MapFile和SequenceFile的比较"></a>MapFile和SequenceFile的比较</h3><ol>
<li>SequenceFile文件是用来存储key-value数据的，但它并<strong>不保证这些存储的key-value是有序的</strong>；MapFile文件则可以看做是存储有序key-value的 SequenceFile文件。</li>
<li>MapFile文件保证key-value的有序（基于key）是通过 ~每一次写入 key-value时的检查机制~ ，这种检查机制其实很简单，就是 ~保证当前正要写入的key-value与上一个刚写入的key-value符合设定的顺序~ ；但是，这种有序是由用户来保证的，一旦写入的key-value不符合key的非递减顺序，则会直接报错而不是自动的去对输入的key-value排序；</li>
</ol>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><p>​    引入压缩是为了<strong>减少储存文件所需空间</strong>，还可以<strong>降低其在网络上传输的时间</strong>。</p>
<p>​    此处简单列出Hadoop上的常用的压缩：<br><img src="/2017/08/11/0811HadoopIO/compressionTypes.tiff" alt=""></p>
<p><em>关于切分</em><br>​    举Bzip2为例，Bzip2支持切分splitting.hdfs上文件1GB，如按照默认块64MB，那么这个 文件被分为16个块（1024/64）。如果把这个块放入MR任务 ，将有<strong>16个map任务输入</strong>。 如果算法不支持切分，后果是MR把这个文件作为<strong>一个Map输入</strong>。这样任务减少了，降低了数据的本地性。<br>​    </p>
<h3 id="Hadoop中实现压缩的方法"><a href="#Hadoop中实现压缩的方法" class="headerlink" title="Hadoop中实现压缩的方法"></a>Hadoop中实现压缩的方法</h3><p>​    大致说来有两种途径：CodeC和使用本地库。</p>
<h4 id="Codec"><a href="#Codec" class="headerlink" title="Codec"></a>Codec</h4><p>​    Hadoop中压缩解压类实现<code>CompressionCodec</code>接口<code>createOutputStream</code>来创建一个<code>CompressionOutputStream</code>，将其压缩格式写入底层的流。</p>
<h3 id="本地库"><a href="#本地库" class="headerlink" title="本地库"></a>本地库</h3><p>​    Hadoop使用java开发，但是有些需求和操作并不适合java，所以引入了本地 库 native。可以高效执行某些操作。<br>如果频繁使用原生库做压解压任务，可以使用 codecpool，通过CodecPool的getCompressor方法获得Compressor对象， 需要传入Codec 。这样可以节省创建Codec对象开销 ，允许反复使用。<br>​<br>​    不同的压缩方法性能上各不相同，<strong>根据不同的使用场景来选择最合适的压缩算法</strong>。</p>
<p>各个压缩算法的特点如下：    </p>
<ol>
<li>Gzip 优点<strong>是压缩率高，速度快</strong>。Hadoop支持与直接处理文本一 样。缺点不支持split，当文件压缩在128m内，都可以用gzip（也就是说存在于一个数据块内）；</li>
<li>Izo 优点压缩速度快，具有合理的压缩率，支持split，是<strong>最流行的</strong>压缩格式。支持native库；缺点是比gzip压缩率低，<strong>hadoop本身不支持</strong>，需要安装；在应用中对lzo格式文件需要处理如 指定 inputformat为lzo格式</li>
<li>Snappy压缩 高速压缩率合理支持本地库。不支持split， <strong>hadoop不支持</strong>，要安装linux没有对应命令；当MR输出数据较大， 作为到reduce数据压缩格式</li>
<li>Bzip2 支持split，很高的压缩率，比gzip高，<strong>hadoop支持但不支持native</strong>，linux自带命令使用方便。缺点压缩解压速度慢</li>
</ol>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><h3 id="Hadoop引入序列化的原因"><a href="#Hadoop引入序列化的原因" class="headerlink" title="Hadoop引入序列化的原因"></a>Hadoop引入序列化的原因</h3><ol>
<li>Hadoop在<strong>集群之间通信或者RPC调用时需要序列化</strong>，而且要求序列化要快，且体积要小，占用带宽小；</li>
<li><strong>Java的序列化机制不满足Hadoop的需求</strong>，占用大量计算开销，且序列化结果体积过大;；它的引用机制也导致大文件不能被切分，浪费空间；此外，很难对其他语言进行扩展使用；</li>
<li>java的反序列化过程每次都会<strong>构造新的对象，不能复用对象</strong>。</li>
</ol>
<h3 id="Hadoop的序列化类"><a href="#Hadoop的序列化类" class="headerlink" title="Hadoop的序列化类"></a>Hadoop的序列化类</h3><p>Hadoop实现了大量的序列化类，如下如所示：<br><img src="/2017/08/11/0811HadoopIO/Writable-class-hierarchy.png" alt=""><br>重点关注右上方的部分，基本上是对Java基本类型的封装。<br><img src="/2017/08/11/0811HadoopIO/writable1.tiff" alt=""></p>
<h3 id="Hadoop的序列化接口"><a href="#Hadoop的序列化接口" class="headerlink" title="Hadoop的序列化接口"></a>Hadoop的序列化接口</h3><h4 id="Writable"><a href="#Writable" class="headerlink" title="Writable"></a>Writable</h4><p>​    Hadoop的序列化类是通过实现序列化接口<code>Writable</code>实现的，<code>Writable</code>接口是基于DataInput与DatOutput的简单高效可序列化接口。继承该接口必须实现的两个方法是<code>readField()</code>和<code>write()</code></p>
<h4 id="WritableComparable"><a href="#WritableComparable" class="headerlink" title="WritableComparable"></a>WritableComparable</h4><p>​    同时实现序列化和比较。类似java的Comparable接口，用于类型的比较。MR其中一个阶段叫排序， 默认使用Key来排序。<br>需要同时实现三个方法<code>readField()</code>，<code>write()</code>和<code>compareTo()</code>。</p>
<p>WritableComparable的简单实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> entity;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.DataInput;</div><div class="line"><span class="keyword">import</span> java.io.DataOutput;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 本类是一个Text的二元组&lt;/br&gt;</span></div><div class="line"><span class="comment"> * 实现了WritableComparable接口&lt;/br&gt;</span></div><div class="line"><span class="comment"> * 比较的方式是，首先比较第一个元素，若相同，则比较第二个元素。&lt;/br&gt;</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> hw</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TextPair</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">TextPair</span>&gt; </span>&#123;</div><div class="line">	</div><div class="line">	<span class="keyword">private</span> Text firstText;</div><div class="line">	<span class="keyword">private</span> Text secondText;</div><div class="line">	</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">//因为Text本身也就实现了Writable结构，此时就直接调用二元组内的两个Text的write方法即可。</span></div><div class="line">		<span class="keyword">this</span>.firstText.write(out);</div><div class="line">		<span class="keyword">this</span>.secondText.write(out);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">// TODO Auto-generated method stub</span></div><div class="line">		<span class="keyword">this</span>.firstText.readFields(in);</div><div class="line">		<span class="keyword">this</span>.secondText.readFields(in);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(TextPair o)</span> </span>&#123;</div><div class="line">		<span class="comment">//首先比较firstText，不同则比较secondText</span></div><div class="line">		<span class="keyword">int</span> result = firstText.compareTo(o.getSecondText());</div><div class="line">		<span class="keyword">return</span> result!=<span class="number">0</span>? result: <span class="keyword">this</span>.secondText.compareTo(o.getSecondText());</div><div class="line">	&#125;</div><div class="line">	</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">getFirstText</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">return</span> firstText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFirstText</span><span class="params">(Text firstText)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.firstText = firstText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">getSecondText</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">return</span> secondText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSecondText</span><span class="params">(Text secondText)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.secondText = secondText;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="实现自定义的序列化对象"><a href="#实现自定义的序列化对象" class="headerlink" title="实现自定义的序列化对象"></a>实现自定义的序列化对象</h3><p>​    实现自定义的Hadoop序列化对象，也需要和内置的序列化对象一样，实现<code>Writable</code>接口，其中必须实现的两个方法是<code>readField()</code>和<code>write()</code>。</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/11/0811HadoopIO/" data-id="cj6dixft60005injo3sxajymz" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0810HDFSreadwrite" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/10/0810HDFSreadwrite/">HDFS的读写策略</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/10/0810HDFSreadwrite/" class="article-date"><time datetime="2017-08-10T13:29:25.000Z" itemprop="datePublished">2017-08-10</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>​    向HDFS读写数据的过程中涉及到以下几个部分：</p>
<ol>
<li>客户端节点</li>
<li>客户端节点上的JVM</li>
<li>数据节点</li>
<li>名称节点</li>
</ol>
<h2 id="从HDFS读取数据"><a href="#从HDFS读取数据" class="headerlink" title="从HDFS读取数据"></a>从HDFS读取数据</h2><p><img src="/2017/08/10/0810HDFSreadwrite/a_client_reading_data_from_HDFS.png" alt=""><br>​<br>流程如下：</p>
<ol>
<li>客户端回调用FileSystem的实例<code>DistributedFileSystem</code>的open方法，获得这个文件的<strong>输入流</strong>；</li>
<li>以RPC的方式调用名称节点，以确定文件开头部分块的位置（包括副本的位置，按照与客户端的距离来排序），</li>
<li>客户端按照就近原则，调用输入流的read方法从最近的DataNode读取数据， 若客户端本身就是一个数据节点，则直接从节点上读取（期间将操作日志发送给NameNode）；</li>
<li>客户端读取到数据块末端，将关闭与这个DataNode 的连接，然后<strong>重新查找</strong>下一个数据块。</li>
<li>重复2-4直到当客户端读取完成，<code>DFSInputStream</code>将被关闭；</li>
</ol>
<h2 id="向HDFS写数据"><a href="#向HDFS写数据" class="headerlink" title="向HDFS写数据"></a>向HDFS写数据</h2><p><img src="/2017/08/10/0810HDFSreadwrite/a_client_writing_data_to_HDFS.png" alt=""></p>
<ol>
<li>客户端通过在<code>DistributedFileSystem</code>调用<code>create</code>方法来创建文件；</li>
<li><code>DistributedFileSystem</code>的一个RPC调用NameNode，在文件系统的密码<strong>命名空间</strong>中创建一个新文件；NameNode 将通过一些检查，比如 ~文件是否存在~，客户端 ~是否拥有创建权限~ 等;通过检查之后，在NameNode 添加文件信息。注意，因为此时文件没有数据，所以<strong>NameNode 上也没有文件数据块的信息</strong>。</li>
<li>创建结束之后，HDFS会返回一个输出流 DFSDataOutputStream 给客户端；</li>
<li>客户端调用输出流DFSDataOutputStream的write方法向HDFS 中对应的文件写入数据。</li>
<li>数据首先会被<strong>分包</strong>，这些分包会写人一个<strong>输出流的内部队列</strong> Data 队列中， ~接收完数据分包，输出流DFSDataOutputStream会向NameNode申请保存文件和副本数据块的若干个DataNode ， 这若干个DataNode 会形成一个数据传输管道。~ DFSDataOutputStream 将数据传输给距离上最短的DataNode ，这个DataNode 接收到数据包之后会传给下一个DataNode 。 ~数据在各DataNode之间通过管道流动，而不是全部由输出流分发~， 以减少传输开销。</li>
<li>因为各DataNode位于不同机器上，数据需要通过网络发送，所以，为了保证所有DataNode 的数据都是准确的， ~接收到数据的 DataNode 要向<strong>发送者</strong>发送确认包(ACK Packet )~ 。对于某个数据块，只有当DFSDataOutputStream 收到了所有DataNode 的正确ACK，才能确认传输结束。DFSDataOutputStream 内部专 门维护了一个<strong>等待ACK 队列</strong>，这一队列保存已经进入管道传输数据、但是并未被完全确认的数据包。</li>
<li>重复4-6，DFSDataInputStream 继续等待直到所有数据写入完毕并<strong>被确认</strong>后，调用<code>close</code>关闭流，调用<code>complete</code>方法通知NameNode 文件写入完成。NameNode 接收到complete消息之后， ~等待相应数量的副本写入完毕后~， 告知客户端。</li>
</ol>
<p><em>数据流列表形成一个管线。</em><br><em>一个包只有在被管线中的<strong>所有节点</strong>确认后才会被移出确认队列</em></p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/10/0810HDFSreadwrite/" data-id="cj6dixfsr0000injob1shso3u" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0810HDFoverview" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/10/0810HDFoverview/">HDFS简述</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/10/0810HDFoverview/" class="article-date"><time datetime="2017-08-10T13:29:25.000Z" itemprop="datePublished">2017-08-10</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="什么是HDFS"><a href="#什么是HDFS" class="headerlink" title="什么是HDFS"></a>什么是HDFS</h2><pre><code>HDFS是Hadoop的一个分布式文件系统 (Hadoop Distributed File System)。对外部客户机而言,HDFS 就像一个传统的分级文件系统。可以创建、删除、移动或重命名文件,等等。
HDFS在Hadoop框架中所处的位置如下图所示：
</code></pre><p><img src="/2017/08/10/0810HDFoverview/Hframework.tiff" alt=""></p>
<p><em>关于文件系统</em></p>
<blockquote>
<pre><code>文件系统由三部分组成:与**文件管理有关软件**、 **被管理文件**以及**实施文件管理所需数据结构**。常见的有 Explorer、Total Commander在每台存储设 备里有很多被 管理文件如通用结构, 由超级块、节 点、数据块、 目录块、间接 块组成。  
从系统角度来看,文件系统是对文件存储器空 间进行组织和分配,负责文件存储并对存入的 文件进行保护和检索的系统。  
</code></pre></blockquote>
<h2 id="HDFS所能提供的"><a href="#HDFS所能提供的" class="headerlink" title="HDFS所能提供的"></a>HDFS所能提供的</h2><h3 id="大规模数据分布存储能力"><a href="#大规模数据分布存储能力" class="headerlink" title="大规模数据分布存储能力"></a>大规模数据分布存储能力</h3><p>​    Hadoop能够将超大的数据分块存储到不同的分区上，从而实现存储超大数据的功能。</p>
<h3 id="高并发访问能力"><a href="#高并发访问能力" class="headerlink" title="高并发访问能力"></a>高并发访问能力</h3><p><img src="/2017/08/10/0810HDFoverview/concurrent.tiff" alt=""><br>​    数据存在于多个节点上，读取的时候可以并发读取）</p>
<h3 id="流式文件访问-提供简单的一致性模型"><a href="#流式文件访问-提供简单的一致性模型" class="headerlink" title="流式文件访问,提供简单的一致性模型"></a>流式文件访问,提供简单的一致性模型</h3><p>​    DFS是用<strong>流处理方式</strong>处理文件, 每个文件在系统里都能找到它的本地化映像,所以对于用户来说,别管文件是什么格式的,也不用在意被分到哪里,只管从DFS里取出就可以了。<br>​    一次写入,多次读取。 <u>数据源通常由源生成或从数据源直接复制而来,接着长时间在此数据集上进行各类分析</u>,大数据不需要搬来搬去。</p>
<h3 id="强大的容错能力（冗余存储）"><a href="#强大的容错能力（冗余存储）" class="headerlink" title="强大的容错能力（冗余存储）"></a>强大的容错能力（冗余存储）</h3><p><img src="/2017/08/10/0810HDFoverview/check.tiff" alt=""><br>​    每个分片文件需要分片服务器校验。</p>
<p>​    </p>
<p>​    此处就又有一些问题：</p>
<p>​    文件分布后调用会效率很低吗? 文件分布处理过程丢失了怎 么办?文件种类很多到底分到哪好?<br>​    解决以上问题，HDFS采用分片冗余，本地校验。</p>
<p>​    数据冗余式存储, 直接将多份的分片文件交给分片后的存储服务器去校验 。</p>
<p><img src="/2017/08/10/0810HDFoverview/redundency.tiff" alt=""><br>​    冗余后的分片文件还有个额外功能, <u>只要冗余的分片文件中有一份是完整的,经过多次协同调整后,其他分片文件也将完整</u> 。（自动修正能力）</p>
<h2 id="HDFS的架构"><a href="#HDFS的架构" class="headerlink" title="HDFS的架构"></a>HDFS的架构</h2><p><img src="/2017/08/10/0810HDFoverview/Block.tiff" alt=""></p>
<p>在HDFS这个分布式的文件系统中，存在着<strong>两种节点</strong>，DataNode和NameNode。<br>一个HDFS集群是由<strong>一个</strong>NameNode和<strong>一定数目</strong>的DataNode组成。</p>
<h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>​    NameNode是一个主服务器,用来<u>管理整个 文件系统的命名空间和元数据</u>,以及<u>处理来自外界的文件访问请求</u> 。</p>
<p>​    NameNode 保存了文件系统的三种元数据：<br>• 命名空间, 即<strong>整个分布式文件系统的目录结构</strong>；<br>• 数据块与文件名的映射表；<br>• 每个数据块副本的位置信息,每一个数据块默认有3 个副本；<br>（一言蔽之，DataNode用来<strong>管理客户端访问</strong>，<strong>管理数据</strong>）</p>
<p>​    稍微具体地说，即：<br>​    Namenode执行文件系统的名字空间操作，比如<strong>打开</strong>、<strong>关闭</strong>、<strong>重命名</strong>文件或目录。它也负责<strong>确定数据块到具体Datanode节点的映射</strong>。</p>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>​    稍微具体地说，即：<br>​    DataNode 负责处理文件系统用户具体的数 据读写请求,同时也可以处理NameNode 对数据块的创建、删除副本的指令。</p>
<h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h3><p>​    HFDS的文件以<strong>块</strong>作为基本单位，默认大小设为64M字节。<br>​    HDFS将一个文件分为一个或数个块来存储。</p>
<p>使用数据块有以下的优点：</p>
<ol>
<li>当一个文件大于集群中任意一个磁盘的时候,文件系统可以充分利用集群中所 有的磁盘；</li>
<li>管理块使底层的存储子系统相对简单 ；</li>
<li>块更加适合备份,从而为容错和高可用性的实现带来方便；</li>
<li><p>采用块方式,实现了名字与位置的分离,实现了的存储位置的独立性；</p>
<p>上述优点的<strong>容错</strong>体现在块的<strong>冗余备份</strong>上，简单说来如下：</p>
</li>
</ol>
<ul>
<li>每个块在集群上会存储多 份(replica)，默认是三份；</li>
<li>某个块的所有备份都是同一个ID，这样一来对于块的管理变很便利了，不需要专门去记录某一块是那一份数据的；</li>
<li><p>系统可以根据机架的配置自动分配备份位置；也就是说，某一个数据块存于某个机架上的节点上，那么另外的几份备份会被存在另外的机架的两个节点上；</p>
<p>此外，在关于数据校验的设计上，每个块会在本地文件系统产生两个文件,一个是<strong>实际的数据文件</strong>,另一个是<strong>块的附加信息文件,其中包括数据的校验和</strong>。</p>
</li>
</ul>
<h3 id="元数据-－－-NameNode的账目"><a href="#元数据-－－-NameNode的账目" class="headerlink" title="元数据  －－  NameNode的账目"></a>元数据  －－  NameNode的账目</h3><p>HDFS的元数据包括：</p>
<ul>
<li>文件系统目录树信息<br>• 文件名,目录名<br>• 文件和目录的从属关系<br>• 文件和目录的大小,创建及最后访问时间<br>• 权限</li>
<li>文件和块的对应关系<br>• 文件由哪些块组成</li>
<li>块的存放位置<br>• 机器名,块ID</li>
</ul>
<h4 id="HDFS对元数据和实际数据的存储方法"><a href="#HDFS对元数据和实际数据的存储方法" class="headerlink" title="HDFS对元数据和实际数据的存储方法"></a>HDFS对元数据和实际数据的存储方法</h4><p>​    元数据存储在一台指定的服务器上，而且是存在<strong>内存</strong>中的(<strong>NameNode</strong>)<br>​    实际数据储存在集群的其他机器的本地文件系统中 (<strong>DataNode</strong>)</p>
<h2 id="访问HDFS的方法"><a href="#访问HDFS的方法" class="headerlink" title="访问HDFS的方法"></a>访问HDFS的方法</h2><p>采用命令行客户端，或者使用相关的API客户端。</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/10/0810HDFoverview/" data-id="cj6dixfsw0001injon2pb58s4" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-cap" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/07/cap/">cap理论</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/07/cap/" class="article-date"><time datetime="2017-08-07T13:03:25.000Z" itemprop="datePublished">2017-08-07</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="CAP理论概述"><a href="#CAP理论概述" class="headerlink" title="CAP理论概述"></a>CAP理论概述</h3><p>C: Consistency 一致性<br>A: Availability 可用性<br>P: Partition tolerance 分区容错性</p>
<p>该理论认为，一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这<strong>三项中的两项</strong>。</p>
<p>（数据）一致性：<br>all nodes see the same data at the same time，即更新操作成功并返回客户端完成后，<strong>所有节点</strong>在<strong>同一时间</strong>的<strong>数据完全一致</strong></p>
<p> （服务）可用性：<br>Reads and writes always succeed，也就是说，服务必须<strong>一直可用</strong>，而且是正常响应时间。</p>
<p>（系统）分区容错性：<br>the system continues to operate despite arbitrary message loss or failure of part of the system，即分布式系统在~遇到某节点或网络分区故障~的时候，仍然能够对外提供满足<strong>一致性</strong>和<strong>可用性</strong>的服务。</p>
<p>参考：<a href="http://www.hollischuang.com/archives/666" target="_blank" rel="external">分布式系统的CAP理论-HollisChuang’s Blog</a></p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/07/cap/" data-id="cj6dixft90006injohrrxo2ib" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-test" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/07/test/">第一天作业笔记</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/07/test/" class="article-date"><time datetime="2017-08-07T10:42:19.000Z" itemprop="datePublished">2017-08-07</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="数据的单位：-K-M-G-T-P-E-Z-Y-D-N"><a href="#数据的单位：-K-M-G-T-P-E-Z-Y-D-N" class="headerlink" title="数据的单位： K M G T P E Z Y D N"></a>数据的单位： K M G T P E Z Y D N</h3><p>大数据设计到的数据单位有下：<br>B ，KB，MB，GB，TB，PB，EB，ZB，YB<br>其中，从左到右，单位之间的关系如下：<br>1B     = 8 bites<br>1KB    =  1024B<br>1MB=  1024KB<br>1GB    =  1024MB<br>1TB    =  1024GB<br>1PB    =  1024TB<br>1EB    =  1024PB<br>1ZB    =  1024EB<br>1YB    =  1024ZB<br>1DB    =  1024YB<br>1NB    =  1024DB</p>
<hr>
<h3 id="什么是大数据？"><a href="#什么是大数据？" class="headerlink" title="什么是大数据？"></a>什么是大数据？</h3><blockquote>
<p>Big data is a term for <strong>data sets</strong> that are ~so large or complex that traditional data processing application software is inadequate to deal with them~.<br><a href="http://www.webopedia.com/TERM/S/structured_data.html" target="_blank" rel="external">What is Structured Data? Webopedia Definition</a><br>根据维基百科的对大数据的定义，可以看到大数据是一个关于数据集的术语。<br>本身足够大或者足够复杂以至于传统的数据处理应用软件不足够处理它们。</p>
</blockquote>
<ol>
<li>数据类型： 结构化的， 非结构化的</li>
</ol>
<p>结构化数据结构:</p>
<blockquote>
<p>Structured data refers to any data that <strong>resides in a fixed field</strong> within a record or file. This includes data contained in <strong>relational databases</strong> and <strong>spreadsheets</strong>.<br>​    也就是说，结构化的数据是行数据，存储在数据库里,可以用二维表结构来逻辑表达实现的数据。</p>
</blockquote>
<p>非结构话的数据结构:</p>
<blockquote>
<p>Unstructured data is all those things that <strong>can’t be so readily classified and fit into a neat box</strong>: photos and graphic images, videos, streaming instrument data, webpages, PDF files, PowerPoint presentations, emails, blog entries, wikis and word processing documents.<br>​    也即是说，非结构化的数据大多指的是办公文档、文本、图片、XML、HTML、各类报表、图像和音频/视频信息等等数据。</p>
</blockquote>
<hr>
<h3 id="大数据的4个V"><a href="#大数据的4个V" class="headerlink" title="大数据的4个V"></a>大数据的4个V</h3><p>大数据的四个V包括：</p>
<ol>
<li>Velocity：实现<strong>快速的数据流传</strong></li>
<li>Variety： 具有<strong>多样的数据类型</strong></li>
<li>Volume： 存有<strong>海量的数据规模</strong>（TB，PB，EB级别）</li>
<li>Value：存在着<strong>巨大的价值</strong></li>
</ol>
<hr>
<h3 id="大数据的工作流程"><a href="#大数据的工作流程" class="headerlink" title="大数据的工作流程"></a>大数据的工作流程</h3><ul>
<li>采集数据<br>数据的类型多种多样。可以是各大网络上每天产生的数据，可以是各种传感器所产生的数据，可以是科研或者实验日志。<br>采集的方法一般有以下几种：<ul>
<li>系统日志采集方法，即在系统内部使用专用的数据采集工具来采集日志</li>
<li>网络数据采集方法：通过网络爬虫或者网站提供的API来获取数据。这种方法多用来收集非结构化数据；</li>
</ul>
</li>
<li>清洗数据ETL</li>
<li>ETL的概念<br> ETL表示<code>Extract Transform Load</code>，也就是抽取，转换，装载的过<br> 程，是构建<strong>数据仓库</strong>的一个重要环节；</li>
<li>数据清理<br> 数据清洗（data cleansing/data cleaning/data scrubing）是一个<strong>减少错误</strong>和<strong>不一致性</strong>、<strong>解决对象识别</strong>的过程。<br> 也就是说，在数据清理结果，将通过ETL处理过程，减少原数据的错误和不一致型，从业有利于下一阶段的分析；</li>
<li>分析数据<br>就是对清洗过后的数据进行分析和挖掘。<br>利用分布式数据库，或者分布式计算集群来对存储于其内的海量数据进行普通的分析和分类汇总等，以满足大多数常见的分析需求。<br>一般来说，本阶段会有<strong>数据量大</strong>，其对<strong>系统资源，特别是I/O</strong>会有极大的占用等问题。</li>
<li>呈现<br>也就是将分析后的数据进行呈现。<br>一般采用各种可视化工具，将数据转化为图表等形式进行呈现。<br>当前可用的可视化工具有：Tableau，ChartBlocks，Datawrapper，Plotly，RAW等等；</li>
</ul>
<hr>
<h3 id="计算模式"><a href="#计算模式" class="headerlink" title="计算模式"></a>计算模式</h3><ul>
<li>批处理<br>批处理就是对某对象进行批量的处理。<br>在大数据处理中，最适合的批处理是<strong>MapReduce</strong>。<br>简单的讲，MapReduce对具有<strong>简单数据关系</strong>，<strong>易于划分</strong>的大规模数据采用<strong>分而治之</strong>的方式进行<strong>并行处理</strong>。本身是一个~单输入、两阶段( Map 和Reduce) 的数据处理过程~。</li>
<li>流计算<br>流计算对一定<strong>时间窗口内</strong>应用系统产生的新数据完成<strong>实时的计算</strong>，避免造成数据堆积和丢失。<br>流计算能够较好地解决MapReduce模型存在的延迟大的问题。<br>使用场景有：<br>统计网站中每一个页面，域名的点击次数<br>内部系统的运行监控（统计被监控服务器的运行状态）<br>记录最大值和最小值<br><a href="http://yangxiaowei.cn/wordpress/?p=551" target="_blank" rel="external">关于大数据下流计算的一些问题</a></li>
<li>迭代计算<br>迭代计算能够解决批量计算的难以迭代的缺陷，该计算模式通常用于处理大规模的科学计算。<br>相关的框架有：Apache Giraph，HaLoop，Twister等等。</li>
<li>交互式处理<br>特点：<ul>
<li>系统与操作人员以人机对话的方式<strong>一问一答</strong>；</li>
<li>操作人员提出请求,数据以对话的方式输入,系统便提供相应的数据或提示信息,引导操作人员<strong>逐步</strong>完成所需的操作,直至获得最后处理结果；</li>
<li>存储在系统中的数据文件<strong>能够被及时处理修改</strong>,同时处理<strong>结果可以立刻被使用</strong>；<br>典型系统：Dremel、spark</li>
</ul>
</li>
<li>图计算<br>“图计算”是以“<strong>图论</strong>”为基础的对现实世界的一种“图”结构的抽象表达，以及在这种数据结构上的计算模式。<br>引入图计算是因为图数据结构很好的表达了<strong>数据之间的关联性</strong>( dependencies between data )，关联性计算是大数据计算的核心——~通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用的信息~。比如，通过为购物者之间的关系建模，就能很快找到口味相似的用户，并为之推荐商品；或者在社交网络中，通过传播关系发现意见领袖。<br>典型的系统包括Google 公司的Pregel 、Facebook Giraph 、Spark 下的GraphX；<br><a href="http://www.csdn.net/article/1970-01-01/2825748" target="_blank" rel="external">如何利用“图计算”实现大规模实时预测分析-CSDN.NET</a></li>
<li>内存计算<br>内存计算是以<strong>大数据为中心</strong>、依托计算机硬件的发展、依靠新型的软件体系结构,即,通过对体系结构及编程模型等进行重大革新,~将数据装入内存中处理,而尽量避免 I/O 操作的一种新型的以数据为中心~的并行计算模式.<a href="http://www.jos.org.cn/ch/reader/create_pdf.aspx?file_no=5103&amp;journal_id=jos" target="_blank" rel="external">内存计算技术研究综述</a></li>
</ul>
<hr>
<h3 id="数据库类型："><a href="#数据库类型：" class="headerlink" title="数据库类型："></a>数据库类型：</h3><p><a href="http://www.jianshu.com/p/107c6b045245" target="_blank" rel="external">超全的数据库分类介绍 - 简书</a></p>
<ul>
<li>列存储（Column-oriented）数据库<br>列存储数据库将数据存储在<strong>列族</strong>中，一个列族存储<strong>经常被一起查询的相关数据</strong>，比如人类，我们经常会查询某个人的姓名和年龄，而不是薪资。这种情况下姓名和年龄会被放到一个列族中，薪资会被放到另一个列族中。<br>这种数据库通常用来应对分布式存储海量数据。<br>典型产品：Cassandra、HBase。</li>
<li>基于文档（mongoldb）<br>文档型数据库的灵感是来自于Lotus Notes办公软件，而且它同第一种键值数据库类似。该类型的数据模型是<strong>版本化的文档</strong>，<strong>半结构化的文档</strong>以<strong>特定的格式</strong>存储，比如JSON。~文档型数据库可以看作是键值数据库的升级版，允许之间嵌套键值~。而且文档型数据库比键值数据库的查询效率更高。<br>面向文档数据库会将数据以文档形式存储。~每个文档都是<strong>自包含</strong>的数据单元，是一系列数据项的集合。~每个数据项都有一个名词与对应值，值既可以是简单的数据类型，如字符串、数字和日期等；也可以是复杂的类型，如有序列表和关联对象。数据存储的<strong>最小单位</strong>是<strong>文档</strong>，同一个表中存储的文档属性可以是不同的，数据可以使用XML、JSON或JSONB等多种形式存储。<br>典型产品：MongoDB、CouchDB</li>
<li>基于内存<br>内存数据库是指一种将<strong>全部内容存放在内存</strong>中，而非传统数据库那样存放在外部存储器中的数据库。内存数据库指的是所有的数据访问控制都在内存中进行，这是与磁盘数据库相对而言的。<br>典型产品：Redis</li>
</ul>
<hr>
<h3 id="分布式系统（拜占庭将军问题）"><a href="#分布式系统（拜占庭将军问题）" class="headerlink" title="分布式系统（拜占庭将军问题）"></a>分布式系统（拜占庭将军问题）</h3><p>引用一下Distributed Systems Concepts and Design（Third Edition）中的一句话：<br>A distributed system is one in which components <strong>located at networked computers</strong> communicate and coordinate their actions <strong>only by passing messages</strong>。<br>从此得出两个要点：</p>
<ul>
<li>组件分布与网络计算机；</li>
<li>仅通过消息传递来通信和协调；</li>
</ul>
<p>在分布式系统中有一个著名的问题：拜占庭将军问题。<br>此处引用巴比特网的一边文章简单介绍：<br>有以下几个要点需要注意：</p>
<ul>
<li>问题假设了消息的信道是没有问题的（当考虑了信道是有问题的，则涉及到另一个问题了<strong>两军问题</strong>）；</li>
<li><strong>一致性</strong>和<strong>正确性</strong>是该问题的要求；</li>
<li>结论是：若叛徒数为m，当将军总数n至少为3m+1时，问题可解；</li>
</ul>
<p><a href="http://www.8btc.com/baizhantingjiangjun" target="_blank" rel="external">拜占庭将军问题深入探讨 | 巴比特</a></p>
<hr>
<h3 id="关于CDH"><a href="#关于CDH" class="headerlink" title="关于CDH"></a>关于CDH</h3><p>CDH全称为Cloudera’s Distribution Including Apache Hadoop。<br>CDH首先是100%开源，基于Apache协议。基于Apache Hadoop和相关projiect开发。可以做批量处理，交互式sql查询和及时查询，基于角色的权限控制。在企业中使用最广的hadoop分发版本。</p>
<p>CHD和Hadoop的关系类似于 Red Hat 之于Linux。</p>
<hr>
<h3 id="Hadoop技术栈"><a href="#Hadoop技术栈" class="headerlink" title="Hadoop技术栈"></a>Hadoop技术栈</h3><ol>
<li>hdfs<br>Hadoop的分布式文件系统，提供数据存储的功能。</li>
<li>mapreduce<br>Hadoop 的核心。是一个可以对大量数据进行分布式处理的软件框架，基于Map/Reduce技术。</li>
<li>hive<br>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。<br><a href="https://baike.baidu.com/item/hive/67986" target="_blank" rel="external">hive（数据仓库工具）_百度百科</a></li>
<li>hbase<br>HBase是一个开源的非关系型分布式数据库（NoSQL），它参考了谷歌的BigTable建模，实现的编程语言为Java。它是Apache软件基金会Hadoop项目的一部分，运行于HDFS文件系统之上，为Hadoop提供类似于BigTable规模的服务。<br>用于改善<strong>数据的访问</strong>。<br><a href="https://www.bing.com/knows/search?q=hbase&amp;mkt=zh-cn" target="_blank" rel="external">hbase - Bing 网典</a></li>
<li>Sqoop<br>主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。<br>主要用于<strong>其他数据库</strong>和<strong>Hadoop中的数据库</strong>举行数据传输。</li>
<li>Zookeeper<br>zookeeper作为一个开源的分布式应用协调系统，已经用到了许多分布式项目中，用来完成<strong>统一命名服务</strong>、<strong>状态同步服务</strong>、<strong>集群管理</strong>、<strong>分布式应用配置项的管理</strong>等工作。<br>换句话说，Zookeeper主要用来对分布式项目进行管理。</li>
<li>Mahout<br>Mahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。Mahout包含许多实现，包括聚类、分类、推荐过滤、频繁子项挖掘。此外，通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到云中。<br><a href="https://baike.baidu.com/item/mahout" target="_blank" rel="external">mahout_百度百科</a></li>
</ol>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/07/test/" data-id="cj6dixfti000cinjox62qcu82" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>


    </footer>
  </div>
  
</article>



  




        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <p></p>

</div>


  
  <div class="sidebar-module">
    <h4>Categories</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/大数据/">大数据</a><span class="sidebar-module-list-count">8</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tags</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/HDFS/">HDFS</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/MapReduce/">MapReduce</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/代码/">代码</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/分布式系统/">分布式系统</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/大数据/">大数据</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tag Cloud</h4>
    <p class="tagcloud">
      <a href="/tags/HDFS/" style="font-size: 16.67px;">HDFS</a> <a href="/tags/MapReduce/" style="font-size: 13.33px;">MapReduce</a> <a href="/tags/代码/" style="font-size: 10px;">代码</a> <a href="/tags/分布式系统/" style="font-size: 20px;">分布式系统</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a>
    </p>
  </div>


  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/08/">August 2017</a><span class="sidebar-module-list-count">9</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2017/08/15/hadoopMRAdv/">hadoop高级</a>
        </li>
      
        <li>
          <a href="/2017/08/15/wordcount/">wordcount的编写</a>
        </li>
      
        <li>
          <a href="/2017/08/14/Hadoop-MapReduce/">Hadoop_MapReduce</a>
        </li>
      
        <li>
          <a href="/2017/08/11/0811JavaHadoop/">使用Java对HDFS进行文件操作</a>
        </li>
      
        <li>
          <a href="/2017/08/11/0811HadoopIO/">Hadoop IO操作</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2017 H.W.Huang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>



<script src="/js/script.js"></script>

</body>
</html>
