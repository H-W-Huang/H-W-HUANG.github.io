<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>HWHuang的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="HWHuang的博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="HWHuang的博客">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HWHuang的博客">
  
    <link rel="alternate" href="/atom.xml" title="HWHuang的博客" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/styles.css">
  

</head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class="active"
                 href="/index.html">Home</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">HWHuang的博客</h1>
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          
  
    <article id="post-test-1" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/29/test-1/">test</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/29/test-1/" class="article-date"><time datetime="2017-08-29T11:52:23.000Z" itemprop="datePublished">2017-08-29</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/29/test-1/" data-id="cj6xjfezy000ptsjoo0dg6ife" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-hbase" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/29/hbase/">HBase 简介</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/29/hbase/" class="article-date"><time datetime="2017-08-29T11:43:13.000Z" itemprop="datePublished">2017-08-29</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="什么是HBase"><a href="#什么是HBase" class="headerlink" title="什么是HBase"></a>什么是HBase</h2><p>​    HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase<strong>基于列的而不是基于行</strong>的模式。</p>
<p><img src="/2017/08/29/hbase/b3b7d0a20cf431ad8b01445e4b36acaf2fdd9881.jpg" alt=""></p>
<h2 id="HBase的特点"><a href="#HBase的特点" class="headerlink" title="HBase的特点"></a>HBase的特点</h2><p>HBase有以下特点：</p>
<ol>
<li>大（在一个表中可以有数十亿行，上百万列）</li>
<li>无模式</li>
<li>面向列</li>
<li>稀疏方式存储</li>
<li>数据多版本（每个单元的数据可以有<strong>多个版本</strong>）</li>
<li>数据类型单一（全部是String）</li>
</ol>
<p><em>关于按行存储和按列存储</em><br>​    按行存储，以传统的 RDBMS 为例，为某一个表插入某一条数据时，插入的基本单位是<strong>行</strong>；<br>​    按列存储，为表格插入某一条数据时，插入的基本单位是<strong>列</strong>。<br>HBase 本身可以理解为一个大的 Map ，按照Key和Value来存数据。</p>
<h2 id="HBase的组成"><a href="#HBase的组成" class="headerlink" title="HBase的组成"></a>HBase的组成</h2><p>HBase有以下的基本概念：</p>
<p><img src="/2017/08/29/hbase/Base基本概念.png" alt=""></p>
<ol>
<li>Row Key：<br>本身是一个 <strong>Byte Array</strong> 。作为表中记录的<strong>主键</strong>。</li>
<li>Column Family：<br>有一个字符串名称，含有一个或者多个列。</li>
<li>Column：<br>从属于某一个 Column Family ，列名称是编码在Cell中的；</li>
<li>Version Number：<br>每一个 row Key 的 Version Number 是唯一的；<br>默认值为：<strong>时间戳</strong>；<br>类型：Long</li>
<li>Value<br>以 Byte Array 的形式存储。</li>
</ol>
<p>HBase的结构<br><img src="/2017/08/29/hbase/HBaseTables.png" alt=""></p>
<h2 id="HBase架构"><a href="#HBase架构" class="headerlink" title="HBase架构"></a>HBase架构</h2><p>​    在 HBase 的集群中主要由 Master 和 Region Server 组成，以及 Zookeeper，具体模块如下图所示。</p>
<p><img src="/2017/08/29/hbase/image002.png" alt=""></p>
<p>​    可见，HBase是建立在HDFS上的。</p>
<h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><p>HBase Master 用于</p>
<ol>
<li><strong>协调</strong>多个 Region Server；</li>
<li><strong>侦测</strong>各个 Region Server 之间的状态；</li>
<li><strong>平衡</strong> Region Server 之间的负载；</li>
<li><strong>分配</strong> Region 给 Region Server；</li>
<li><strong>DDL</strong>操作；<br><u>HBase 允许多个 Master 节点共存</u> ，但是这需要 Zookeeper 的帮助。不过当多个 Master 节点共存时，只有一个 Master 是提供服务的，其他的 Master 节点处于待命的状态。当正在工作的 Master 节点宕机时，其他的 Master 则会接管 HBase 的集群。</li>
</ol>
<h3 id="Region-Server"><a href="#Region-Server" class="headerlink" title="Region Server"></a>Region Server</h3><p>​    对于一个 Region Server 而言，其 ~包括了多个 Region~ 。Region Server 的作用只是<strong>管理表格</strong>，以及实现<strong>读写操作</strong>。Client 直接连接 Region Server，并通信获取 HBase 中的数据。</p>
<h3 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h3><p>​    [image:F70C090E-38E1-4A96-B349-2BCD779FDD49-4784-000046EBEFB38D5B/Region.png]</p>
<p>​    对于 Region 而言，则是真实存放 HBase 数据的地方，也就说 ~Region 是 HBase 可用性和分布式的基本单位~ 。如果当一个表格很大，并由多个 CF 组成时，那么表的数据将存放在多个 Region 之间，并且在每个 Region 中会关联多个存储的单元（Store）。    </p>
<p>​    每个 Region 包含着多个 Store 对象。每个 Store 包含一个 MemStore，和一个或多个 HFile。</p>
<h4 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h4><p>​    </p>
<p>​    Zookeeper 是作为 HBase Master 的 HA 解决方案。也就是说，是 Zookeeper 保证了至少有一个 HBase Master 处于运行状态。并且 Zookeeper 负责 Region 和 Region Server 的注册。其实 Zookeeper 发展到目前为止，已经成为了分布式大数据框架中容错性的标准框架。不光是 HBase，几乎所有的分布式大数据相关的开源框架，都依赖于 Zookeeper 实现 HA（High Available）。    </p>
<p>​    HBase Master 与 Region Server 之间的关系是依赖 Zookeeper 来维护的。</p>
<p><a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-bigdata-hbase/index.html" target="_blank" rel="external">HBase 深入浅出</a></p>
<h2 id="HBase的工作流程"><a href="#HBase的工作流程" class="headerlink" title="HBase的工作流程"></a>HBase的工作流程</h2><p><img src="/2017/08/29/hbase/image003.png" alt=""></p>
<p>👉 Client -&gt;   Region Server:</p>
<p>​    当一个 Client 需要访问 HBase 集群时，Client 需要先和 Zookeeper 来通信，然后才会找到对应的 Region Server。</p>
<p>👉  Region Server -&gt; Region:</p>
<p>​    每一个 Region Server 管理着很多个 Region。 ~对于 HBase 来说，Region 是 HBase 并行化的基本单元。~ 因此，数据也都存储在 Region 中。这里我们需要特别注意， ~每一个 Region 都<strong>只存储一个</strong> Column Family 的数据，并且是该 CF 中的一段（按 Row 的区间分成多个 Region）~ 。Region 所能存储的数据大小是有<strong>上限</strong>的，当达到该上限时（Threshold），Region 会进行<strong>分裂</strong>，数据也会分裂到多个 Region 中，这样便可以提高数据的并行化，以及提高数据的容量。每个 Region 包含着多个 Store 对象。每个 Store 包含一个 MemStore，和一个或多个 HFile。MemStore 便是数据在<strong>内存中的实体</strong>，并且一般都是<strong>有序的</strong>。</p>
<p>👉  Region -&gt; HDFS:</p>
<p>​    当数据向 Region 写入的时候，会先写入 MemStore。<br>​    当 MemStore 中的数据需要向底层文件系统倾倒（Dump）时（例如 MemStore 中的数据体积到达 MemStore 配置的最大值），Store 便会创建         StoreFile，而 StoreFile 就是对 HFile 一层封装。<br>​    所以 MemStore 中的数据会最终写入到 HFile 中，也就是磁盘 IO。<br>​    由于 HBase 底层依靠 HDFS，因此 HFile 都存储在 HDFS 之中。</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/29/hbase/" data-id="cj6xjfezl000ftsjo4flfcm27" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/">HBase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NoSQL/">NoSQL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据存储/">数据存储</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-hiveIntro" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/18/hiveIntro/">hive简介</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/18/hiveIntro/" class="article-date"><time datetime="2017-08-18T01:12:45.000Z" itemprop="datePublished">2017-08-18</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Hive是什么"><a href="#Hive是什么" class="headerlink" title="Hive是什么"></a>Hive是什么</h2><pre><code>hive是**基于Hadoop**的一个**数据仓库工具**，可以将**结构化的数据文件**映射为一张数据库表，并提供简单的**sql查询**功能，可以 ~将sql语句转换为MapReduce任务进行运行~ 。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。
</code></pre><h3 id="Hadoop和Hive的关系"><a href="#Hadoop和Hive的关系" class="headerlink" title="Hadoop和Hive的关系"></a>Hadoop和Hive的关系</h3><pre><code>下图展示Hadoop和Hive之间的关系。
</code></pre><p><img src="/2017/08/18/hiveIntro/HiveAndHadoop.tiff" alt=""></p>
<h3 id="Hive在Hadoop技术栈中所处的位置"><a href="#Hive在Hadoop技术栈中所处的位置" class="headerlink" title="Hive在Hadoop技术栈中所处的位置"></a>Hive在Hadoop技术栈中所处的位置</h3><p><img src="/2017/08/18/hiveIntro/HiveInStack.tiff" alt=""></p>
<h2 id="Hive的架构"><a href="#Hive的架构" class="headerlink" title="Hive的架构"></a>Hive的架构</h2><pre><code>Hive的架构如下图所示：
</code></pre><p><img src="/2017/08/18/hiveIntro/HiveArch.tiff" alt=""></p>
<p><img src="/2017/08/18/hiveIntro/HiveArch2.tiff" alt=""></p>
<p>Hive主要包括以下四个部分：</p>
<ul>
<li>用户接口，包括 CLI，JDBC/ODBC，WebUI<br>• 元数据存储(metastore)， hive 的元数据结构描述信息库，可选用不同的关系型数据库来存储，通过配置文件修改、查看数据库配置信息。默认存储在自带的数据库derby中，线上使用时一般<br>换为MySQL；<br>• 驱动器(Driver)，解释器、编译器、优化器、执行器；解释器、编译器、优化器完成HQL查询语句从<strong>词法分析</strong>、<strong>语法分析</strong>、<strong>编译</strong>、<strong>优化</strong>以及<strong>查询计划的生成</strong>。生成的查询计划存储在HDFS中，并在随后<strong>由MapReduce调用</strong>执行。<br>• Hadoop，用MapReduce 进行计算，用 HDFS 进行存储；</li>
</ul>
<h2 id="Hive的数据存储"><a href="#Hive的数据存储" class="headerlink" title="Hive的数据存储"></a>Hive的数据存储</h2><pre><code>Hive没有专门的数据存储格式，也没有为数据建立索引，用户可以非常自由的组织 Hive 中的表。（也也为不能建立索引，才需要使用Bucket）
Hive 中**所有**的数据都**存储在 HDFS** 中，Hive中包含以下**四种数据模型**:Table，External Table，Partition，Bucket。
Hive 中的 Table 和数据库中的 Table 在概念上 是类似的，**每一个 Table 在 Hive 中都有一个相应的目录存储数据**。
</code></pre><h3 id="四种数据模型"><a href="#四种数据模型" class="headerlink" title="四种数据模型"></a>四种数据模型</h3><h4 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h4><h4 id="partition"><a href="#partition" class="headerlink" title="partition"></a>partition</h4><pre><code>Hive 表中的一个 Partition 对应于表下的一个目录,所有的Partition 的数据**都存储在对应的目录中**。
</code></pre><h4 id="Buckets"><a href="#Buckets" class="headerlink" title="Buckets"></a>Buckets</h4><pre><code>Buckets 对指定列计算 hash,根据 hash 值**切分数据**,每一个 Bucket 对应一个文件。可用于 采样:
</code></pre><h4 id="External-Table"><a href="#External-Table" class="headerlink" title="External Table"></a>External Table</h4><pre><code>External Table 指向**已经在 HDFS 中存在的数据**, 可以创建 Partition。
</code></pre><p>​    </p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/18/hiveIntro/" data-id="cj6xjfezu000mtsjogoyqdbxy" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-MyTopKClass" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/16/MyTopKClass/">MyTopKClass</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/16/MyTopKClass/" class="article-date"><time datetime="2017-08-16T00:30:08.000Z" itemprop="datePublished">2017-08-16</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>编写MR任找出前3个最大的数字:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> test;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.DataInput;</div><div class="line"><span class="keyword">import</span> java.io.DataOutput;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.net.URI;</div><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.Iterator;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">//并非每个操作都需要reduce</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTopKClass</span> </span>&#123;</div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span>  String INPUT_PATH = <span class="string">"hdfs://192.168.178.128:9000/input/topk.txt"</span>;</div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span>  String OUTPUT_PATH = <span class="string">"hdfs://192.168.178.128:9000/output03"</span>;</div><div class="line">	</div><div class="line"></div><div class="line">	</div><div class="line">	<span class="comment">// mapper</span></div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTopKMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">NullWritable</span>&gt;</span>&#123;</div><div class="line">		<span class="keyword">int</span>[] a = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">3</span>];</div><div class="line">		</div><div class="line">		<span class="meta">@Override</span></div><div class="line">		<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, IntWritable, NullWritable&gt;.Context context)</span></span></div><div class="line"><span class="function">				<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">			<span class="comment">// TODO Auto-generated method stub</span></div><div class="line">			String line = value.toString().trim();</div><div class="line">			<span class="keyword">if</span>(line.length()!=<span class="number">0</span>)&#123;</div><div class="line">				Arrays.sort(a);</div><div class="line">				<span class="keyword">int</span> input = Integer.parseInt(line);</div><div class="line">				a[<span class="number">0</span>] = a[<span class="number">0</span>] &lt; input? input : a[<span class="number">0</span>];</div><div class="line">			&#125;</div><div class="line">			System.out.println(<span class="string">"mapping"</span>);</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		</div><div class="line">		<span class="meta">@Override</span></div><div class="line">		<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Mapper&lt;LongWritable, Text, IntWritable, NullWritable&gt;.Context context)</span></span></div><div class="line"><span class="function">				<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> i : a)&#123;</div><div class="line">				System.out.println(<span class="string">"in map(),writing "</span>+i);</div><div class="line">				context.write(<span class="keyword">new</span> IntWritable(i),NullWritable.get());</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">//尝试使用reducer使其倒序输出</span></div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTopKReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>, <span class="title">NullWritable</span>, <span class="title">IntWritable</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line">		</div><div class="line">		<span class="keyword">int</span>[] nums = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">3</span>];</div><div class="line">		<span class="keyword">int</span> i= <span class="number">0</span>;</div><div class="line">		</div><div class="line">		<span class="meta">@Override</span></div><div class="line">		<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable arg0, Iterable&lt;NullWritable&gt; arg1,</span></span></div><div class="line"><span class="function"><span class="params">				Reducer&lt;IntWritable, NullWritable, IntWritable, IntWritable&gt;.Context arg2)</span></span></div><div class="line"><span class="function">						<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">			nums[i++] = arg0.get();</div><div class="line">		&#125;	</div><div class="line">		</div><div class="line">		<span class="meta">@Override</span></div><div class="line">		<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Reducer&lt;IntWritable, NullWritable, IntWritable, IntWritable&gt;.Context context)</span></span></div><div class="line"><span class="function">				<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">			Arrays.sort(nums);</div><div class="line">			<span class="keyword">int</span> topk = <span class="number">1</span> ;</div><div class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> i = nums.length-<span class="number">1</span> ; i &gt;= <span class="number">0</span> ; i--) &#123;</div><div class="line">				context.write(<span class="keyword">new</span> IntWritable(topk++), <span class="keyword">new</span> IntWritable(nums[i]));</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	</div><div class="line">	</div><div class="line">	</div><div class="line">	</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		</div><div class="line">		<span class="comment">// 作业前的准备</span></div><div class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">		FileSystem fs = FileSystem.get(URI.create(OUTPUT_PATH),conf);</div><div class="line">		<span class="comment">// 检查目录是否存在，是则但删除之</span></div><div class="line">		<span class="keyword">if</span>(fs.exists(<span class="keyword">new</span> Path(OUTPUT_PATH)))&#123;</div><div class="line">			fs.delete(<span class="keyword">new</span> Path(OUTPUT_PATH));</div><div class="line">		&#125;</div><div class="line">			</div><div class="line">		</div><div class="line">		<span class="comment">//第一步，创建Job</span></div><div class="line">		Job job = <span class="keyword">new</span> Job(conf,<span class="string">"myjob"</span>);</div><div class="line">		</div><div class="line">		<span class="comment">//设置Job,mapper,reducer,combiner(optional)</span></div><div class="line">		job.setJarByClass(MyTopKClass.class);</div><div class="line">		job.setMapperClass(MyTopKMapper.class);</div><div class="line">		job.setReducerClass(MyTopKReducer.class);</div><div class="line">		<span class="comment">//job.setCombinerClass(MySortReducer.class);</span></div><div class="line">		</div><div class="line">		job.setMapOutputKeyClass(IntWritable.class);</div><div class="line">		job.setMapOutputValueClass(NullWritable.class);</div><div class="line">		</div><div class="line">		<span class="comment">//设置Output类型</span></div><div class="line">		job.setOutputKeyClass(IntWritable.class);</div><div class="line">		job.setOutputValueClass(IntWritable.class);</div><div class="line">		</div><div class="line">		</div><div class="line">		FileInputFormat.addInputPath(job,<span class="keyword">new</span> Path(INPUT_PATH));</div><div class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(OUTPUT_PATH));</div><div class="line">		</div><div class="line">		<span class="comment">//最后一步</span></div><div class="line">		job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/16/MyTopKClass/" data-id="cj6xjfeyy0009tsjogoibfdzf" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      

    </footer>
  </div>
  
</article>



  
    <article id="post-hadoopMRAdv" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/15/hadoopMRAdv/">hadoop高级</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/15/hadoopMRAdv/" class="article-date"><time datetime="2017-08-15T11:39:30.000Z" itemprop="datePublished">2017-08-15</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="MapReduce的具体过程"><a href="#MapReduce的具体过程" class="headerlink" title="MapReduce的具体过程"></a>MapReduce的具体过程</h2><p>​    MapReduce的工作流程涉及到5个小流程：input，splitting，<strong>mapping</strong>，shuffling，<strong>reducing</strong>。如下图所示：<br><img src="/2017/08/15/hadoopMRAdv/MRProcess.tiff" alt=""></p>
<h3 id="Splitting"><a href="#Splitting" class="headerlink" title="Splitting"></a>Splitting</h3><p>​    在这个过程主要涉及到两个类<code>InputFormat</code>和<code>InputSplit</code>。</p>
<h4 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a>InputFormat</h4><p>​    通过使用InputFormat，MapReduce框架可以做到：</p>
<ol>
<li>验证作业的输入的正确性；</li>
<li>把<strong>输入文件</strong>切分成多个<strong>逻辑InputSplits</strong>，并把每一个InputSplit分别分发给一个<strong>单独的MapperTask</strong>；</li>
<li><p>提供RecordReader的实现，这个RecordReader从指定的InputSplit中正确读出一条一条的Ｋ－Ｖ对，~这些 Ｋ－Ｖ对将由我们写的Mapper方法处理~ 。</p>
<p>当数据传送给map时，map会将输入分片传送到InputFormat，InputFormat则调用方法<code>getRecordReader()</code>生成<code>RecordReader</code>，RecordReader再通过<code>creatKey()</code>、<code>creatValue()</code>方法创建可供map处理的一个一个的<key,value>对。简而言之，<code>InputFormat()</code>方法是用来<strong>生成可供map处理的<key,value>对</key,value></strong>的。</key,value></p>
<p>默认使用的ITextInputFormat，在TextInputFormat中，每个文件（或其一部分）都会单独地作为map的输入，而这个是继承自FileInputFormat的。<br>之后，每行数据都会生成一条记录，每条记录则表示成<key,value>形式：</key,value></p>
</li>
</ol>
<ul>
<li>key值是每个数据的记录在数据分片中<strong>字节偏移量</strong>，数据类型LongWritable；</li>
<li>value值是每行的内容，数据类型是Text。</li>
</ul>
<h4 id="关于inputSplit"><a href="#关于inputSplit" class="headerlink" title="关于inputSplit"></a>关于inputSplit</h4><p>​    任何分割操作的实现都继承自Apache抽象基类——InputSplit，它定义了分割的长度及位置。<br>​    InputSplit是hadoop定义的用来传送给每个单独的map的数据，InputSplit存储的<strong>并非数据本身</strong>，而是一个<strong>分片长度</strong>和一个<strong>记录数据位置的数组</strong>。生成InputSplit的方法可以通过InputFormat()来设置。</p>
<p>参考：</p>
<ol>
<li><a href="http://blog.csdn.net/zolalad/article/details/12653093" target="_blank" rel="external">MapReduce中InputFormat和InputSplit解读 - zolalad的专栏        - CSDN博客</a></li>
</ol>
<h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><p><img src="/2017/08/15/hadoopMRAdv/mapstages.tiff" alt=""></p>
<h4 id="Combiner"><a href="#Combiner" class="headerlink" title="Combiner"></a>Combiner</h4><p>​    每一个map都可能会产生大量的本地输出，Combiner的作用就是对map端 的输出先做一次合并，以减少在map和reduce节点之间的数据传输量， 以提高网络IO性能。</p>
<p>​    Combiner的作用</p>
<ol>
<li>Combiner实现<strong>本地key</strong>的聚合，对map输出的key排序value进行迭代；</li>
<li>Combiner还有本地reduce功能（其本质上就是一个reduce）</li>
</ol>
<p>Combiner作用图解：<br><img src="/2017/08/15/hadoopMRAdv/combiner.tiff" alt=""><br>可以看到，combiner是在分区之前开始工作的。</p>
<p>​    Combiner的使用方法<br>在main方法里，使用一下方法指定某个job的combiner</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">job.setCombinerClass(MyReducer.class);</div></pre></td></tr></table></figure>
<h4 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h4><p>​    MapReduce的使用者通常会指定Reduce任务和Reduce任务输出文件的数量（R）。<br>​    用户在中间key上使用分区函数来对数据进行分区，之后在输入到后续任务执行进程。一个默认的分区函数式使用hash方法（比如常见的：hash(key) mod R）进行分区。hash方法能够产生非常平衡的分区。</p>
<p>分区Partitioner主要作用在于以下两点</p>
<ol>
<li>根据业务需要，产生多个输出文件</li>
<li>多个reduce任务并发运行，提高整体job的运行效率</li>
</ol>
<h3 id="Shuffling"><a href="#Shuffling" class="headerlink" title="Shuffling"></a>Shuffling</h3><h4 id="shuffle的定义"><a href="#shuffle的定义" class="headerlink" title="shuffle的定义"></a>shuffle的定义</h4><p>在《Hadoop权威指南》中，对Shuffle的解释如下：</p>
<blockquote>
<p>MapReduce makes the guarantee that the input to every reducer is sorted by key. The process by which the system performs the sort—and transfers the map outputs to the reducers as inputs—is known as the shuffle. </p>
</blockquote>
<p>​    MapReduce保证每一个reducer的输入<strong>都是已经按照key拍过序的</strong>。这个<u>由系统执行的将mapper的输出排序后传输到reducer作为其输入的过程</u>称为shuffle（洗牌）。<br>​    从定义可以看出，sort这个动作发生在shuffle这个阶段。<br>​    这个过程有<strong>排序</strong>，也有<strong>复制</strong>。</p>
<p>shuffle和sort在mr过程中的体现如下图：<br><img src="/2017/08/15/hadoopMRAdv/shufflingAndSort.png" alt=""><br>输入，map，分区，排序，分组，存到磁盘<br>fetch，sort，combine，reduce    </p>
<h4 id="Shuffle中的Map端："><a href="#Shuffle中的Map端：" class="headerlink" title="Shuffle中的Map端："></a>Shuffle中的Map端：</h4><p><img src="/2017/08/15/hadoopMRAdv/mapInShuffle.png" alt=""><br>对上图的解释如下：<br>​    在map端，先是InputSplit，在InputSplit中含有DataNode中的数据，每 个InputSplit都会分配一个Mapper任务，Mapper任务结束后产 <k2，v2>的输出， <strong>这些输出先存放在缓存中</strong>，每个map有 个环形内存缓冲区， 于存储任务的 输出。默认为100MB(io.sort.mb属性)， 旦达到阀值0.8(<code>io.sort.spill.percent</code>)， 一个后台线程就把内容写到(spill)Linux<strong>本地磁盘</strong>中的指定目录(mapred.local.dir)下的新建的一个溢出写文件。<br>​    写磁盘前，<strong>要进 partition、sort和combine等操作</strong>。通过分区，将同类型的数据分开处 ，之后对同分区的数据进行排序，如果有Combiner，还要对排序后的数据进 combine。等最后记录写完，将全部溢出 件合并为 个分区且 排序的 件<br>最后将磁盘中的数据送到Reduce中，图中Map输出有三个分区，有一个分区数据被送到图示的Reduce任务中，剩下的两个分区被送到其他Reducer任务中。  图示的Reducer任务的其他的三个输 则来其他节点的Map输出。</k2，v2></p>
<p>一些要点：</p>
<ol>
<li>map的输出存在一个环形内存缓冲区，内容超过80%的时候，内存的内容会被<strong>新开的线程</strong>写入到HDFS；</li>
<li>线程根据数据最终被送到的recuder，将<strong>数据划分为相应的分区</strong>；</li>
<li>在每个分区中，后台线程按照key进行内排序，此时若是有一个combiner，combiner将基于排序后的输出来运行。也就是说在溢写文件被写入到磁盘<strong>之前</strong>运行了combiner；</li>
<li>一个map有可能会产生若干个溢写文件（spill file），最终溢写文件会被<strong>合并</strong>成一个<strong>已分区</strong>且<strong>已排序</strong>的文件。</li>
<li>map的输出文件位于其tasktracker的<strong>本地磁盘</strong>。</li>
</ol>
<p>​    </p>
<h4 id="Shuffle中的Reduce端"><a href="#Shuffle中的Reduce端" class="headerlink" title="Shuffle中的Reduce端"></a>Shuffle中的Reduce端</h4><p><img src="/2017/08/15/hadoopMRAdv/ReduceInShuffle.tiff" alt=""><br>​    Copy阶段:Reducer通过Http式得到输出文件的分区。reduce端可能从n个map的结果中获取数据， 这些map的执行速度不尽相同， 当其中一个map运行结束时，reduce就会从JobTracker中获取该信息。map运行结束后TaskTracker会得到消息，进而将消息汇报给JobTracker，reduce定时从 JobTracker获取该信息，reduce端默认有5个数据复制线程从map端复制数据；<br>​    Merge阶段:如果形成多个磁盘文件会进行合并。从map端复制来的数据 先写到reduce端的<strong>缓存</strong>中，同样缓存占用<strong>到达一定阈值后会将数据写到磁盘中</strong>，<strong>同样会进 partition、combine、排序等过程</strong>。如果形成多个磁盘 文件还会进行合并，<strong>最后一次合并的结果</strong>作为reduce的输入而不是写到磁盘中；</p>
<p>一些要点：</p>
<ol>
<li>只要有一个map任务完成了，reduce任务就开始复制该map的输出</li>
<li>reducer通过<strong>HTTP</strong>得到输出文件的<strong>分区</strong>。</li>
<li>在将数据从内存存到磁盘中这个过程，就可以使用各种<strong>压缩</strong>算法了；</li>
</ol>
<h4 id="Shuffle过程中需要注意的以下点"><a href="#Shuffle过程中需要注意的以下点" class="headerlink" title="Shuffle过程中需要注意的以下点"></a>Shuffle过程中需要注意的以下点</h4><p>Shuffle同时涉及到Map和Reduce两端。<br>在Map端，map的所处的节点上，会有一个partition，combine，sort的操作；<br>在Reduce端，同样会有partition，combine，sort的操作，不过这次发生在分布式文件系统上；</p>
<h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><p><img src="/2017/08/15/hadoopMRAdv/reduce.png" alt=""><br>此处主要是对受到的的<k2,v2>执行程序员编写的reduce方法。</k2,v2></p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/15/hadoopMRAdv/" data-id="cj6xjfezp000itsjoz7jv4cb1" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-wordcount" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/15/wordcount/">wordcount的编写</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/15/wordcount/" class="article-date"><time datetime="2017-08-15T11:29:34.000Z" itemprop="datePublished">2017-08-15</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>在Eclipse创建MapReduce项目，新建一个MyWordCount类。</p>
<p>编写代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">package</span> test;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.net.URI;</div><div class="line"><span class="keyword">import</span> java.util.Iterator;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyWordCount</span> </span>&#123;</div><div class="line">	</div><div class="line">		<span class="keyword">private</span> <span class="keyword">static</span>  String INPUT_PATH = <span class="string">"hdfs://192.168.178.128:9000/input/words.txt"</span>;</div><div class="line">		<span class="keyword">private</span> <span class="keyword">static</span>  String OUTPUT_PATH = <span class="string">"hdfs://192.168.178.128:9000/output01"</span>;</div><div class="line">	</div><div class="line">		<span class="comment">// mapper</span></div><div class="line">		<span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line">			</div><div class="line">			<span class="meta">@Override</span></div><div class="line">			<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, // offset 偏移量</span></span></div><div class="line"><span class="function"><span class="params">					Text value,</span></span></div><div class="line"><span class="function"><span class="params">					Context context)</span></span></div><div class="line"><span class="function">							<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">				<span class="comment">//map处理开始</span></div><div class="line">				<span class="comment">//切词</span></div><div class="line">				String[] lines = value.toString().split(<span class="string">"\\s+"</span>);</div><div class="line">				<span class="comment">// 举个例子：line x: hello world</span></div><div class="line">				<span class="comment">// 得到的结果就是: (hello,1) (world,1) ，分别写入到context中</span></div><div class="line">				<span class="keyword">for</span> (String word : lines) &#123;</div><div class="line">					context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</div><div class="line">				&#125;</div><div class="line">				System.out.println(<span class="string">"mapping......"</span>);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		</div><div class="line">		<span class="comment">//reducer</span></div><div class="line">		<span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line">			</div><div class="line">			<span class="meta">@Override</span></div><div class="line">			<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, // map -&gt; sort part -&gt; reduce</span></span></div><div class="line"><span class="function"><span class="params">					Iterable&lt;IntWritable&gt; value,</span></span></div><div class="line"><span class="function"><span class="params">					Context context)</span></span></div><div class="line"><span class="function">							<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">				</div><div class="line">				<span class="comment">// reducer得到的key是唯一的</span></div><div class="line">				<span class="comment">// 此处举个例子， 该reducer处理的key是 hello</span></div><div class="line">				<span class="comment">// 目前有的键值对有 hello 1 </span></div><div class="line">				<span class="comment">// 				hello 1</span></div><div class="line">				<span class="comment">// 				hello 1</span></div><div class="line">				<span class="comment">//得到的结果将会是 hello 3</span></div><div class="line">				<span class="keyword">int</span> sum = <span class="number">0</span>;</div><div class="line">				Iterator&lt;IntWritable&gt; it = value.iterator();</div><div class="line">				<span class="keyword">while</span>(it.hasNext())&#123;</div><div class="line">					sum += it.next().get();</div><div class="line">				&#125;</div><div class="line"> 				</div><div class="line">				<span class="comment">//将结果写入到context</span></div><div class="line">				context.write(key, <span class="keyword">new</span> IntWritable(sum));	</div><div class="line">				System.out.println(<span class="string">"reducing......"</span>);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		</div><div class="line">		</div><div class="line">		</div><div class="line"></div><div class="line">		<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">			</div><div class="line">			<span class="comment">// 作业前的准备</span></div><div class="line">			Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">			FileSystem fs = FileSystem.get(URI.create(OUTPUT_PATH),conf);</div><div class="line">			<span class="comment">// 检查目录是否存在，是则但删除之</span></div><div class="line">			<span class="keyword">if</span>(fs.exists(<span class="keyword">new</span> Path(OUTPUT_PATH)))&#123;</div><div class="line">				fs.delete(<span class="keyword">new</span> Path(OUTPUT_PATH));</div><div class="line">			&#125;</div><div class="line">				</div><div class="line">			</div><div class="line">			<span class="comment">//第一步，创建Job</span></div><div class="line">			Job job = <span class="keyword">new</span> Job(conf,<span class="string">"myjob"</span>);</div><div class="line">			</div><div class="line">			<span class="comment">//设置Job,mapper,reducer,combiner(optional)</span></div><div class="line">			job.setJarByClass(MyWordCount.class);</div><div class="line">			job.setMapperClass(MyMapper.class);</div><div class="line">			job.setReducerClass(MyReducer.class);</div><div class="line">			job.setCombinerClass(MyReducer.class);</div><div class="line">			</div><div class="line">			</div><div class="line">			<span class="comment">//设置Output类型</span></div><div class="line">			job.setOutputKeyClass(Text.class);</div><div class="line">			job.setOutputValueClass(IntWritable.class);</div><div class="line">			</div><div class="line">			FileInputFormat.addInputPath(job,<span class="keyword">new</span> Path(INPUT_PATH));</div><div class="line">			FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(OUTPUT_PATH));</div><div class="line">			</div><div class="line">			<span class="comment">//最后一步</span></div><div class="line">			job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line">			</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>运行输出如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div></pre></td><td class="code"><pre><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">34</span> WARN util.NativeCodeLoader: Unable to load <span class="keyword">native</span>-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">35</span> INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">35</span> INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">36</span> WARN mapreduce.JobSubmitter: Hadoop command-line option parsing not performed. Implement the Tool <span class="class"><span class="keyword">interface</span> <span class="title">and</span> <span class="title">execute</span> <span class="title">your</span> <span class="title">application</span> <span class="title">with</span> <span class="title">ToolRunner</span> <span class="title">to</span> <span class="title">remedy</span> <span class="title">this</span>.</span></div><div class="line">17/08/15 19:37:36 WARN mapreduce.JobSubmitter: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">36</span> INFO input.FileInputFormat: Total input paths to process : <span class="number">1</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">36</span> INFO mapreduce.JobSubmitter: number of splits:<span class="number">1</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">36</span> INFO mapreduce.JobSubmitter: Submitting tokens <span class="keyword">for</span> job: job_local484001035_0001</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapreduce.Job: The url to track the job: http:<span class="comment">//localhost:8080/</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapreduce.Job: Running job: job_local484001035_0001</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.LocalJobRunner: OutputCommitter set in config <span class="keyword">null</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.LocalJobRunner: Waiting <span class="keyword">for</span> map tasks</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.LocalJobRunner: Starting task: attempt_local484001035_0001_m_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.Task:  Using ResourceCalculatorProcessTree : <span class="keyword">null</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: Processing split: hdfs:<span class="comment">//192.168.178.128:9000/input/words.txt:0+86</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: (EQUATOR) <span class="number">0</span> kvi <span class="number">26214396</span>(<span class="number">104857584</span>)</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: mapreduce.task.io.sort.mb: <span class="number">100</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: soft limit at <span class="number">83886080</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: bufstart = <span class="number">0</span>; bufvoid = <span class="number">104857600</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: kvstart = <span class="number">26214396</span>; length = <span class="number">6553600</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">37</span> INFO mapred.MapTask: Map output collector <span class="class"><span class="keyword">class</span> </span>= org.apache.hadoop.mapred.MapTask$MapOutputBuffer</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapreduce.Job: Job job_local484001035_0001 running in uber mode : <span class="keyword">false</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapreduce.Job:  map <span class="number">0</span>% reduce <span class="number">0</span>%</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line">mapping......</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: </div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: Starting flush of map output</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: Spilling map output</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: bufstart = <span class="number">0</span>; bufend = <span class="number">134</span>; bufvoid = <span class="number">104857600</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: kvstart = <span class="number">26214396</span>(<span class="number">104857584</span>); kvend = <span class="number">26214352</span>(<span class="number">104857408</span>); length = <span class="number">45</span>/<span class="number">6553600</span></div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.MapTask: Finished spill <span class="number">0</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Task: Task:attempt_local484001035_0001_m_000000_0 is done. And is in the process of committing</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: map</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Task: Task <span class="string">'attempt_local484001035_0001_m_000000_0'</span> done.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: Finishing task: attempt_local484001035_0001_m_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: map task executor complete.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: Waiting <span class="keyword">for</span> reduce tasks</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: Starting task: attempt_local484001035_0001_r_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Task:  Using ResourceCalculatorProcessTree : <span class="keyword">null</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@<span class="number">2</span>ae4ec62</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=<span class="number">1336252800</span>, maxSingleShuffleLimit=<span class="number">334063200</span>, mergeThreshold=<span class="number">881926912</span>, ioSortFactor=<span class="number">10</span>, memToMemMergeOutputsThreshold=<span class="number">10</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.EventFetcher: attempt_local484001035_0001_r_000000_0 Thread started: EventFetcher <span class="keyword">for</span> fetching Map Completion Events</div><div class="line">17/08/15 19:37:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local484001035_0001_m_000000_0 decomp: 95 len: 99 to MEMORY</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.InMemoryMapOutput: Read <span class="number">95</span> bytes from map-output <span class="keyword">for</span> attempt_local484001035_0001_m_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: <span class="number">95</span>, inMemoryMapOutputs.size() -&gt; <span class="number">1</span>, commitMemory -&gt; <span class="number">0</span>, usedMemory -&gt;<span class="number">95</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: <span class="number">1</span> / <span class="number">1</span> copied.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: finalMerge called with <span class="number">1</span> in-memory map-outputs and <span class="number">0</span> on-disk map-outputs</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Merger: Merging <span class="number">1</span> sorted segments</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Merger: Down to the last merge-pass, with <span class="number">1</span> segments left of total size: <span class="number">88</span> bytes</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: Merged <span class="number">1</span> segments, <span class="number">95</span> bytes to disk to satisfy reduce memory limit</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: Merging <span class="number">1</span> files, <span class="number">99</span> bytes from disk</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO reduce.MergeManagerImpl: Merging <span class="number">0</span> segments, <span class="number">0</span> bytes from memory into reduce</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Merger: Merging <span class="number">1</span> sorted segments</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.Merger: Down to the last merge-pass, with <span class="number">1</span> segments left of total size: <span class="number">88</span> bytes</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO mapred.LocalJobRunner: <span class="number">1</span> / <span class="number">1</span> copied.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">38</span> INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line">reducing......</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapreduce.Job:  map <span class="number">100</span>% reduce <span class="number">0</span>%</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.Task: Task:attempt_local484001035_0001_r_000000_0 is done. And is in the process of committing</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.LocalJobRunner: <span class="number">1</span> / <span class="number">1</span> copied.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.Task: Task attempt_local484001035_0001_r_000000_0 is allowed to commit now</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO output.FileOutputCommitter: Saved output of task <span class="string">'attempt_local484001035_0001_r_000000_0'</span> to hdfs:<span class="comment">//192.168.178.128:9000/output01/_temporary/0/task_local484001035_0001_r_000000</span></div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.LocalJobRunner: reduce &gt; reduce</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.Task: Task <span class="string">'attempt_local484001035_0001_r_000000_0'</span> done.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.LocalJobRunner: Finishing task: attempt_local484001035_0001_r_000000_0</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">39</span> INFO mapred.LocalJobRunner: reduce task executor complete.</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">40</span> INFO mapreduce.Job:  map <span class="number">100</span>% reduce <span class="number">100</span>%</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">40</span> INFO mapreduce.Job: Job job_local484001035_0001 completed successfully</div><div class="line"><span class="number">17</span>/<span class="number">08</span>/<span class="number">15</span> <span class="number">19</span>:<span class="number">37</span>:<span class="number">40</span> INFO mapreduce.Job: Counters: <span class="number">35</span></div><div class="line">	File System Counters</div><div class="line">		FILE: Number of bytes read=<span class="number">562</span></div><div class="line">		FILE: Number of bytes written=<span class="number">491225</span></div><div class="line">		FILE: Number of read operations=<span class="number">0</span></div><div class="line">		FILE: Number of large read operations=<span class="number">0</span></div><div class="line">		FILE: Number of write operations=<span class="number">0</span></div><div class="line">		HDFS: Number of bytes read=<span class="number">172</span></div><div class="line">		HDFS: Number of bytes written=<span class="number">65</span></div><div class="line">		HDFS: Number of read operations=<span class="number">15</span></div><div class="line">		HDFS: Number of large read operations=<span class="number">0</span></div><div class="line">		HDFS: Number of write operations=<span class="number">6</span></div><div class="line">	Map-Reduce Framework</div><div class="line">		Map input records=<span class="number">7</span></div><div class="line">		Map output records=<span class="number">12</span></div><div class="line">		Map output bytes=<span class="number">134</span></div><div class="line">		Map output materialized bytes=<span class="number">99</span></div><div class="line">		Input split bytes=<span class="number">108</span></div><div class="line">		Combine input records=<span class="number">12</span></div><div class="line">		Combine output records=<span class="number">7</span></div><div class="line">		Reduce input groups=<span class="number">7</span></div><div class="line">		Reduce shuffle bytes=<span class="number">99</span></div><div class="line">		Reduce input records=<span class="number">7</span></div><div class="line">		Reduce output records=<span class="number">7</span></div><div class="line">		Spilled Records=<span class="number">14</span></div><div class="line">		Shuffled Maps =<span class="number">1</span></div><div class="line">		Failed Shuffles=<span class="number">0</span></div><div class="line">		Merged Map outputs=<span class="number">1</span></div><div class="line">		<span class="function">GC time <span class="title">elapsed</span> <span class="params">(ms)</span></span>=<span class="number">0</span></div><div class="line">		<span class="function">Total committed heap <span class="title">usage</span> <span class="params">(bytes)</span></span>=<span class="number">475004928</span></div><div class="line">	Shuffle Errors</div><div class="line">		BAD_ID=<span class="number">0</span></div><div class="line">		CONNECTION=<span class="number">0</span></div><div class="line">		IO_ERROR=<span class="number">0</span></div><div class="line">		WRONG_LENGTH=<span class="number">0</span></div><div class="line">		WRONG_MAP=<span class="number">0</span></div><div class="line">		WRONG_REDUCE=<span class="number">0</span></div><div class="line">	File Input Format Counters </div><div class="line">		Bytes Read=<span class="number">86</span></div><div class="line">	File Output Format Counters </div><div class="line">		Bytes Written=<span class="number">65</span></div></pre></td></tr></table></figure>
<p>输入文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hello liuchengwu</div><div class="line">hello liulao</div><div class="line">hello dora</div><div class="line">hello liu</div><div class="line">hello hadoop</div><div class="line">helloWorld</div><div class="line">helloWorld</div></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">dora	1</div><div class="line">hadoop	1</div><div class="line">hello	5</div><div class="line">helloWorld	2</div><div class="line">liu	1</div><div class="line">liuchengwu	1</div><div class="line">liulao	1</div></pre></td></tr></table></figure>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/15/wordcount/" data-id="cj6xjff01000stsjowkr6esmr" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/代码/">代码</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-Hadoop-MapReduce" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/14/Hadoop-MapReduce/">Hadoop_MapReduce</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/14/Hadoop-MapReduce/" class="article-date"><time datetime="2017-08-14T14:39:19.000Z" itemprop="datePublished">2017-08-14</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="什么是MapReduce"><a href="#什么是MapReduce" class="headerlink" title="什么是MapReduce"></a>什么是MapReduce</h2><p>​    MapReduce是一个编程模型，用以进行大数据量的计算。<br>​    Hadoop MapReduce是一个软件框架，基于该框架能够 容易地编写应用程序，这些应用程序能够运行在由上千个商用机器组成的大集群上，并以一种<strong>可靠的</strong>，<strong>具有容错能力</strong>的方式并行地处理上TB级别的海量数据集。</p>
<h3 id="MapReduce的特点"><a href="#MapReduce的特点" class="headerlink" title="MapReduce的特点"></a>MapReduce的特点</h3><p>MapReduce的特点如下：</p>
<ol>
<li>软件框架</li>
<li>并行处理 </li>
<li>可靠且容错 </li>
<li>大规模集群 </li>
<li>海量数据集</li>
</ol>
<h3 id="MapReduce的处理方式"><a href="#MapReduce的处理方式" class="headerlink" title="MapReduce的处理方式"></a>MapReduce的处理方式</h3><p>​    MapReduce处理工作的主要思想是：<strong>分而治之</strong>。<br>​    整个工作流程中涉及到4个实体：客户端，JobTracker，TaskTracker，HDFS。<br>​    在这四个实体中，和目前的编程有关的就是<strong>客户端</strong>，编写的就是MapReduce程序。<br>​    该工作流程如下图所示：<br><img src="/2017/08/14/Hadoop-MapReduce/runAMapReduceJob.jpg" alt=""></p>
<p>​    客户端：提交MapReduce工作；<br>​    JobTracker：协调作业的运行，只有一个；<br>​    TaskTracker：处理作业划分后的任务，可以有多个；<br>​    HDFS：在其他实体之间<strong>共享</strong>作业文件；<br>‼️工作流程如下：</p>
<ol>
<li>客户端要<strong>编写mapreduce程序</strong>，配置好mapreduce的作业也就是<strong>job</strong>， 接下来向JobTracker请求一个jobID，这个时候JobTracker为该job任务分配一个新的ID值；</li>
<li>接着JokTracker进行各种检查和计算，例如，<strong>输出目录是否存在</strong>（存在则无法运行），<strong>输入目录是否存在</strong>（不存在则不可运行），<strong>据输入计算输入分片(Input Split)</strong>，完成之后JobTracker为Job分配资源，客户端向JobTracker提交作业(submitJob)；</li>
<li>JobTracker初始化作业，初始化主要做的是<strong>将Job放入一个内部的队列</strong>，让配置好的<strong>作业调度器</strong>能调度到这个作业。作业调度器会初始化这个job，初始化就是创建一个正在运行的job对象(封装任务和记录信息)， 以便JobTracker跟踪job的状态和进程</li>
<li>初始化完毕，作业调度器获取输入分片信息，<strong>每个分片创建一个map任务</strong>；</li>
<li>开始分配任务，taskTracker通过心跳包💗（5s为间隔）向JobTracker发送自己的状态，两者通过心跳包通信；</li>
<li>开始执行任务。当jobtracker获得了最后一个完成指定任务的 tasktracker操作成功的通知时候，jobtracker会把整个job状态置为成功。</li>
</ol>
<h3 id="MapReduce的运行机制"><a href="#MapReduce的运行机制" class="headerlink" title="MapReduce的运行机制"></a>MapReduce的运行机制</h3><p><img src="/2017/08/14/Hadoop-MapReduce/mapreduceArch.tiff" alt=""><br>​    在Hadoop中，一个MapReduce作业会把输入的数据集切分为<strong>若干独立的数据块</strong>，由Map任务以<strong>完全并行</strong>的方式处理；<br>​    框架会<strong>对Map的输出先进行排序，</strong>然后把结果输入给Reduce任务；<br>作业的输入和输出都会<strong>被存储在文件系统中</strong>，整个框架负责任务的调度和监控，以及重新执行已经关闭的任务；<br>​    MapReduce框架和分布式文件系统是运行在<strong>一组相同的节点</strong>， 计算节点和存储节点都是在一起的；</p>
<h3 id="MapReduce的输入输出"><a href="#MapReduce的输入输出" class="headerlink" title="MapReduce的输入输出"></a>MapReduce的输入输出</h3><p>​    在整个MapReduce作业中，涉及到<strong>三组键值对</strong>。</p>
<p><img src="/2017/08/14/Hadoop-MapReduce/keysAndValues.tiff" alt=""><br>​    <k1，v1>：初始输入，作为map的输入；<br>​    <k2，v2>：map的输出，在经过shuffle（排序等操作）后，作为reduce的输入；<br>​    <k3，v3>：reduce的输出。</k3，v3></k2，v2></k1，v1></p>
<h3 id="MapReduce的工作流程"><a href="#MapReduce的工作流程" class="headerlink" title="MapReduce的工作流程"></a>MapReduce的工作流程</h3><p>​    MapReduce的工作流程涉及到5个小流程：input，splitting，<strong>mapping</strong>，shuffling，<strong>reducing</strong>，final result。如下图所示：</p>
<p><img src="/2017/08/14/Hadoop-MapReduce/MRProcess.tiff" alt=""><br>​    教程将其重新划分为<strong>5个阶段</strong>。输入分片(input split)、 map阶段、 combiner阶段、 shuffle阶段和 reduce阶段。</p>
<h4 id="输入分片-input-split"><a href="#输入分片-input-split" class="headerlink" title="输入分片(input split):"></a>输入分片(input split):</h4><p>​    在进行map计算之前，mapreduce会根 据输入文件计算输入分片(input split)，每个输入分片(input split)针对一个map任务；<br>​    输入分片(input split)存储的并非数据本身，而是一个分片长度和 一个记录数据的位置的数组，输入分片(input split)往往和hdfs 的block(块)关系很密切</p>
<blockquote>
<p>假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别 是3mb、65mb和127mb，那么mapreduce会把<strong>3mb文件分为一个输入分片（3mb &lt; 64mb）</strong> (input split)，<strong>65mb则是两个( 128mb &gt; 65mb &gt; 64mb)</strong>输入分片(input split)；而<strong>127mb也是两个输入分片（128mb &gt; 127mb &gt; 64mb）</strong>(input split)；<br>即我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个 map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce 优化计算的一个关键点。  </p>
</blockquote>
<h4 id="map阶段"><a href="#map阶段" class="headerlink" title="map阶段"></a>map阶段</h4><p>​    程序员编写的map函数；</p>
<h4 id="combine阶段"><a href="#combine阶段" class="headerlink" title="combine阶段"></a>combine阶段</h4><p>​    Combiner阶段是程序员可以选择的，<strong>combiner其实也是一种reduce操作</strong>，因此我们看见WordCount类里是用reduce进行加载的。<br>​    Combiner是一个<strong>本地化的reduce操作</strong>，它是map运算的后续操作，主要是在map计算出中间文件前做一个<strong>简单的合并重复key值的操作</strong>。</p>
<blockquote>
<p>例如：我们对文件里的单词频率做统计，map计算时候如果碰到一个hadoop的单词就会记录为1，但是这篇文章里hadoop可能会出现n多次，那么map输出 文件冗余就会很多，因此在reduce计算前对相同的key做一个合并操作， 那么文件会变小，这样就提高了宽带的传输效率，毕竟hadoop计算力宽带资源往往是计算的瓶颈也是最为宝贵的资源，但是combiner操作是有 风险的，使用它的原则是<strong>combiner的输入不会影响到reduce计算的最终输入</strong>，<br>​    例如:如果计算只是求总数，最大值，最小值可以使用combiner，但是做平 均值计算使用combiner的话，最终的reduce计算结果就会出错。  </p>
</blockquote>
<h4 id="shuffle阶段"><a href="#shuffle阶段" class="headerlink" title="shuffle阶段"></a>shuffle阶段</h4><p>​    将map的输出作为reduce的输入的过程。</p>
<h4 id="reduce阶段"><a href="#reduce阶段" class="headerlink" title="reduce阶段"></a>reduce阶段</h4><p>​    和map函数一样也是程序员编写的,最终结果是存储在 hdfs上的。</p>
<h2 id="琐碎"><a href="#琐碎" class="headerlink" title="琐碎"></a>琐碎</h2><blockquote>
<p>每一个片都会开一个mapper，最终将所有分片进行聚合<br>所有的这些过程都是通过jobtracker进行调度的。<br>每个阶段都是一个进程（class），每个进程启动无数个mapper（线程）<br>有多少个key，就有多少个reducer。<br>分块－不同块内的数据是不相同的  </p>
</blockquote>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/14/Hadoop-MapReduce/" data-id="cj6xjfeyo0006tsjo3arm4c0s" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/">MapReduce</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0811JavaHadoop" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/11/0811JavaHadoop/">使用Java对HDFS进行文件操作</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/11/0811JavaHadoop/" class="article-date"><time datetime="2017-08-11T11:38:25.000Z" itemprop="datePublished">2017-08-11</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="主要的API"><a href="#主要的API" class="headerlink" title="主要的API"></a>主要的API</h2><pre><code>在Java中，涉及到HDFS文件操作的API主要有以下几个：
</code></pre><ol>
<li>FileSystem</li>
<li>FSDataOutputStream</li>
<li>FSDataInputStream</li>
<li>Path</li>
<li>Configuration </li>
<li><p>IOUtils</p>
<h3 id="FileSystem"><a href="#FileSystem" class="headerlink" title="FileSystem"></a>FileSystem</h3><p>FileSytem的全限定名为<code>org.apache.hadoop.fs.FileSystem</code>。<br><code>org.apache.hadoop.fs.FileSystem</code>是在分布式环境中<strong>访问和管理HDFS中的文件/目录的通用类</strong>。<br>文件内容以多个大尺寸块（64M）的形式存储在datanode中，namenode中记录了这些块的信息和文件的元信息。<br>FileSystem可以读或者<strong>按块的顺序流式</strong>的访问。<br>FileSystem首先从NameNode中得到块的信息，然后<strong>一个接一个</strong>的读。 ~它打开第一个块，它将读完关闭后才访问下一块~ 。<br>HDFS的复制块带来了更高的可靠性和可扩展性。如果client是数据节点中的一个，它将先访问本地，如果失败的话，它才会到集群的其他节点去访问。</p>
<p><code>FileSystem</code>使用<code>FSDataOutputStream</code> 及<code>FSDataInputStream</code>来读写流的内容。Hadoop提供了多种FileSystem的实现：</p>
</li>
</ol>
<ul>
<li>DistributedFileSystem（DFS）：在分布式的环境中，访问HDFS文件</li>
<li>LocalFileSystem:在本地系统中访问HDFS文件</li>
<li>FTPFileSystem：访问HDFS文件的FTP客户端</li>
<li>WebHdfsFileSystem：通过web接口访问HDFS文件。</li>
</ul>
<h4 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h4><p>FileSystem 的常用操作和相关方法：</p>
<ol>
<li>创建文件<ul>
<li><code>booleancreateNewFile(Path f )</code><ul>
<li><strong>不会覆盖</strong>已有文件</li>
<li>创建成功返回true,失败返回false<br>• <code>FSDataOutputStreamcreate(Path f)</code><br>• <strong>覆盖</strong>已有文件<br>• 创建文件并返回输出流<br>• <code>FSDataOutputStreamcreate(Path f,booleanoverwrite)</code><br>• 创建文件并返回输出流<br>• <code>FSDataOutputStreamcreate(Path f,boolean overwrite,int buffer)</code><br>• <code>FSDataOutputStreamcreate(Path f,boolean overwrite,int buffer,short replication,long blockSize)</code></li>
</ul>
</li>
</ul>
</li>
<li>打开文件<ul>
<li><code>FSDataInputStreamopen(Pathf)</code></li>
<li><code>FSDataInputStreamopen(Pathf,intbufferSize)</code><ul>
<li>返回输入流</li>
<li>如果文件不存在会抛出异常</li>
<li>不指定bufferSize时,会从Configuration中读取 io.file.buffer.size,默认为4096字节</li>
</ul>
</li>
</ul>
</li>
<li>文件追加<br>• <code>FSDataOutputStream append(Path f)</code><br>• <code>FSDataOutputStream append(Path f, int bufferSize)</code><br>   • 块不足64M时,会补足到64M<br>   • 块达到64M之前,该块不可见,ls看不到该块新增的大小,也无 法读取<br>   • 不能同时多个writer追加同一个文件</li>
<li>从本地拷贝文件到HDFS<ul>
<li><code>void copyFromLocalFile (Path src, Path dst)</code></li>
<li><code>void moveFromLocalFile (Path src, Path dst)</code></li>
</ul>
</li>
<li>从HDFS拷贝文件到本地<ul>
<li><code>void copyToLocalFile(boolean delsrc,Path src,Path dst)</code></li>
<li><code>void moveToLocalFile (Path src, Path dst)</code></li>
</ul>
</li>
<li>创建目录<ul>
<li>boolean mkdirs (Path f)<br>• boolean mkdirs (Path f, FsPermission permission)<br>• static boolean mkdirs (FileSystem fs, Path dir, FsPermission permission)<br>• 支持多级目录同时创建 (类似mkdir -p)<br>• 默认权限是755<br>• 成功返回true</li>
</ul>
</li>
<li>删除及重命名<ul>
<li><code>boolean delete (Path f, boolean recursive)</code></li>
<li><code>boolean deleteOnExit (f)</code> 当关闭FileSystem时,才会删除</li>
</ul>
</li>
<li>获取文件或目录信息<ul>
<li>FileStatus[ ] listStatus (Path f, PathFilter filter)<br>• FileStatus[ ] listStatus (Path[ ] dir, PathFilter filter)<br>FileStatus信息包括:</li>
<li>绝对路径</li>
<li>文件大小(单位:字节)</li>
<li>文件访问时间</li>
<li>块大小、复制份数</li>
<li>文件所属用户、组、访问权限</li>
</ul>
</li>
<li>设置文件或目录属性<ul>
<li>void setOwner (Path p, String username, String groupname)<ul>
<li>设置文件或目录所属用户及组</li>
<li>参数p指定文件或目录</li>
<li>参数username,设置此文件或目录的所属用户 – 只返回列出指定目录下的文件或目录信息<br>• void setPermission (Path p, FsPermission permission)<br>• 设置文件或目录权限<br>• 参数p指定文件或目录<br>• 参数permission,指定权限,权限同linux权限雷同<br>• void setReplication (Path f, short replication)<br>• 设置文件复制份数<br>• 参数f指定文件<br>• 参数replication指定复制份数</li>
</ul>
</li>
</ul>
</li>
<li>获取文件或目录信息<ul>
<li>void setTimes (Path f, long mtime, long atime)<br>设置文件的修改及访问时间</li>
</ul>
</li>
</ol>
<h3 id="FSDataOutputStream"><a href="#FSDataOutputStream" class="headerlink" title="FSDataOutputStream"></a>FSDataOutputStream</h3><pre><code>FSDataOutputStream的全限定名是`org.apache.hadoop.fs.FSDataOutputStream`。
使用Java向HDFS**写入数据**时需要用到的流。
</code></pre><h3 id="FSDataInputStream"><a href="#FSDataInputStream" class="headerlink" title="FSDataInputStream"></a>FSDataInputStream</h3><pre><code>FSDataInputStream的全限定名是`org.apache.hadoop.fs. FSDataInputStream `。
使用Java向HDFS**读取数据**时需要用到的流。
</code></pre><h3 id="Path"><a href="#Path" class="headerlink" title="Path"></a>Path</h3><pre><code>Path的全限定名为`org.apache.hadoop.fs.Path`
用来封装HDFS的路径。
</code></pre><h4 id="常用的方法"><a href="#常用的方法" class="headerlink" title="常用的方法"></a>常用的方法</h4><p>Path的常有方法有：</p>
<ol>
<li><code>int depth()</code>：返回路径的深度</li>
<li><code>String getName()</code>： 返回路径上最后的资源名称</li>
<li><code>Path getParent()</code>：返回父目录，如果已是根目录则返回null</li>
<li><code>Path suffix(String suffix)</code>：参数suffix给Path增加后缀，返回加完后缀的Path实例；</li>
<li><code>getFileSystem (Configuration conf)</code>：返回该Path所属的文件系统实例</li>
</ol>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><pre><code>Configuration全限定名为
</code></pre><p><code>org.apache.hadoop.conf. Configuration</code>。<br>​<br>Configuration对配置文件的读取顺序：先加载缺省配置文件，再加载用户定义的配置文件；且对于每一个文件只加载一次：第一个在classpath出现的（这个和Linux系统上在匹配命令时候的逻辑是一样的）</p>
<p><em>classpath的目录顺序:</em></p>
<ol>
<li>$HADOOP_CONF_DIR</li>
<li>$JAVA_HOME/lib/tools.jar 如果$HADOOP_HOME目录下有build目录，则添加build下各子目录</li>
<li>$HADOOP_HOME/hadoop-core-*.jar</li>
<li>$HADOOP_HOME/lib/*.jar</li>
<li>用户在hadoop-env.sh中定义的$HADOOP_CLASS_PATH</li>
<li>当前作为hadoop jar …参数提交的JAR包</li>
</ol>
<h4 id="常用方法-1"><a href="#常用方法-1" class="headerlink" title="常用方法"></a>常用方法</h4><pre><code>Configuration的常用方法有:
</code></pre><ol>
<li>static void addDefaultResource(String name)</li>
<li>void addResource(InputStream in)</li>
<li>void addResource (Path file)  本地文件</li>
<li>void addResource(String name)  classpath中的文件</li>
<li>void addResource (URL url)void set(String name, String value)</li>
<li>void setBoolean(String name, boolean value)</li>
<li>void setInt(String name, String value)</li>
<li>void setLong(String name, long value)</li>
<li>void setFloat(String name, float value)</li>
<li>void setIfUnset(String name, String value)</li>
<li>void setBooleanIfUnset(String name, boolean value)</li>
<li>String get(String name)</li>
<li>boolean getBoolean(String name, boolean defaultValue)</li>
</ol>
<h3 id="IOUtils"><a href="#IOUtils" class="headerlink" title="IOUtils"></a>IOUtils</h3><pre><code>IOUtils的全限定名是`org.apache.hadoop.fs. IOUtils `。
IOUtils是一个I/O帮助类,提供的都是静态方法,不需要实例化。
</code></pre><h4 id="常用方法-2"><a href="#常用方法-2" class="headerlink" title="常用方法"></a>常用方法</h4><p>IOUtils的常用方法有：</p>
<ul>
<li>public static void copyBytes (InputStream in , outputStream out,<br>Configuration conf)<br>• public static void copyBytes (InputStream in , outputStream out, Configuration conf, boolean close)</li>
</ul>
<h2 id="开发基本步骤"><a href="#开发基本步骤" class="headerlink" title="开发基本步骤"></a>开发基本步骤</h2><p>使用Java在HDFS上的基本开发步骤如下：</p>
<ol>
<li>实例化<code>Configuration</code></li>
<li>实例化<code>FileSystem</code><ol>
<li>获取相关Stream进行文件／目录操作</li>
</ol>
</li>
</ol>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/11/0811JavaHadoop/" data-id="cj6xjfeyk0005tsjonvicuwcu" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0811HadoopIO" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/11/0811HadoopIO/">Hadoop IO操作</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/11/0811HadoopIO/" class="article-date"><time datetime="2017-08-11T11:31:25.000Z" itemprop="datePublished">2017-08-11</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>Hadoop的IO操作中涉及到几个问题：</p>
<ol>
<li>保持数据完整性</li>
<li>数据压缩</li>
<li>序列化</li>
<li>存储数据的数据结构</li>
</ol>
<h2 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h2><p>​    针对数据在存储过程损坏和丢失的问题，Haddop采取了<strong>数据校验</strong>和<strong>后台进程检测数据块</strong>的方法。<br>​    数据校验采用的是<strong>循环冗余校验CRC-32</strong>。</p>
<h3 id="验证时机"><a href="#验证时机" class="headerlink" title="验证时机"></a>验证时机</h3><ol>
<li>数据节点负责在存储数据及其校验和前验证它们收到的数据（发生于数据块副本传输的时候） （存储时）；</li>
<li>客户端读取数据节点上的数据时候，会验证校验和（读取时）；</li>
<li>每个数据节点有一个后台进程定期验证存在该节点山的所有数据块（定时）；</li>
</ol>
<p>关于验证，涉及到两个类，一个是<code>FileSystem</code>，一个是<code>CheckSumFileSystem</code>。</p>
<h2 id="基于文件的数据结构"><a href="#基于文件的数据结构" class="headerlink" title="基于文件的数据结构"></a>基于文件的数据结构</h2><p>HDFS和MR主要针对大数据文件来设计，在<em>小文件处理上效率低</em>。解决方法是选择一个容器，将这些小文件包装起来， 将整个文件作为一条记录，可以获取更高效率的储存和处理， 避免多次打开关闭流耗费计算资源.hdfs提供了两种类型的容器 <strong>SequenceFile</strong>和<strong>MapFile</strong>。<br><em>一言蔽之，这些文件结构用来解决HDFS存储小文件的痛点。</em></p>
<h3 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h3><p>​    SequenceFile是Hadoop对二进制键／值对持久化的数据结构。<br>​    Sequence的内部结构如下：<br><img src="/2017/08/11/0811HadoopIO/The-internal-structure-of-a-sequence-file-with-no-compression-and-record-compression.png" alt=""></p>
<p>​    Sequence file由一系列的二进制key/value组成，如果key为小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。文件中每条记录是可序列化，可持久化的键值对，提相应的<strong>读写器</strong>和<strong>排序器</strong>，写操作根据压缩的类型分为3种：</p>
<ul>
<li><p>Write 无压缩写数据，</p>
</li>
<li><p>RecordCompressWriter记录级压缩文件，只压缩值</p>
</li>
<li><p>BlockCompressWrite块级压缩文件，键值采用独立压缩方式;</p>
<p>关于压缩的对象，value可以压缩，key不可以压缩。</p>
<h4 id="对SequenceFile的操作方法："><a href="#对SequenceFile的操作方法：" class="headerlink" title="对SequenceFile的操作方法："></a>对SequenceFile的操作方法：</h4><p>​    Hadoop-0.21.0版本开始中提供了SequenceFile，包括Writer，Reader和SequenceFileSorter类进行写， 读和排序操作。</p>
</li>
</ul>
<p>​    该方案对于小文件的存取都比较自由，不限制用户和文件的多少， <strong>支持Append追加写入</strong>，支持三级文档压缩(不压缩、文件级、块级别)。</p>
<h3 id="MapFile"><a href="#MapFile" class="headerlink" title="MapFile"></a>MapFile</h3><p>​    MapFile是<strong>经过排序</strong>过后的<strong>带索引</strong>的SequenceFile可以根据键进行查找。</p>
<p><img src="/2017/08/11/0811HadoopIO/mapFile.png" alt=""></p>
<p>​    一个MapFile ~可以通过SequenceFile的地址，进行分类查找的格式~ 。使用这个格 式的优点在于，首先会将SequenceFile中的地址都加载入内存，并且进行了key 值排序，从而提供更快的数据查找。<br>与SequenceFile只生成一个文件不同，MapFile<strong>生成一个文件夹</strong>。 索引模型按<strong>128个键</strong>建立的，可以通过<code>io.map.index.interval</code>来修改。</p>
<p>​    </p>
<h3 id="MapFile和SequenceFile的比较"><a href="#MapFile和SequenceFile的比较" class="headerlink" title="MapFile和SequenceFile的比较"></a>MapFile和SequenceFile的比较</h3><ol>
<li>SequenceFile文件是用来存储key-value数据的，但它并<strong>不保证这些存储的key-value是有序的</strong>；MapFile文件则可以看做是存储有序key-value的 SequenceFile文件。</li>
<li>MapFile文件保证key-value的有序（基于key）是通过 ~每一次写入 key-value时的检查机制~ ，这种检查机制其实很简单，就是 ~保证当前正要写入的key-value与上一个刚写入的key-value符合设定的顺序~ ；但是，这种有序是由用户来保证的，一旦写入的key-value不符合key的非递减顺序，则会直接报错而不是自动的去对输入的key-value排序；</li>
</ol>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><p>​    引入压缩是为了<strong>减少储存文件所需空间</strong>，还可以<strong>降低其在网络上传输的时间</strong>。</p>
<p>​    此处简单列出Hadoop上的常用的压缩：<br><img src="/2017/08/11/0811HadoopIO/compressionTypes.tiff" alt=""></p>
<p><em>关于切分</em><br>​    举Bzip2为例，Bzip2支持切分splitting.hdfs上文件1GB，如按照默认块64MB，那么这个 文件被分为16个块（1024/64）。如果把这个块放入MR任务 ，将有<strong>16个map任务输入</strong>。 如果算法不支持切分，后果是MR把这个文件作为<strong>一个Map输入</strong>。这样任务减少了，降低了数据的本地性。<br>​    </p>
<h3 id="Hadoop中实现压缩的方法"><a href="#Hadoop中实现压缩的方法" class="headerlink" title="Hadoop中实现压缩的方法"></a>Hadoop中实现压缩的方法</h3><p>​    大致说来有两种途径：CodeC和使用本地库。</p>
<h4 id="Codec"><a href="#Codec" class="headerlink" title="Codec"></a>Codec</h4><p>​    Hadoop中压缩解压类实现<code>CompressionCodec</code>接口<code>createOutputStream</code>来创建一个<code>CompressionOutputStream</code>，将其压缩格式写入底层的流。</p>
<h3 id="本地库"><a href="#本地库" class="headerlink" title="本地库"></a>本地库</h3><p>​    Hadoop使用java开发，但是有些需求和操作并不适合java，所以引入了本地 库 native。可以高效执行某些操作。<br>如果频繁使用原生库做压解压任务，可以使用 codecpool，通过CodecPool的getCompressor方法获得Compressor对象， 需要传入Codec 。这样可以节省创建Codec对象开销 ，允许反复使用。<br>​<br>​    不同的压缩方法性能上各不相同，<strong>根据不同的使用场景来选择最合适的压缩算法</strong>。</p>
<p>各个压缩算法的特点如下：    </p>
<ol>
<li>Gzip 优点<strong>是压缩率高，速度快</strong>。Hadoop支持与直接处理文本一 样。缺点不支持split，当文件压缩在128m内，都可以用gzip（也就是说存在于一个数据块内）；</li>
<li>Izo 优点压缩速度快，具有合理的压缩率，支持split，是<strong>最流行的</strong>压缩格式。支持native库；缺点是比gzip压缩率低，<strong>hadoop本身不支持</strong>，需要安装；在应用中对lzo格式文件需要处理如 指定 inputformat为lzo格式</li>
<li>Snappy压缩 高速压缩率合理支持本地库。不支持split， <strong>hadoop不支持</strong>，要安装linux没有对应命令；当MR输出数据较大， 作为到reduce数据压缩格式</li>
<li>Bzip2 支持split，很高的压缩率，比gzip高，<strong>hadoop支持但不支持native</strong>，linux自带命令使用方便。缺点压缩解压速度慢</li>
</ol>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><h3 id="Hadoop引入序列化的原因"><a href="#Hadoop引入序列化的原因" class="headerlink" title="Hadoop引入序列化的原因"></a>Hadoop引入序列化的原因</h3><ol>
<li>Hadoop在<strong>集群之间通信或者RPC调用时需要序列化</strong>，而且要求序列化要快，且体积要小，占用带宽小；</li>
<li><strong>Java的序列化机制不满足Hadoop的需求</strong>，占用大量计算开销，且序列化结果体积过大;；它的引用机制也导致大文件不能被切分，浪费空间；此外，很难对其他语言进行扩展使用；</li>
<li>java的反序列化过程每次都会<strong>构造新的对象，不能复用对象</strong>。</li>
</ol>
<h3 id="Hadoop的序列化类"><a href="#Hadoop的序列化类" class="headerlink" title="Hadoop的序列化类"></a>Hadoop的序列化类</h3><p>Hadoop实现了大量的序列化类，如下如所示：<br><img src="/2017/08/11/0811HadoopIO/Writable-class-hierarchy.png" alt=""><br>重点关注右上方的部分，基本上是对Java基本类型的封装。<br><img src="/2017/08/11/0811HadoopIO/writable1.tiff" alt=""></p>
<h3 id="Hadoop的序列化接口"><a href="#Hadoop的序列化接口" class="headerlink" title="Hadoop的序列化接口"></a>Hadoop的序列化接口</h3><h4 id="Writable"><a href="#Writable" class="headerlink" title="Writable"></a>Writable</h4><p>​    Hadoop的序列化类是通过实现序列化接口<code>Writable</code>实现的，<code>Writable</code>接口是基于DataInput与DatOutput的简单高效可序列化接口。继承该接口必须实现的两个方法是<code>readField()</code>和<code>write()</code></p>
<h4 id="WritableComparable"><a href="#WritableComparable" class="headerlink" title="WritableComparable"></a>WritableComparable</h4><p>​    同时实现序列化和比较。类似java的Comparable接口，用于类型的比较。MR其中一个阶段叫排序， 默认使用Key来排序。<br>需要同时实现三个方法<code>readField()</code>，<code>write()</code>和<code>compareTo()</code>。</p>
<p>WritableComparable的简单实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> entity;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.DataInput;</div><div class="line"><span class="keyword">import</span> java.io.DataOutput;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 本类是一个Text的二元组&lt;/br&gt;</span></div><div class="line"><span class="comment"> * 实现了WritableComparable接口&lt;/br&gt;</span></div><div class="line"><span class="comment"> * 比较的方式是，首先比较第一个元素，若相同，则比较第二个元素。&lt;/br&gt;</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> hw</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TextPair</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">TextPair</span>&gt; </span>&#123;</div><div class="line">	</div><div class="line">	<span class="keyword">private</span> Text firstText;</div><div class="line">	<span class="keyword">private</span> Text secondText;</div><div class="line">	</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">//因为Text本身也就实现了Writable结构，此时就直接调用二元组内的两个Text的write方法即可。</span></div><div class="line">		<span class="keyword">this</span>.firstText.write(out);</div><div class="line">		<span class="keyword">this</span>.secondText.write(out);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">		<span class="comment">// TODO Auto-generated method stub</span></div><div class="line">		<span class="keyword">this</span>.firstText.readFields(in);</div><div class="line">		<span class="keyword">this</span>.secondText.readFields(in);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	</div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(TextPair o)</span> </span>&#123;</div><div class="line">		<span class="comment">//首先比较firstText，不同则比较secondText</span></div><div class="line">		<span class="keyword">int</span> result = firstText.compareTo(o.getSecondText());</div><div class="line">		<span class="keyword">return</span> result!=<span class="number">0</span>? result: <span class="keyword">this</span>.secondText.compareTo(o.getSecondText());</div><div class="line">	&#125;</div><div class="line">	</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">getFirstText</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">return</span> firstText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFirstText</span><span class="params">(Text firstText)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.firstText = firstText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">getSecondText</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">return</span> secondText;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSecondText</span><span class="params">(Text secondText)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.secondText = secondText;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="实现自定义的序列化对象"><a href="#实现自定义的序列化对象" class="headerlink" title="实现自定义的序列化对象"></a>实现自定义的序列化对象</h3><p>​    实现自定义的Hadoop序列化对象，也需要和内置的序列化对象一样，实现<code>Writable</code>接口，其中必须实现的两个方法是<code>readField()</code>和<code>write()</code>。</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/11/0811HadoopIO/" data-id="cj6xjfey80004tsjoffbi37pq" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  
    <article id="post-0810HDFSreadwrite" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/10/0810HDFSreadwrite/">HDFS的读写策略</a>
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/08/10/0810HDFSreadwrite/" class="article-date"><time datetime="2017-08-10T13:29:25.000Z" itemprop="datePublished">2017-08-10</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>​    向HDFS读写数据的过程中涉及到以下几个部分：</p>
<ol>
<li>客户端节点</li>
<li>客户端节点上的JVM</li>
<li>数据节点</li>
<li>名称节点</li>
</ol>
<h2 id="从HDFS读取数据"><a href="#从HDFS读取数据" class="headerlink" title="从HDFS读取数据"></a>从HDFS读取数据</h2><p><img src="/2017/08/10/0810HDFSreadwrite/a_client_reading_data_from_HDFS.png" alt=""><br>​<br>流程如下：</p>
<ol>
<li>客户端回调用FileSystem的实例<code>DistributedFileSystem</code>的open方法，获得这个文件的<strong>输入流</strong>；</li>
<li>以RPC的方式调用名称节点，以确定文件开头部分块的位置（包括副本的位置，按照与客户端的距离来排序），</li>
<li>客户端按照就近原则，调用输入流的read方法从最近的DataNode读取数据， 若客户端本身就是一个数据节点，则直接从节点上读取（期间将操作日志发送给NameNode）；</li>
<li>客户端读取到数据块末端，将关闭与这个DataNode 的连接，然后<strong>重新查找</strong>下一个数据块。</li>
<li>重复2-4直到当客户端读取完成，<code>DFSInputStream</code>将被关闭；</li>
</ol>
<h2 id="向HDFS写数据"><a href="#向HDFS写数据" class="headerlink" title="向HDFS写数据"></a>向HDFS写数据</h2><p><img src="/2017/08/10/0810HDFSreadwrite/a_client_writing_data_to_HDFS.png" alt=""></p>
<ol>
<li>客户端通过在<code>DistributedFileSystem</code>调用<code>create</code>方法来创建文件；</li>
<li><code>DistributedFileSystem</code>的一个RPC调用NameNode，在文件系统的密码<strong>命名空间</strong>中创建一个新文件；NameNode 将通过一些检查，比如 ~文件是否存在~，客户端 ~是否拥有创建权限~ 等;通过检查之后，在NameNode 添加文件信息。注意，因为此时文件没有数据，所以<strong>NameNode 上也没有文件数据块的信息</strong>。</li>
<li>创建结束之后，HDFS会返回一个输出流 DFSDataOutputStream 给客户端；</li>
<li>客户端调用输出流DFSDataOutputStream的write方法向HDFS 中对应的文件写入数据。</li>
<li>数据首先会被<strong>分包</strong>，这些分包会写人一个<strong>输出流的内部队列</strong> Data 队列中， ~接收完数据分包，输出流DFSDataOutputStream会向NameNode申请保存文件和副本数据块的若干个DataNode ， 这若干个DataNode 会形成一个数据传输管道。~ DFSDataOutputStream 将数据传输给距离上最短的DataNode ，这个DataNode 接收到数据包之后会传给下一个DataNode 。 ~数据在各DataNode之间通过管道流动，而不是全部由输出流分发~， 以减少传输开销。</li>
<li>因为各DataNode位于不同机器上，数据需要通过网络发送，所以，为了保证所有DataNode 的数据都是准确的， ~接收到数据的 DataNode 要向<strong>发送者</strong>发送确认包(ACK Packet )~ 。对于某个数据块，只有当DFSDataOutputStream 收到了所有DataNode 的正确ACK，才能确认传输结束。DFSDataOutputStream 内部专 门维护了一个<strong>等待ACK 队列</strong>，这一队列保存已经进入管道传输数据、但是并未被完全确认的数据包。</li>
<li>重复4-6，DFSDataInputStream 继续等待直到所有数据写入完毕并<strong>被确认</strong>后，调用<code>close</code>关闭流，调用<code>complete</code>方法通知NameNode 文件写入完成。NameNode 接收到complete消息之后， ~等待相应数量的副本写入完毕后~， 告知客户端。</li>
</ol>
<p><em>数据流列表形成一个管线。</em><br><em>一个包只有在被管线中的<strong>所有节点</strong>确认后才会被移出确认队列</em></p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/10/0810HDFSreadwrite/" data-id="cj6xjfexr0000tsjoahuucx9a" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分布式系统/">分布式系统</a></li></ul>


    </footer>
  </div>
  
</article>



  


  <div id="page-nav">
    <nav><ul class="pagination"><li class="disabled"><span class="page-prev"><i class="fa fa-chevron-left"></i> Prev</a></li><li class="active"><span class="page-number">1</span></li><li><a class="page-number" href="/page/2/">2</a></li><li><a class="page-next" rel="next" href="/page/2/">Next <i class="fa fa-chevron-right"></i></a></li></ul></nav>
  </div>



        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <p></p>

</div>


  
  <div class="sidebar-module">
    <h4>Categories</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/大数据/">大数据</a><span class="sidebar-module-list-count">8</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tags</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/HBase/">HBase</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/HDFS/">HDFS</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/MapReduce/">MapReduce</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/NoSQL/">NoSQL</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/代码/">代码</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/分布式系统/">分布式系统</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/数据存储/">数据存储</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tag Cloud</h4>
    <p class="tagcloud">
      <a href="/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/tags/HDFS/" style="font-size: 16.67px;">HDFS</a> <a href="/tags/MapReduce/" style="font-size: 13.33px;">MapReduce</a> <a href="/tags/NoSQL/" style="font-size: 10px;">NoSQL</a> <a href="/tags/代码/" style="font-size: 10px;">代码</a> <a href="/tags/分布式系统/" style="font-size: 20px;">分布式系统</a> <a href="/tags/数据存储/" style="font-size: 10px;">数据存储</a>
    </p>
  </div>


  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/08/">August 2017</a><span class="sidebar-module-list-count">12</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2017/08/29/test-1/">test</a>
        </li>
      
        <li>
          <a href="/2017/08/29/hbase/">HBase 简介</a>
        </li>
      
        <li>
          <a href="/2017/08/18/hiveIntro/">hive简介</a>
        </li>
      
        <li>
          <a href="/2017/08/16/MyTopKClass/">MyTopKClass</a>
        </li>
      
        <li>
          <a href="/2017/08/15/hadoopMRAdv/">hadoop高级</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2017 H.W.Huang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>



<script src="/js/script.js"></script>

</body>
</html>
